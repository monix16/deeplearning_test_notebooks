{
  "paragraphs": [
    {
      "text": "os.environ[\"KERAS_BACKEND\"] \u003d \"theano\"\nos.environ[\u0027THEANO_FLAGS\u0027] \u003d \u0027floatX\u003dfloat32,device\u003dcuda,dnn.library_path\u003d/usr/local/cuda/lib64,dnn.include_path\u003d/usr/local/cuda/include/\u0027\nos.environ[\u0027MKL_THREADING_LAYER\u0027] \u003d \"GNU\"",
      "user": "prashantp@qubole.com",
      "dateUpdated": "May 25, 2018 7:13:40 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [],
        "numCompletedTasks": 0,
        "numTasks": 0,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1516357950793_-1673444453",
      "id": "20180119-103230_1331949073_q_DBYREUFXGA1527182639",
      "dateCreated": "Jan 19, 2018 10:32:30 AM",
      "dateSubmitted": "May 24, 2018 5:24:33 PM",
      "dateStarted": "May 24, 2018 5:24:45 PM",
      "dateFinished": "May 24, 2018 5:25:07 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "# Confirming backend is changed to Theano\nfrom __future__ import print_function\nimport keras\nfrom keras import backend as K\nprint(K.backend())",
      "dateUpdated": "Jan 22, 2018 7:24:16 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [],
        "numCompletedTasks": 0,
        "numTasks": 0,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1516357959483_-708094139",
      "id": "20180119-103239_536108425_q_DBYREUFXGA1527182639",
      "dateCreated": "Jan 19, 2018 10:32:39 AM",
      "dateSubmitted": "May 24, 2018 5:24:45 PM",
      "dateStarted": "May 24, 2018 5:25:08 PM",
      "dateFinished": "May 24, 2018 5:25:27 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "from __future__ import print_function\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM\n\n# ----------------------------------------------------------\n# EDITABLE PARAMETERS\n# Read the documentation in the script head for more details\n# ----------------------------------------------------------\n\n# length of input\ninput_len \u003d 1000\n\n# The window length of the moving average used to generate\n# the output from the input in the input/output pair used\n# to train the LSTM\n# e.g. if tsteps\u003d2 and input\u003d[1, 2, 3, 4, 5],\n#      then output\u003d[1.5, 2.5, 3.5, 4.5]\ntsteps \u003d 2\n\n# The input sequence length that the LSTM is trained on for each output point\nlahead \u003d 1\n\n# training parameters passed to \"model.fit(...)\"\nbatch_size \u003d 1\nepochs \u003d 10\n\n# ------------\n# MAIN PROGRAM\n# ------------\n\nprint(\"*\" * 33)\nif lahead \u003e\u003d tsteps:\n    print(\"STATELESS LSTM WILL ALSO CONVERGE\")\nelse:\n    print(\"STATELESS LSTM WILL NOT CONVERGE\")\nprint(\"*\" * 33)\n\nnp.random.seed(1986)\n\nprint(\u0027Generating Data...\u0027)\n\n\ndef gen_uniform_amp(amp\u003d1, xn\u003d10000):\n    \"\"\"Generates uniform random data between\n    -amp and +amp\n    and of length xn\n    Arguments:\n        amp: maximum/minimum range of uniform data\n        xn: length of series\n    \"\"\"\n    data_input \u003d np.random.uniform(-1 * amp, +1 * amp, xn)\n    data_input \u003d pd.DataFrame(data_input)\n    return data_input\n\n# Since the output is a moving average of the input,\n# the first few points of output will be NaN\n# and will be dropped from the generated data\n# before training the LSTM.\n# Also, when lahead \u003e 1,\n# the preprocessing step later of \"rolling window view\"\n# will also cause some points to be lost.\n# For aesthetic reasons,\n# in order to maintain generated data length \u003d input_len after pre-processing,\n# add a few points to account for the values that will be lost.\nto_drop \u003d max(tsteps - 1, lahead - 1)\ndata_input \u003d gen_uniform_amp(amp\u003d0.1, xn\u003dinput_len + to_drop)\n\n# set the target to be a N-point average of the input\nexpected_output \u003d data_input.rolling(window\u003dtsteps, center\u003dFalse).mean()\n\n# when lahead \u003e 1, need to convert the input to \"rolling window view\"\n# https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html\nif lahead \u003e 1:\n    data_input \u003d np.repeat(data_input.values, repeats\u003dlahead, axis\u003d1)\n    data_input \u003d pd.DataFrame(data_input)\n    for i, c in enumerate(data_input.columns):\n        data_input[c] \u003d data_input[c].shift(i)\n\n# drop the nan\nexpected_output \u003d expected_output[to_drop:]\ndata_input \u003d data_input[to_drop:]\n\nprint(\u0027Input shape:\u0027, data_input.shape)\nprint(\u0027Output shape:\u0027, expected_output.shape)\nprint(\u0027Input head: \u0027)\nprint(data_input.head())\nprint(\u0027Output head: \u0027)\nprint(expected_output.head())\nprint(\u0027Input tail: \u0027)\nprint(data_input.tail())\nprint(\u0027Output tail: \u0027)\nprint(expected_output.tail())\n\nprint(\u0027Plotting input and expected output\u0027)\nplt.plot(data_input[0][:10], \u0027.\u0027)\nplt.plot(expected_output[0][:10], \u0027-\u0027)\nplt.legend([\u0027Input\u0027, \u0027Expected output\u0027])\nplt.title(\u0027Input\u0027)\nplt.show()\n\n\ndef create_model(stateful):\n    model \u003d Sequential()\n    model.add(LSTM(20,\n              input_shape\u003d(lahead, 1),\n              batch_size\u003dbatch_size,\n              stateful\u003dstateful))\n    model.add(Dense(1))\n    model.compile(loss\u003d\u0027mse\u0027, optimizer\u003d\u0027adam\u0027)\n    return model\n\nprint(\u0027Creating Stateful Model...\u0027)\nmodel_stateful \u003d create_model(stateful\u003dTrue)\n\n\n# split train/test data\ndef split_data(x, y, ratio\u003d0.8):\n    to_train \u003d int(input_len * ratio)\n    # tweak to match with batch_size\n    to_train -\u003d to_train % batch_size\n\n    x_train \u003d x[:to_train]\n    y_train \u003d y[:to_train]\n    x_test \u003d x[to_train:]\n    y_test \u003d y[to_train:]\n\n    # tweak to match with batch_size\n    to_drop \u003d x.shape[0] % batch_size\n    if to_drop \u003e 0:\n        x_test \u003d x_test[:-1 * to_drop]\n        y_test \u003d y_test[:-1 * to_drop]\n\n    # some reshaping\n    reshape_3 \u003d lambda x: x.values.reshape((x.shape[0], x.shape[1], 1))\n    x_train \u003d reshape_3(x_train)\n    x_test \u003d reshape_3(x_test)\n\n    reshape_2 \u003d lambda x: x.values.reshape((x.shape[0], 1))\n    y_train \u003d reshape_2(y_train)\n    y_test \u003d reshape_2(y_test)\n\n    return (x_train, y_train), (x_test, y_test)\n\n\n(x_train, y_train), (x_test, y_test) \u003d split_data(data_input, expected_output)\nprint(\u0027x_train.shape: \u0027, x_train.shape)\nprint(\u0027y_train.shape: \u0027, y_train.shape)\nprint(\u0027x_test.shape: \u0027, x_test.shape)\nprint(\u0027y_test.shape: \u0027, y_test.shape)\n\nprint(\u0027Training\u0027)\nfor i in range(epochs):\n    print(\u0027Epoch\u0027, i + 1, \u0027/\u0027, epochs)\n    # Note that the last state for sample i in a batch will\n    # be used as initial state for sample i in the next batch.\n    # Thus we are simultaneously training on batch_size series with\n    # lower resolution than the original series contained in data_input.\n    # Each of these series are offset by one step and can be\n    # extracted with data_input[i::batch_size].\n    model_stateful.fit(x_train,\n                       y_train,\n                       batch_size\u003dbatch_size,\n                       epochs\u003d1,\n                       verbose\u003d1,\n                       validation_data\u003d(x_test, y_test),\n                       shuffle\u003dFalse)\n    model_stateful.reset_states()\n\nprint(\u0027Predicting\u0027)\npredicted_stateful \u003d model_stateful.predict(x_test, batch_size\u003dbatch_size)\n\nprint(\u0027Creating Stateless Model...\u0027)\nmodel_stateless \u003d create_model(stateful\u003dFalse)\n\nprint(\u0027Training\u0027)\nmodel_stateless.fit(x_train,\n                    y_train,\n                    batch_size\u003dbatch_size,\n                    epochs\u003depochs,\n                    verbose\u003d1,\n                    validation_data\u003d(x_test, y_test),\n                    shuffle\u003dFalse)\n\nprint(\u0027Predicting\u0027)\npredicted_stateless \u003d model_stateless.predict(x_test, batch_size\u003dbatch_size)\n\n# ----------------------------\n\nprint(\u0027Plotting Results\u0027)\nplt.subplot(3, 1, 1)\nplt.plot(y_test)\nplt.title(\u0027Expected\u0027)\nplt.subplot(3, 1, 2)\n# drop the first \"tsteps-1\" because it is not possible to predict them\n# since the \"previous\" timesteps to use do not exist\nplt.plot((y_test - predicted_stateful).flatten()[tsteps - 1:])\nplt.title(\u0027Stateful: Expected - Predicted\u0027)\nplt.subplot(3, 1, 3)\nplt.plot((y_test - predicted_stateless).flatten())\nplt.title(\u0027Stateless: Expected - Predicted\u0027)\nplt.show()",
      "dateUpdated": "Jan 22, 2018 7:24:16 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [],
        "numCompletedTasks": 0,
        "numTasks": 0,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1516096596026_-639354192",
      "id": "20180116-095636_1884716572_q_DBYREUFXGA1527182639",
      "dateCreated": "Jan 16, 2018 9:56:36 AM",
      "dateSubmitted": "May 24, 2018 5:25:08 PM",
      "dateStarted": "May 24, 2018 5:25:27 PM",
      "dateFinished": "May 24, 2018 5:25:36 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "Example script showing how to use a stateful LSTM model and how its stateless counterpart performs. stateful: Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch.\n\nInput : IMDB Dataset. It contains 25000 movie reviews in each training as well as testing batch.\n    Each batch contains a dictionary with the following elements:-\n    \n    data -- an array of 25000 lists for each training as well as testing batch. Each element in the list has different length and is a review encoded as a sequence of word              indexes(integers).\n    labels -- an array of 25000 numbers either 0 or 1 for both training as well as testing batch. The number at index i indicates the label of the ith image in the                      array data.\n\nOutput : Loss of the final epoch of the trained model.",
      "user": "prashantp@qubole.com",
      "dateUpdated": "Jan 22, 2018 7:28:06 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": false,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1516099849469_142711150",
      "id": "20180116-105049_1979187662_q_DBYREUFXGA1527182639",
      "dateCreated": "Jan 16, 2018 10:50:49 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1516605856890_-1678579655",
      "id": "20180122-072416_520739515_q_DBYREUFXGA1527182639",
      "dateCreated": "Jan 22, 2018 7:24:16 AM",
      "dateSubmitted": "May 24, 2018 5:25:27 PM",
      "dateStarted": "May 24, 2018 5:25:36 PM",
      "dateFinished": "May 24, 2018 5:25:36 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "lstm_stateful",
  "id": "DBYREUFXGA1527182639",
  "angularObjects": {
    "2DEK3QV851090031527168688484:shared_process": [],
    "2DGTE4P2N1090031527182654062:shared_process": [],
    "2DENTX6JV1090031527168688466:shared_process": [],
    "2DDED8P751090031527168688477:shared_process": [],
    "2DDBWFX441090031527168688480:shared_process": []
  },
  "config": {
    "isDashboard": false,
    "defaultLang": "pyspark"
  },
  "info": {},
  "source": "FCN"
}