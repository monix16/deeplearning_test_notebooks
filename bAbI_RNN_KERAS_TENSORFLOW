{
  "paragraphs": [
    {
      "text": "from __future__ import print_function\nfrom functools import reduce\nimport re\nimport tarfile\n\nimport numpy as np\n\nfrom keras.utils.data_utils import get_file\nfrom keras.layers.embeddings import Embedding\nfrom keras import layers\nfrom keras.layers import recurrent\nfrom keras.models import Model\nfrom keras.preprocessing.sequence import pad_sequences\n\n\ndef tokenize(sent):\n    \u0027\u0027\u0027Return the tokens of a sentence including punctuation.\n    \u003e\u003e\u003e tokenize(\u0027Bob dropped the apple. Where is the apple?\u0027)\n    [\u0027Bob\u0027, \u0027dropped\u0027, \u0027the\u0027, \u0027apple\u0027, \u0027.\u0027, \u0027Where\u0027, \u0027is\u0027, \u0027the\u0027, \u0027apple\u0027, \u0027?\u0027]\n    \u0027\u0027\u0027\n    return [x.strip() for x in re.split(\u0027(\\W+)?\u0027, sent) if x.strip()]\n\n\ndef parse_stories(lines, only_supporting\u003dFalse):\n    \u0027\u0027\u0027Parse stories provided in the bAbi tasks format\n    If only_supporting is true,\n    only the sentences that support the answer are kept.\n    \u0027\u0027\u0027\n    data \u003d []\n    story \u003d []\n    for line in lines:\n        line \u003d line.decode(\u0027utf-8\u0027).strip()\n        nid, line \u003d line.split(\u0027 \u0027, 1)\n        nid \u003d int(nid)\n        if nid \u003d\u003d 1:\n            story \u003d []\n        if \u0027\\t\u0027 in line:\n            q, a, supporting \u003d line.split(\u0027\\t\u0027)\n            q \u003d tokenize(q)\n            substory \u003d None\n            if only_supporting:\n                # Only select the related substory\n                supporting \u003d map(int, supporting.split())\n                substory \u003d [story[i - 1] for i in supporting]\n            else:\n                # Provide all the substories\n                substory \u003d [x for x in story if x]\n            data.append((substory, q, a))\n            story.append(\u0027\u0027)\n        else:\n            sent \u003d tokenize(line)\n            story.append(sent)\n    return data\n\n\ndef get_stories(f, only_supporting\u003dFalse, max_length\u003dNone):\n    \u0027\u0027\u0027Given a file name, read the file, retrieve the stories,\n    and then convert the sentences into a single story.\n    If max_length is supplied,\n    any stories longer than max_length tokens will be discarded.\n    \u0027\u0027\u0027\n    data \u003d parse_stories(f.readlines(), only_supporting\u003donly_supporting)\n    flatten \u003d lambda data: reduce(lambda x, y: x + y, data)\n    data \u003d [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) \u003c max_length]\n    return data\n\n\ndef vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n    xs \u003d []\n    xqs \u003d []\n    ys \u003d []\n    for story, query, answer in data:\n        x \u003d [word_idx[w] for w in story]\n        xq \u003d [word_idx[w] for w in query]\n        # let\u0027s not forget that index 0 is reserved\n        y \u003d np.zeros(len(word_idx) + 1)\n        y[word_idx[answer]] \u003d 1\n        xs.append(x)\n        xqs.append(xq)\n        ys.append(y)\n    return pad_sequences(xs, maxlen\u003dstory_maxlen), pad_sequences(xqs, maxlen\u003dquery_maxlen), np.array(ys)\n\nRNN \u003d recurrent.LSTM\nEMBED_HIDDEN_SIZE \u003d 50\nSENT_HIDDEN_SIZE \u003d 100\nQUERY_HIDDEN_SIZE \u003d 100\nBATCH_SIZE \u003d 32\nEPOCHS \u003d 5\nprint(\u0027RNN / Embed / Sent / Query \u003d {}, {}, {}, {}\u0027.format(RNN,\n                                                           EMBED_HIDDEN_SIZE,\n                                                           SENT_HIDDEN_SIZE,\n                                                           QUERY_HIDDEN_SIZE))\n\ntry:\n    path \u003d get_file(\u0027babi-tasks-v1-2.tar.gz\u0027, origin\u003d\u0027https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\u0027)\nexcept:\n    print(\u0027Error downloading dataset, please download it manually:\\n\u0027\n          \u0027$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\\n\u0027\n          \u0027$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz\u0027)\n    raise\ntar \u003d tarfile.open(path)\n# Default QA1 with 1000 samples\n# challenge \u003d \u0027tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt\u0027\n# QA1 with 10,000 samples\n# challenge \u003d \u0027tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt\u0027\n# QA2 with 1000 samples\nchallenge \u003d \u0027tasks_1-20_v1-2/en/qa2_two-supporting-facts_{}.txt\u0027\n# QA2 with 10,000 samples\n# challenge \u003d \u0027tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt\u0027\ntrain \u003d get_stories(tar.extractfile(challenge.format(\u0027train\u0027)))\ntest \u003d get_stories(tar.extractfile(challenge.format(\u0027test\u0027)))\n\nvocab \u003d set()\nfor story, q, answer in train + test:\n    vocab |\u003d set(story + q + [answer])\nvocab \u003d sorted(vocab)\n\n# Reserve 0 for masking via pad_sequences\nvocab_size \u003d len(vocab) + 1\nword_idx \u003d dict((c, i + 1) for i, c in enumerate(vocab))\nstory_maxlen \u003d max(map(len, (x for x, _, _ in train + test)))\nquery_maxlen \u003d max(map(len, (x for _, x, _ in train + test)))\n\nx, xq, y \u003d vectorize_stories(train, word_idx, story_maxlen, query_maxlen)\ntx, txq, ty \u003d vectorize_stories(test, word_idx, story_maxlen, query_maxlen)\n\nprint(\u0027vocab \u003d {}\u0027.format(vocab))\nprint(\u0027x.shape \u003d {}\u0027.format(x.shape))\nprint(\u0027xq.shape \u003d {}\u0027.format(xq.shape))\nprint(\u0027y.shape \u003d {}\u0027.format(y.shape))\nprint(\u0027story_maxlen, query_maxlen \u003d {}, {}\u0027.format(story_maxlen, query_maxlen))\n\nprint(\u0027Build model...\u0027)\n\nsentence \u003d layers.Input(shape\u003d(story_maxlen,), dtype\u003d\u0027int32\u0027)\nencoded_sentence \u003d layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(sentence)\nencoded_sentence \u003d layers.Dropout(0.3)(encoded_sentence)\n\nquestion \u003d layers.Input(shape\u003d(query_maxlen,), dtype\u003d\u0027int32\u0027)\nencoded_question \u003d layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(question)\nencoded_question \u003d layers.Dropout(0.3)(encoded_question)\nencoded_question \u003d RNN(EMBED_HIDDEN_SIZE)(encoded_question)\nencoded_question \u003d layers.RepeatVector(story_maxlen)(encoded_question)\n\nmerged \u003d layers.add([encoded_sentence, encoded_question])\nmerged \u003d RNN(EMBED_HIDDEN_SIZE)(merged)\nmerged \u003d layers.Dropout(0.3)(merged)\npreds \u003d layers.Dense(vocab_size, activation\u003d\u0027softmax\u0027)(merged)\n\nmodel \u003d Model([sentence, question], preds)\nmodel.compile(optimizer\u003d\u0027adam\u0027,\n              loss\u003d\u0027categorical_crossentropy\u0027,\n              metrics\u003d[\u0027accuracy\u0027])\n\nprint(\u0027Training\u0027)\nmodel.fit([x, xq], y,\n          batch_size\u003dBATCH_SIZE,\n          epochs\u003dEPOCHS,\n          validation_split\u003d0.05)\nloss, acc \u003d model.evaluate([tx, txq], ty,\n                           batch_size\u003dBATCH_SIZE)\nprint(\u0027Test loss / test accuracy \u003d {:.4f} / {:.4f}\u0027.format(loss, acc))",
      "user": "prashantp@qubole.com",
      "dateUpdated": "Jan 16, 2018 6:53:16 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [],
        "numCompletedTasks": 0,
        "numTasks": 0,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1516019341729_-919397811",
      "id": "20180115-122901_1736191046_q_2EST7KBRP31516019108",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "RNN / Embed / Sent / Query \u003d \u003cclass \u0027keras.layers.recurrent.LSTM\u0027\u003e, 50, 100, 100\nvocab \u003d [\u0027.\u0027, \u0027?\u0027, \u0027Daniel\u0027, \u0027John\u0027, \u0027Mary\u0027, \u0027Sandra\u0027, \u0027Where\u0027, \u0027apple\u0027, \u0027back\u0027, \u0027bathroom\u0027, \u0027bedroom\u0027, \u0027discarded\u0027, \u0027down\u0027, \u0027dropped\u0027, \u0027football\u0027, \u0027garden\u0027, \u0027got\u0027, \u0027grabbed\u0027, \u0027hallway\u0027, \u0027is\u0027, \u0027journeyed\u0027, \u0027kitchen\u0027, \u0027left\u0027, \u0027milk\u0027, \u0027moved\u0027, \u0027office\u0027, \u0027picked\u0027, \u0027put\u0027, \u0027the\u0027, \u0027there\u0027, \u0027to\u0027, \u0027took\u0027, \u0027travelled\u0027, \u0027up\u0027, \u0027went\u0027]\nx.shape \u003d (1000, 552)\nxq.shape \u003d (1000, 5)\ny.shape \u003d (1000, 36)\nstory_maxlen, query_maxlen \u003d 552, 5\nBuild model...\nTraining\nTrain on 950 samples, validate on 50 samples\nEpoch 1/5\n\n 32/950 [\u003e.............................] - ETA: 35s - loss: 3.5841 - acc: 0.0000e+00\n\n 64/950 [\u003d\u003e............................] - ETA: 24s - loss: 3.5794 - acc: 0.0156    \n\n 96/950 [\u003d\u003d\u003e...........................] - ETA: 20s - loss: 3.5749 - acc: 0.0729\n\n128/950 [\u003d\u003d\u003d\u003e..........................] - ETA: 18s - loss: 3.5704 - acc: 0.0859\n\n160/950 [\u003d\u003d\u003d\u003d\u003e.........................] - ETA: 16s - loss: 3.5652 - acc: 0.0938\n\n192/950 [\u003d\u003d\u003d\u003d\u003d\u003e........................] - ETA: 15s - loss: 3.5609 - acc: 0.1094\n\n224/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003e.......................] - ETA: 14s - loss: 3.5544 - acc: 0.1339\n\n256/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e......................] - ETA: 13s - loss: 3.5490 - acc: 0.1172\n\n288/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.....................] - ETA: 12s - loss: 3.5421 - acc: 0.1215\n\n320/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e....................] - ETA: 11s - loss: 3.5348 - acc: 0.1281\n\n352/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...................] - ETA: 11s - loss: 3.5265 - acc: 0.1335\n\n384/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..................] - ETA: 10s - loss: 3.5170 - acc: 0.1354\n\n416/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.................] - ETA: 9s - loss: 3.5041 - acc: 0.1466 \n\n448/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e................] - ETA: 9s - loss: 3.4917 - acc: 0.1496\n\n480/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...............] - ETA: 8s - loss: 3.4761 - acc: 0.1500\n\n512/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..............] - ETA: 7s - loss: 3.4563 - acc: 0.1543\n\n544/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.............] - ETA: 7s - loss: 3.4347 - acc: 0.1544\n\n576/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e............] - ETA: 6s - loss: 3.4176 - acc: 0.1476\n\n608/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...........] - ETA: 6s - loss: 3.3885 - acc: 0.1530\n\n640/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..........] - ETA: 5s - loss: 3.3535 - acc: 0.1594\n\n672/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.........] - ETA: 4s - loss: 3.3276 - acc: 0.1607\n\n704/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e........] - ETA: 4s - loss: 3.2933 - acc: 0.1605\n\n736/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.......] - ETA: 3s - loss: 3.2645 - acc: 0.1603\n\n768/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e......] - ETA: 3s - loss: 3.2310 - acc: 0.1654\n\n800/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.....] - ETA: 2s - loss: 3.1961 - acc: 0.1675\n\n832/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e....] - ETA: 2s - loss: 3.1598 - acc: 0.1695\n\n864/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...] - ETA: 1s - loss: 3.1342 - acc: 0.1701\n\n896/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..] - ETA: 0s - loss: 3.1078 - acc: 0.1741\n\n928/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.] - ETA: 0s - loss: 3.0704 - acc: 0.1789\n\n950/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 17s 18ms/step - loss: 3.0590 - acc: 0.1768 - val_loss: 2.3817 - val_acc: 0.0600\n\nEpoch 2/5\n\n 32/950 [\u003e.............................] - ETA: 15s - loss: 2.1734 - acc: 0.2188\n\n 64/950 [\u003d\u003e............................] - ETA: 14s - loss: 2.2015 - acc: 0.2500\n\n 96/950 [\u003d\u003d\u003e...........................] - ETA: 14s - loss: 2.2213 - acc: 0.2292\n\n128/950 [\u003d\u003d\u003d\u003e..........................] - ETA: 13s - loss: 2.2360 - acc: 0.2109\n\n160/950 [\u003d\u003d\u003d\u003d\u003e.........................] - ETA: 13s - loss: 2.2148 - acc: 0.1875\n\n192/950 [\u003d\u003d\u003d\u003d\u003d\u003e........................] - ETA: 12s - loss: 2.2004 - acc: 0.1719\n\n224/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003e.......................] - ETA: 11s - loss: 2.1776 - acc: 0.1652\n\n256/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e......................] - ETA: 11s - loss: 2.1466 - acc: 0.1797\n\n288/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.....................] - ETA: 10s - loss: 2.1246 - acc: 0.1806\n\n320/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e....................] - ETA: 10s - loss: 2.1179 - acc: 0.1750\n\n352/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...................] - ETA: 9s - loss: 2.0938 - acc: 0.1847 \n\n384/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..................] - ETA: 9s - loss: 2.1034 - acc: 0.1797\n\n416/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.................] - ETA: 8s - loss: 2.0889 - acc: 0.1779\n\n448/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e................] - ETA: 8s - loss: 2.0752 - acc: 0.1853\n\n480/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...............] - ETA: 7s - loss: 2.0707 - acc: 0.1896\n\n512/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..............] - ETA: 7s - loss: 2.0612 - acc: 0.1934\n\n544/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.............] - ETA: 6s - loss: 2.0530 - acc: 0.1967\n\n576/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e............] - ETA: 6s - loss: 2.0477 - acc: 0.2014\n\n608/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...........] - ETA: 5s - loss: 2.0494 - acc: 0.1957\n\n640/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..........] - ETA: 5s - loss: 2.0448 - acc: 0.1906\n\n672/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.........] - ETA: 4s - loss: 2.0398 - acc: 0.1920\n\n704/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e........] - ETA: 4s - loss: 2.0400 - acc: 0.1889\n\n736/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.......] - ETA: 3s - loss: 2.0359 - acc: 0.1861\n\n768/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e......] - ETA: 2s - loss: 2.0381 - acc: 0.1836\n\n800/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.....] - ETA: 2s - loss: 2.0418 - acc: 0.1787\n\n832/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e....] - ETA: 1s - loss: 2.0403 - acc: 0.1755\n\n864/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...] - ETA: 1s - loss: 2.0359 - acc: 0.1736\n\n896/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..] - ETA: 0s - loss: 2.0348 - acc: 0.1741\n\n928/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.] - ETA: 0s - loss: 2.0276 - acc: 0.1778\n\n950/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 16s 17ms/step - loss: 2.0254 - acc: 0.1758 - val_loss: 1.8587 - val_acc: 0.0600\n\nEpoch 3/5\n\n 32/950 [\u003e.............................] - ETA: 14s - loss: 1.7620 - acc: 0.2188\n\n 64/950 [\u003d\u003e............................] - ETA: 14s - loss: 1.7953 - acc: 0.2500\n\n 96/950 [\u003d\u003d\u003e...........................] - ETA: 13s - loss: 1.7909 - acc: 0.2604\n\n128/950 [\u003d\u003d\u003d\u003e..........................] - ETA: 13s - loss: 1.8560 - acc: 0.2188\n\n160/950 [\u003d\u003d\u003d\u003d\u003e.........................] - ETA: 12s - loss: 1.8498 - acc: 0.2125\n\n192/950 [\u003d\u003d\u003d\u003d\u003d\u003e........................] - ETA: 12s - loss: 1.8345 - acc: 0.2240\n\n224/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003e.......................] - ETA: 11s - loss: 1.8336 - acc: 0.2366\n\n256/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e......................] - ETA: 11s - loss: 1.8528 - acc: 0.2227\n\n288/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.....................] - ETA: 10s - loss: 1.8584 - acc: 0.2188\n\n320/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e....................] - ETA: 10s - loss: 1.8609 - acc: 0.2094\n\n352/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...................] - ETA: 9s - loss: 1.8591 - acc: 0.2102 \n\n384/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..................] - ETA: 9s - loss: 1.8735 - acc: 0.2057\n\n416/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.................] - ETA: 8s - loss: 1.8760 - acc: 0.2043\n\n448/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e................] - ETA: 8s - loss: 1.8650 - acc: 0.2054\n\n480/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...............] - ETA: 7s - loss: 1.8654 - acc: 0.2062\n\n512/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..............] - ETA: 7s - loss: 1.8644 - acc: 0.2090\n\n544/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.............] - ETA: 6s - loss: 1.8626 - acc: 0.2132\n\n576/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e............] - ETA: 6s - loss: 1.8636 - acc: 0.2153\n\n608/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...........] - ETA: 5s - loss: 1.8678 - acc: 0.2072\n\n640/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..........] - ETA: 5s - loss: 1.8756 - acc: 0.2047\n\n672/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.........] - ETA: 4s - loss: 1.8773 - acc: 0.2009\n\n704/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e........] - ETA: 4s - loss: 1.8807 - acc: 0.2031\n\n736/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.......] - ETA: 3s - loss: 1.8892 - acc: 0.1997\n\n768/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e......] - ETA: 3s - loss: 1.8897 - acc: 0.1966\n\n800/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.....] - ETA: 2s - loss: 1.8927 - acc: 0.1988\n\n832/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e....] - ETA: 1s - loss: 1.8963 - acc: 0.1971\n\n864/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...] - ETA: 1s - loss: 1.9008 - acc: 0.1933\n\n896/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..] - ETA: 0s - loss: 1.9019 - acc: 0.1953\n\n928/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.] - ETA: 0s - loss: 1.9048 - acc: 0.1929\n\n950/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 16s 17ms/step - loss: 1.9060 - acc: 0.1905 - val_loss: 1.8130 - val_acc: 0.0600\n\nEpoch 4/5\n\n 32/950 [\u003e.............................] - ETA: 15s - loss: 1.9496 - acc: 0.0625\n\n 64/950 [\u003d\u003e............................] - ETA: 14s - loss: 1.9527 - acc: 0.1250\n\n 96/950 [\u003d\u003d\u003e...........................] - ETA: 14s - loss: 1.9518 - acc: 0.1458\n\n128/950 [\u003d\u003d\u003d\u003e..........................] - ETA: 13s - loss: 1.9438 - acc: 0.1641\n\n160/950 [\u003d\u003d\u003d\u003d\u003e.........................] - ETA: 13s - loss: 1.9446 - acc: 0.1750\n\n192/950 [\u003d\u003d\u003d\u003d\u003d\u003e........................] - ETA: 12s - loss: 1.9211 - acc: 0.1875\n\n224/950 [\u003d\u003d\u003d\u003d\u003d\u003d\u003e.......................] - ETA: 12s - loss: 1.9077 - acc: 0.1875\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-6241247212798465828.py\", line 292, in \u003cmodule\u003e\n    exec(code)\n  File \"\u003cstdin\u003e\", line 126, in \u003cmodule\u003e\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/keras/engine/training.py\", line 1657, in fit\n    validation_steps\u003dvalidation_steps)\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/keras/engine/training.py\", line 1213, in _fit_loop\n    outs \u003d f(ins_batch)\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 2357, in __call__\n    **self.session_kwargs)\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 889, in run\n    run_metadata_ptr)\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\n    options, run_metadata)\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\n    return fn(*args)\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\n    status, run_metadata)\n  File \"/usr/lib/spark/python/pyspark/context.py\", line 239, in signal_handler\n    raise KeyboardInterrupt()\nKeyboardInterrupt\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-6241247212798465828.py\", line 299, in \u003cmodule\u003e\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-6241247212798465828.py\", line 292, in \u003cmodule\u003e\n    exec(code)\n  File \"\u003cstdin\u003e\", line 126, in \u003cmodule\u003e\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/keras/engine/training.py\", line 1657, in fit\n    validation_steps\u003dvalidation_steps)\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/keras/engine/training.py\", line 1213, in _fit_loop\n    outs \u003d f(ins_batch)\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 2357, in __call__\n    **self.session_kwargs)\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 889, in run\n    run_metadata_ptr)\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1120, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\n    options, run_metadata)\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\n    return fn(*args)\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\n    status, run_metadata)\n  File \"/usr/lib/spark/python/pyspark/context.py\", line 239, in signal_handler\n    raise KeyboardInterrupt()\nKeyboardInterrupt\n\n"
      },
      "dateCreated": "Jan 15, 2018 12:29:01 PM",
      "dateStarted": "Jan 16, 2018 6:53:16 AM",
      "dateFinished": "Jan 16, 2018 6:54:14 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "Model :- RNN.\nInput :- Facebook bAbI dataset. Contains stories etc. Contains 10000 training stories.\nThe file format for each task is as follows:\nID text\nID text\nID text\nID question[tab]answer[tab]supporting fact IDS.\n...\n\nThe IDs for a given \"story\" start at 1 and increase.\nWhen the IDs in a file reset back to 1 you can consider the following sentences as a new \"story\".\n\nOutput :- Accuracy which is calculated as : questions are asked based on the stories and its accuracy is measured. Real number in the range 0-1.",
      "user": "prashantp@qubole.com",
      "dateUpdated": "Jan 18, 2018 11:06:28 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": false,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1516019440778_-1534706536",
      "id": "20180115-123040_1534675501_q_2EST7KBRP31516019108",
      "dateCreated": "Jan 15, 2018 12:30:40 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "bAbI_RNN_KERAS_TENSORFLOW",
  "id": "2EST7KBRP31516019108",
  "angularObjects": {
    "2D7M1HZP5932661518613479637:shared_process": [],
    "2D6B43M8G932661515762929145:shared_process": [],
    "2D35KXZZK932661515762929137:shared_process": []
  },
  "config": {
    "isDashboard": false
  },
  "info": {},
  "source": "FCN"
}