{
  "paragraphs": [
    {
      "text": "os.environ[\"KERAS_BACKEND\"] \u003d \"theano\"\nos.environ[\u0027THEANO_FLAGS\u0027] \u003d \u0027floatX\u003dfloat32,device\u003dcuda,dnn.library_path\u003d/usr/local/cuda/lib64,dnn.include_path\u003d/usr/local/cuda/include/\u0027\nos.environ[\u0027MKL_THREADING_LAYER\u0027] \u003d \"GNU\"",
      "user": "prashantp@qubole.com",
      "dateUpdated": "May 25, 2018 7:21:35 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [],
        "numCompletedTasks": 0,
        "numTasks": 0,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1516357900302_1487390355",
      "id": "20180119-103140_151148002_q_GFQ92UAV3A1527183822",
      "dateCreated": "Jan 19, 2018 10:31:40 AM",
      "dateSubmitted": "May 24, 2018 5:44:21 PM",
      "dateStarted": "May 24, 2018 5:44:32 PM",
      "dateFinished": "May 24, 2018 5:44:53 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "# Confirming backend is changed to Theano\nfrom __future__ import print_function\nimport keras\nfrom keras import backend as K\nprint(K.backend())",
      "dateUpdated": "Jan 22, 2018 7:19:56 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [],
        "numCompletedTasks": 0,
        "numTasks": 0,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1516357907341_1792126808",
      "id": "20180119-103147_1343880080_q_GFQ92UAV3A1527183822",
      "dateCreated": "Jan 19, 2018 10:31:47 AM",
      "dateSubmitted": "May 24, 2018 5:44:32 PM",
      "dateStarted": "May 24, 2018 5:44:54 PM",
      "dateFinished": "May 24, 2018 5:44:54 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "from __future__ import print_function\nimport numpy as np\n\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Embedding\nfrom keras.layers import GlobalAveragePooling1D\nfrom keras.datasets import imdb\n\n\ndef create_ngram_set(input_list, ngram_value\u003d2):\n    \"\"\"\n    Extract a set of n-grams from a list of integers.\n    \u003e\u003e\u003e create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value\u003d2)\n    {(4, 9), (4, 1), (1, 4), (9, 4)}\n    \u003e\u003e\u003e create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value\u003d3)\n    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n    \"\"\"\n    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n\n\ndef add_ngram(sequences, token_indice, ngram_range\u003d2):\n    \"\"\"\n    Augment the input list of list (sequences) by appending n-grams values.\n    Example: adding bi-gram\n    \u003e\u003e\u003e sequences \u003d [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n    \u003e\u003e\u003e token_indice \u003d {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n    \u003e\u003e\u003e add_ngram(sequences, token_indice, ngram_range\u003d2)\n    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n    Example: adding tri-gram\n    \u003e\u003e\u003e sequences \u003d [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n    \u003e\u003e\u003e token_indice \u003d {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n    \u003e\u003e\u003e add_ngram(sequences, token_indice, ngram_range\u003d3)\n    [[1, 3, 4, 5, 1337], [1, 3, 7, 9, 2, 1337, 2018]]\n    \"\"\"\n    new_sequences \u003d []\n    for input_list in sequences:\n        new_list \u003d input_list[:]\n        for i in range(len(new_list) - ngram_range + 1):\n            for ngram_value in range(2, ngram_range + 1):\n                ngram \u003d tuple(new_list[i:i + ngram_value])\n                if ngram in token_indice:\n                    new_list.append(token_indice[ngram])\n        new_sequences.append(new_list)\n\n    return new_sequences\n\n# Set parameters:\n# ngram_range \u003d 2 will add bi-grams features\nngram_range \u003d 1\nmax_features \u003d 20000\nmaxlen \u003d 400\nbatch_size \u003d 32\nembedding_dims \u003d 50\nepochs \u003d 2\n\nprint(\u0027Loading data...\u0027)\n(x_train, y_train), (x_test, y_test) \u003d imdb.load_data(num_words\u003dmax_features)\nprint(len(x_train), \u0027train sequences\u0027)\nprint(len(x_test), \u0027test sequences\u0027)\nprint(\u0027Average train sequence length: {}\u0027.format(np.mean(list(map(len, x_train)), dtype\u003dint)))\nprint(\u0027Average test sequence length: {}\u0027.format(np.mean(list(map(len, x_test)), dtype\u003dint)))\n\nif ngram_range \u003e 1:\n    print(\u0027Adding {}-gram features\u0027.format(ngram_range))\n    # Create set of unique n-gram from the training set.\n    ngram_set \u003d set()\n    for input_list in x_train:\n        for i in range(2, ngram_range + 1):\n            set_of_ngram \u003d create_ngram_set(input_list, ngram_value\u003di)\n            ngram_set.update(set_of_ngram)\n\n    # Dictionary mapping n-gram token to a unique integer.\n    # Integer values are greater than max_features in order\n    # to avoid collision with existing features.\n    start_index \u003d max_features + 1\n    token_indice \u003d {v: k + start_index for k, v in enumerate(ngram_set)}\n    indice_token \u003d {token_indice[k]: k for k in token_indice}\n\n    # max_features is the highest integer that could be found in the dataset.\n    max_features \u003d np.max(list(indice_token.keys())) + 1\n\n    # Augmenting x_train and x_test with n-grams features\n    x_train \u003d add_ngram(x_train, token_indice, ngram_range)\n    x_test \u003d add_ngram(x_test, token_indice, ngram_range)\n    print(\u0027Average train sequence length: {}\u0027.format(np.mean(list(map(len, x_train)), dtype\u003dint)))\n    print(\u0027Average test sequence length: {}\u0027.format(np.mean(list(map(len, x_test)), dtype\u003dint)))\n\nprint(\u0027Pad sequences (samples x time)\u0027)\nx_train \u003d sequence.pad_sequences(x_train, maxlen\u003dmaxlen)\nx_test \u003d sequence.pad_sequences(x_test, maxlen\u003dmaxlen)\nprint(\u0027x_train shape:\u0027, x_train.shape)\nprint(\u0027x_test shape:\u0027, x_test.shape)\n\nprint(\u0027Build model...\u0027)\nmodel \u003d Sequential()\n\n# we start off with an efficient embedding layer which maps\n# our vocab indices into embedding_dims dimensions\nmodel.add(Embedding(max_features,\n                    embedding_dims,\n                    input_length\u003dmaxlen))\n\n# we add a GlobalAveragePooling1D, which will average the embeddings\n# of all words in the document\nmodel.add(GlobalAveragePooling1D())\n\n# We project onto a single unit output layer, and squash it with a sigmoid:\nmodel.add(Dense(1, activation\u003d\u0027sigmoid\u0027))\n\nmodel.compile(loss\u003d\u0027binary_crossentropy\u0027,\n              optimizer\u003d\u0027adam\u0027,\n              metrics\u003d[\u0027accuracy\u0027])\n\nmodel.fit(x_train, y_train,\n          batch_size\u003dbatch_size,\n          epochs\u003depochs,\n          validation_data\u003d(x_test, y_test))",
      "dateUpdated": "Jan 22, 2018 7:19:56 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [],
        "numCompletedTasks": 0,
        "numTasks": 0,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1516095486201_649557388",
      "id": "20180116-093806_1995982867_q_GFQ92UAV3A1527183822",
      "dateCreated": "Jan 16, 2018 9:38:06 AM",
      "dateSubmitted": "May 24, 2018 5:44:54 PM",
      "dateStarted": "May 24, 2018 5:44:54 PM",
      "dateFinished": "May 24, 2018 5:44:55 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "Input : IMDB Dataset. It contains 25000 movie reviews in each training as well as testing batch.\n    Each batch contains a dictionary with the following elements:-\n    \n    data -- an array of 25000 lists for each training as well as testing batch. Each element in the list has different length and is a review encoded as a sequence of word              indexes(integers).\n    labels -- an array of 25000 numbers either 0 or 1 for both training as well as testing batch. The number at index i indicates the label of the ith image in the                      array data.\n\nOutput : Accuracy of the classification done by the trained model.  Real No. Between 0-1.",
      "user": "prashantp@qubole.com",
      "dateUpdated": "Jan 22, 2018 7:19:56 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": false,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1516099320473_1243699393",
      "id": "20180116-104200_1205061388_q_GFQ92UAV3A1527183822",
      "dateCreated": "Jan 16, 2018 10:42:00 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "dateUpdated": "Jan 22, 2018 7:19:56 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1516605513266_2069999351",
      "id": "20180122-071833_2104781247_q_GFQ92UAV3A1527183822",
      "dateCreated": "Jan 22, 2018 7:18:33 AM",
      "dateSubmitted": "May 24, 2018 5:44:55 PM",
      "dateStarted": "May 24, 2018 5:44:55 PM",
      "dateFinished": "May 24, 2018 5:44:55 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "imdb_fasttext",
  "id": "GFQ92UAV3A1527183822",
  "angularObjects": {
    "2DDDJQ5MF1090031527183841055:shared_process": [],
    "2DEK3QV851090031527168688484:shared_process": [],
    "2DENTX6JV1090031527168688466:shared_process": [],
    "2DDED8P751090031527168688477:shared_process": [],
    "2DDBWFX441090031527168688480:shared_process": []
  },
  "config": {
    "isDashboard": false,
    "defaultLang": "pyspark"
  },
  "info": {},
  "source": "FCN"
}