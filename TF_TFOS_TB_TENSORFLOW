{
  "paragraphs": [
    {
      "text": "import os\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n \nimport argparse\nimport os\nimport sys\n\n \nimport tensorflow as tf\nfrom qdlpy import DLD\n \nfrom tensorflow.examples.tutorials.mnist import input_data\n \ndef train():\n  # Import data\n  mnist \u003d input_data.read_data_sets(FLAGS.data_dir,\n                                    fake_data\u003dFLAGS.fake_data)\n \n  sess \u003d tf.InteractiveSession()\n  # Create a multilayer model.\n \n  # Input placeholders\n  with tf.name_scope(\u0027input\u0027):\n    x \u003d tf.placeholder(tf.float32, [None, 784], name\u003d\u0027x-input\u0027)\n    y_ \u003d tf.placeholder(tf.int64, [None], name\u003d\u0027y-input\u0027)\n \n  with tf.name_scope(\u0027input_reshape\u0027):\n    image_shaped_input \u003d tf.reshape(x, [-1, 28, 28, 1])\n    tf.summary.image(\u0027input\u0027, image_shaped_input, 10)\n \n  # We can\u0027t initialize these variables to 0 - the network will get stuck.\n  def weight_variable(shape):\n    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n    initial \u003d tf.truncated_normal(shape, stddev\u003d0.1)\n    return tf.Variable(initial)\n \n  def bias_variable(shape):\n    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n    initial \u003d tf.constant(0.1, shape\u003dshape)\n    return tf.Variable(initial)\n \n  def variable_summaries(var):\n    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n    with tf.name_scope(\u0027summaries\u0027):\n      mean \u003d tf.reduce_mean(var)\n      tf.summary.scalar(\u0027mean\u0027, mean)\n      with tf.name_scope(\u0027stddev\u0027):\n        stddev \u003d tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n      tf.summary.scalar(\u0027stddev\u0027, stddev)\n      tf.summary.scalar(\u0027max\u0027, tf.reduce_max(var))\n      tf.summary.scalar(\u0027min\u0027, tf.reduce_min(var))\n      tf.summary.histogram(\u0027histogram\u0027, var)\n \n  def nn_layer(input_tensor, input_dim, output_dim, layer_name, act\u003dtf.nn.relu):\n    print(\"inside nn layer method %s %s %s %s %s\", input_tensor, input_dim, output_dim, layer_name, act)\n    \"\"\"Reusable code for making a simple neural net layer.\n \n    It does a matrix multiply, bias add, and then uses ReLU to nonlinearize.\n    It also sets up name scoping so that the resultant graph is easy to read,\n    and adds a number of summary ops.\n    \"\"\"\n    # Adding a name scope ensures logical grouping of the layers in the graph.\n    with tf.name_scope(layer_name):\n      # This Variable will hold the state of the weights for the layer\n      with tf.name_scope(\u0027weights\u0027):\n        weights \u003d weight_variable([input_dim, output_dim])\n        variable_summaries(weights)\n      with tf.name_scope(\u0027biases\u0027):\n        biases \u003d bias_variable([output_dim])\n        variable_summaries(biases)\n      with tf.name_scope(\u0027Wx_plus_b\u0027):\n        preactivate \u003d tf.matmul(input_tensor, weights) + biases\n        tf.summary.histogram(\u0027pre_activations\u0027, preactivate)\n      activations \u003d act(preactivate, name\u003d\u0027activation\u0027)\n      tf.summary.histogram(\u0027activations\u0027, activations)\n      return activations\n  hidden1 \u003d nn_layer(x, 784, 500, \u0027layer1\u0027)\n \n  with tf.name_scope(\u0027dropout\u0027):\n    keep_prob \u003d tf.placeholder(tf.float32)\n    tf.summary.scalar(\u0027dropout_keep_probability\u0027, keep_prob)\n    dropped \u003d tf.nn.dropout(hidden1, keep_prob)\n \n  # Do not apply softmax activation yet, see below.\n  y \u003d nn_layer(dropped, 500, 10, \u0027layer2\u0027, act\u003dtf.identity)\n \n  with tf.name_scope(\u0027cross_entropy\u0027):\n    # The raw formulation of cross-entropy,\n    #\n    # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.softmax(y)),\n    #                               reduction_indices\u003d[1]))\n    #\n    # can be numerically unstable.\n    #\n    # So here we use tf.losses.sparse_softmax_cross_entropy on the\n    # raw logit outputs of the nn_layer above, and then average across\n    # the batch.\n    with tf.name_scope(\u0027total\u0027):\n      cross_entropy \u003d tf.losses.sparse_softmax_cross_entropy(\n          labels\u003dy_, logits\u003dy)\n  tf.summary.scalar(\u0027cross_entropy\u0027, cross_entropy)\n \n  with tf.name_scope(\u0027train\u0027):\n    train_step \u003d tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(\n        cross_entropy)\n \n  with tf.name_scope(\u0027accuracy\u0027):\n    with tf.name_scope(\u0027correct_prediction\u0027):\n      correct_prediction \u003d tf.equal(tf.argmax(y, 1), y_)\n    with tf.name_scope(\u0027accuracy\u0027):\n      accuracy \u003d tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n  tf.summary.scalar(\u0027accuracy\u0027, accuracy)\n \n  # Merge all the summaries and write them out to\n  # /tmp/tensorflow/mnist/logs/mnist_with_summaries (by default)\n  merged \u003d tf.summary.merge_all()\n  train_writer \u003d tf.summary.FileWriter(FLAGS.log_dir + \u0027/train\u0027, sess.graph)\n  test_writer \u003d tf.summary.FileWriter(FLAGS.log_dir + \u0027/test\u0027)\n  tf.global_variables_initializer().run()\n  # Train the model, and also write summaries.\n  # Every 10th step, measure test-set accuracy, and write test summaries\n  # All other steps, run train_step on training data, \u0026 add training summaries\n \n  def feed_dict(train):\n    \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n    if train or FLAGS.fake_data:\n      xs, ys \u003d mnist.train.next_batch(100, fake_data\u003dFLAGS.fake_data)\n      k \u003d FLAGS.dropout\n    else:\n      xs, ys \u003d mnist.test.images, mnist.test.labels\n      k \u003d 1.0\n    return {x: xs, y_: ys, keep_prob: k}\n \n  for i in range(FLAGS.max_steps):\n    if i % 10 \u003d\u003d 0:  # Record summaries and test-set accuracy\n      summary, acc \u003d sess.run([merged, accuracy], feed_dict\u003dfeed_dict(False))\n      test_writer.add_summary(summary, i)\n    else:  # Record train set summaries, and train\n      if i % 100 \u003d\u003d 99:  # Record execution stats\n        run_options \u003d tf.RunOptions(trace_level\u003dtf.RunOptions.FULL_TRACE)\n        run_metadata \u003d tf.RunMetadata()\n        summary, _ \u003d sess.run([merged, train_step],\n                              feed_dict\u003dfeed_dict(True),\n                              options\u003drun_options,\n                              run_metadata\u003drun_metadata)\n        train_writer.add_run_metadata(run_metadata, \u0027step%03d\u0027 % i)\n        train_writer.add_summary(summary, i)\n      else:  # Record a summary\n        summary, _ \u003d sess.run([merged, train_step], feed_dict\u003dfeed_dict(True))\n        train_writer.add_summary(summary, i)\n  train_writer.close()\n  test_writer.close()\n \nparser \u003d argparse.ArgumentParser()\nparser.add_argument(\u0027--fake_data\u0027, nargs\u003d\u0027?\u0027, const\u003dTrue, type\u003dbool,\n                  default\u003dFalse,\n                  help\u003d\u0027If true, uses fake data for unit testing.\u0027)\nparser.add_argument(\u0027--max_steps\u0027, type\u003dint, default\u003d1000,\n                  help\u003d\u0027Number of steps to run trainer.\u0027)\nparser.add_argument(\u0027--learning_rate\u0027, type\u003dfloat, default\u003d0.001,\n                  help\u003d\u0027Initial learning rate\u0027)\nparser.add_argument(\u0027--dropout\u0027, type\u003dfloat, default\u003d0.9,\n                  help\u003d\u0027Keep probability for training dropout.\u0027)\nparser.add_argument(\n  \u0027--data_dir\u0027,\n  type\u003dstr,\n  default\u003dos.path.join(os.getenv(\u0027TEST_TMPDIR\u0027, \u0027/tmp\u0027),\n                       \u0027tensorflow/mnist/input_data\u0027),\n  help\u003d\u0027Directory for storing input data\u0027)\nparser.add_argument(\u0027--log_dir\u0027, type\u003dstr, default\u003dDLD.register([\u0027tensorboard\u0027], \"mnist_demo_1\")[\u0027tensorboard_hdfs_path\u0027])\nFLAGS, unparsed \u003d parser.parse_known_args([])\ntrain()",
      "user": "somyak@qubole.com",
      "dateUpdated": "Feb 22, 2018 10:30:04 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [],
        "numCompletedTasks": 0,
        "numTasks": 0,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1519295355208_451703247",
      "id": "20180222-102915_1674674690_q_S6VPFDU1FM1519295354",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\n\n+ key\u003d_master_ip\n+ echo 10.0.0.246\nSuccessfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\nExtracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n\n2018-02-22 10:31:17.583275: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\n\n2018-02-22 10:31:18.159851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n\n\n\n2018-02-22 10:31:18.160120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: \nname: GRID K520 major: 3 minor: 0 memoryClockRate(GHz): 0.797\npciBusID: 0000:00:03.0\ntotalMemory: 3.94GiB freeMemory: 3.90GiB\n2018-02-22 10:31:18.160152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1168] Ignoring visible gpu device (device: 0, name: GRID K520, pci bus id: 0000:00:03.0, compute capability: 3.0) with Cuda compute capability 3.0. The minimum required Cuda capability is 3.5.\ninside nn layer method %s %s %s %s %s Tensor(\"input/x-input:0\", shape\u003d(?, 784), dtype\u003dfloat32) 784 500 layer1 \u003cfunction relu at 0x7f5f65e25840\u003e\ninside nn layer method %s %s %s %s %s Tensor(\"dropout/dropout/mul:0\", shape\u003d(?, 500), dtype\u003dfloat32) 500 10 layer2 \u003cfunction identity at 0x7f5f67077c80\u003e\nlog4j:WARN No such property [rollingPolicy] in org.apache.log4j.RollingFileAppender.\n\n\nSLF4J: Class path contains multiple SLF4J bindings.\n\n\n\nSLF4J: Found binding in [jar:file:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/lib/qubole/packages/zeppelin-2.2/zeppelin/interpreter/spark/zeppelin-spark_2.10-0.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n\n2018-02-22 10:31:33.838958: I tensorflow/stream_executor/dso_loader.cc:139] successfully opened CUDA library libcupti.so.9.0 locally\n"
      },
      "dateCreated": "Feb 22, 2018 10:29:15 AM",
      "dateStarted": "Feb 22, 2018 10:30:04 AM",
      "dateFinished": "Feb 22, 2018 10:32:25 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1519295404575_1207005853",
      "id": "20180222-103004_194157941_q_S6VPFDU1FM1519295354",
      "dateCreated": "Feb 22, 2018 10:30:04 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "tf_with_tensorboard",
  "id": "S6VPFDU1FM1519295354",
  "angularObjects": {
    "2D71SY4WP441519288598642:shared_process": [],
    "2D9PNMS6F441519288656400:shared_process": [],
    "2D63N5WBH441519288598645:shared_process": [],
    "2D7ZZM92C441519288598648:shared_process": []
  },
  "config": {
    "isDashboard": false
  },
  "info": {},
  "source": "FCN"
}