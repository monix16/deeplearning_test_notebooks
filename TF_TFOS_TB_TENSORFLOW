{
  "paragraphs": [
    {
      "text": "import os\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n \nimport argparse\nimport os\nimport sys\n\n \nimport tensorflow as tf\nfrom qdlpy import DLD\n \nfrom tensorflow.examples.tutorials.mnist import input_data\n \ndef train():\n  # Import data\n  mnist \u003d input_data.read_data_sets(FLAGS.data_dir,\n                                    fake_data\u003dFLAGS.fake_data)\n \n  sess \u003d tf.InteractiveSession()\n  # Create a multilayer model.\n \n  # Input placeholders\n  with tf.name_scope(\u0027input\u0027):\n    x \u003d tf.placeholder(tf.float32, [None, 784], name\u003d\u0027x-input\u0027)\n    y_ \u003d tf.placeholder(tf.int64, [None], name\u003d\u0027y-input\u0027)\n \n  with tf.name_scope(\u0027input_reshape\u0027):\n    image_shaped_input \u003d tf.reshape(x, [-1, 28, 28, 1])\n    tf.summary.image(\u0027input\u0027, image_shaped_input, 10)\n \n  # We can\u0027t initialize these variables to 0 - the network will get stuck.\n  def weight_variable(shape):\n    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n    initial \u003d tf.truncated_normal(shape, stddev\u003d0.1)\n    return tf.Variable(initial)\n \n  def bias_variable(shape):\n    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n    initial \u003d tf.constant(0.1, shape\u003dshape)\n    return tf.Variable(initial)\n \n  def variable_summaries(var):\n    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n    with tf.name_scope(\u0027summaries\u0027):\n      mean \u003d tf.reduce_mean(var)\n      tf.summary.scalar(\u0027mean\u0027, mean)\n      with tf.name_scope(\u0027stddev\u0027):\n        stddev \u003d tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n      tf.summary.scalar(\u0027stddev\u0027, stddev)\n      tf.summary.scalar(\u0027max\u0027, tf.reduce_max(var))\n      tf.summary.scalar(\u0027min\u0027, tf.reduce_min(var))\n      tf.summary.histogram(\u0027histogram\u0027, var)\n \n  def nn_layer(input_tensor, input_dim, output_dim, layer_name, act\u003dtf.nn.relu):\n    print(\"inside nn layer method %s %s %s %s %s\", input_tensor, input_dim, output_dim, layer_name, act)\n    \"\"\"Reusable code for making a simple neural net layer.\n \n    It does a matrix multiply, bias add, and then uses ReLU to nonlinearize.\n    It also sets up name scoping so that the resultant graph is easy to read,\n    and adds a number of summary ops.\n    \"\"\"\n    # Adding a name scope ensures logical grouping of the layers in the graph.\n    with tf.name_scope(layer_name):\n      # This Variable will hold the state of the weights for the layer\n      with tf.name_scope(\u0027weights\u0027):\n        weights \u003d weight_variable([input_dim, output_dim])\n        variable_summaries(weights)\n      with tf.name_scope(\u0027biases\u0027):\n        biases \u003d bias_variable([output_dim])\n        variable_summaries(biases)\n      with tf.name_scope(\u0027Wx_plus_b\u0027):\n        preactivate \u003d tf.matmul(input_tensor, weights) + biases\n        tf.summary.histogram(\u0027pre_activations\u0027, preactivate)\n      activations \u003d act(preactivate, name\u003d\u0027activation\u0027)\n      tf.summary.histogram(\u0027activations\u0027, activations)\n      return activations\n  hidden1 \u003d nn_layer(x, 784, 500, \u0027layer1\u0027)\n \n  with tf.name_scope(\u0027dropout\u0027):\n    keep_prob \u003d tf.placeholder(tf.float32)\n    tf.summary.scalar(\u0027dropout_keep_probability\u0027, keep_prob)\n    dropped \u003d tf.nn.dropout(hidden1, keep_prob)\n \n  # Do not apply softmax activation yet, see below.\n  y \u003d nn_layer(dropped, 500, 10, \u0027layer2\u0027, act\u003dtf.identity)\n \n  with tf.name_scope(\u0027cross_entropy\u0027):\n    # The raw formulation of cross-entropy,\n    #\n    # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.softmax(y)),\n    #                               reduction_indices\u003d[1]))\n    #\n    # can be numerically unstable.\n    #\n    # So here we use tf.losses.sparse_softmax_cross_entropy on the\n    # raw logit outputs of the nn_layer above, and then average across\n    # the batch.\n    with tf.name_scope(\u0027total\u0027):\n      cross_entropy \u003d tf.losses.sparse_softmax_cross_entropy(\n          labels\u003dy_, logits\u003dy)\n  tf.summary.scalar(\u0027cross_entropy\u0027, cross_entropy)\n \n  with tf.name_scope(\u0027train\u0027):\n    train_step \u003d tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(\n        cross_entropy)\n \n  with tf.name_scope(\u0027accuracy\u0027):\n    with tf.name_scope(\u0027correct_prediction\u0027):\n      correct_prediction \u003d tf.equal(tf.argmax(y, 1), y_)\n    with tf.name_scope(\u0027accuracy\u0027):\n      accuracy \u003d tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n  tf.summary.scalar(\u0027accuracy\u0027, accuracy)\n \n  # Merge all the summaries and write them out to\n  # /tmp/tensorflow/mnist/logs/mnist_with_summaries (by default)\n  merged \u003d tf.summary.merge_all()\n  train_writer \u003d tf.summary.FileWriter(FLAGS.log_dir + \u0027/train\u0027, sess.graph)\n  test_writer \u003d tf.summary.FileWriter(FLAGS.log_dir + \u0027/test\u0027)\n  tf.global_variables_initializer().run()\n  # Train the model, and also write summaries.\n  # Every 10th step, measure test-set accuracy, and write test summaries\n  # All other steps, run train_step on training data, \u0026 add training summaries\n \n  def feed_dict(train):\n    \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n    if train or FLAGS.fake_data:\n      xs, ys \u003d mnist.train.next_batch(100, fake_data\u003dFLAGS.fake_data)\n      k \u003d FLAGS.dropout\n    else:\n      xs, ys \u003d mnist.test.images, mnist.test.labels\n      k \u003d 1.0\n    return {x: xs, y_: ys, keep_prob: k}\n \n  for i in range(FLAGS.max_steps):\n    if i % 10 \u003d\u003d 0:  # Record summaries and test-set accuracy\n      summary, acc \u003d sess.run([merged, accuracy], feed_dict\u003dfeed_dict(False))\n      test_writer.add_summary(summary, i)\n    else:  # Record train set summaries, and train\n      if i % 100 \u003d\u003d 99:  # Record execution stats\n        run_options \u003d tf.RunOptions(trace_level\u003dtf.RunOptions.FULL_TRACE)\n        run_metadata \u003d tf.RunMetadata()\n        summary, _ \u003d sess.run([merged, train_step],\n                              feed_dict\u003dfeed_dict(True),\n                              options\u003drun_options,\n                              run_metadata\u003drun_metadata)\n        train_writer.add_run_metadata(run_metadata, \u0027step%03d\u0027 % i)\n        train_writer.add_summary(summary, i)\n      else:  # Record a summary\n        summary, _ \u003d sess.run([merged, train_step], feed_dict\u003dfeed_dict(True))\n        train_writer.add_summary(summary, i)\n  train_writer.close()\n  test_writer.close()\n \nparser \u003d argparse.ArgumentParser()\nparser.add_argument(\u0027--fake_data\u0027, nargs\u003d\u0027?\u0027, const\u003dTrue, type\u003dbool,\n                  default\u003dFalse,\n                  help\u003d\u0027If true, uses fake data for unit testing.\u0027)\nparser.add_argument(\u0027--max_steps\u0027, type\u003dint, default\u003d1000,\n                  help\u003d\u0027Number of steps to run trainer.\u0027)\nparser.add_argument(\u0027--learning_rate\u0027, type\u003dfloat, default\u003d0.001,\n                  help\u003d\u0027Initial learning rate\u0027)\nparser.add_argument(\u0027--dropout\u0027, type\u003dfloat, default\u003d0.9,\n                  help\u003d\u0027Keep probability for training dropout.\u0027)\nparser.add_argument(\n  \u0027--data_dir\u0027,\n  type\u003dstr,\n  default\u003dos.path.join(os.getenv(\u0027TEST_TMPDIR\u0027, \u0027/tmp\u0027),\n                       \u0027tensorflow/mnist/input_data\u0027),\n  help\u003d\u0027Directory for storing input data\u0027)\nparser.add_argument(\u0027--log_dir\u0027, type\u003dstr, default\u003dDLD.register([\u0027tensorboard\u0027], \"mnist_demo_1\")[\u0027tensorboard_hdfs_path\u0027])\nFLAGS, unparsed \u003d parser.parse_known_args([])\ntrain()",
      "user": "somyak@qubole.com",
      "dateUpdated": "Feb 21, 2018 12:54:53 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [],
        "numCompletedTasks": 0,
        "numTasks": 0,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1519116980697_1961555611",
      "id": "20180220-085620_1357529190_q_REHQJ9HEG71519116979",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 \u003d\u003d np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n\n\n+ key\u003d_master_ip\n+ echo 10.0.0.72\nSuccessfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\nExtracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\nSuccessfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\nSuccessfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\nSuccessfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n\n2018-02-21 00:58:52.504764: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n\n2018-02-21 00:58:59.672192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n\n\n\n\n2018-02-21 00:58:59.672660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: \nname: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\npciBusID: 0000:00:1e.0\ntotalMemory: 15.77GiB freeMemory: 15.36GiB\n\n2018-02-21 00:58:59.672684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -\u003e (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)\ninside nn layer method %s %s %s %s %s Tensor(\"input/x-input:0\", shape\u003d(?, 784), dtype\u003dfloat32) 784 500 layer1 \u003cfunction relu at 0x7fb9e4233048\u003e\ninside nn layer method %s %s %s %s %s Tensor(\"dropout/dropout/mul:0\", shape\u003d(?, 500), dtype\u003dfloat32) 500 10 layer2 \u003cfunction identity at 0x7fb9e4788488\u003e\nlog4j:WARN No such property [rollingPolicy] in org.apache.log4j.RollingFileAppender.\n\n\nSLF4J: Class path contains multiple SLF4J bindings.\n\n\n\nSLF4J: Found binding in [jar:file:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/lib/qubole/packages/zeppelin-2.1/zeppelin/interpreter/spark/zeppelin-spark_2.10-0.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n\n2018-02-21 01:00:38.050764: I tensorflow/stream_executor/dso_loader.cc:139] successfully opened CUDA library libcupti.so.9.0 locally\n"
      },
      "dateCreated": "Feb 20, 2018 8:56:20 AM",
      "dateStarted": "Feb 21, 2018 12:54:53 AM",
      "dateFinished": "Feb 21, 2018 1:01:20 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\ncd  /usr/lib/deep-learning/examples/tfos/mnist\nunzip /usr/lib/deep-learning/examples/tfos/mnist/mnist.zip\n/usr/lib/spark/bin/spark-submit \\\n--master yarn \\\n--num-executors 2 \\\n--archives /usr/lib/deep-learning/examples/tfos/mnist/mnist.zip#mnist \\\n--jars /usr/lib/deep-learning/tensorflow-hadoop-plugin/tensorflow-hadoop-1.0-SNAPSHOT.jar \\\n/usr/lib/deep-learning/examples/tfos/mnist/mnist_data_setup.py \\\n--output /deep-learning/examples/tfos/mnist_data/tfr \\\n--format tfr\n",
      "user": "somyak@qubole.com",
      "dateUpdated": "Feb 21, 2018 1:03:21 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1519116990872_-377332953",
      "id": "20180220-085630_1043681846_q_REHQJ9HEG71519116979",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "Archive:  /usr/lib/deep-learning/examples/tfos/mnist/mnist.zip\n  inflating: t10k-images-idx3-ubyte.gz  \n extracting: t10k-labels-idx1-ubyte.gz  \n  inflating: train-images-idx3-ubyte.gz  \n extracting: train-labels-idx1-ubyte.gz  \nWarning: Skipping download of JAR file:/usr/lib/deep-learning/tensorflow-hadoop-plugin/tensorflow-hadoop-1.0-SNAPSHOT.jar. Scheme file not supported\nDefault value for autoscalingv2 : false\nmaxExecutors have been set to default : 2\n18/02/21 01:03:34 main INFO Utils: Registered signal handlers for exception exit hook [TERM, HUP, INT]\n/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 \u003d\u003d np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nargs: Namespace(format\u003d\u0027tfr\u0027, num_partitions\u003d10, output\u003d\u0027/deep-learning/examples/tfos/mnist_data/tfr\u0027, read\u003dFalse, verify\u003dFalse)\n18/02/21 01:05:12 Thread-4 INFO SparkContext: Running Spark version 2.1.1\n18/02/21 01:05:12 Thread-4 INFO SparkContext: Spark configuration:\nspark.R.cmd\u003d/usr/lib/a-4.2.0-r-3.3.2/bin/R\nspark.app.name\u003dmnist_parallelize\nspark.authenticate\u003dfalse\nspark.authenticate.enableSaslEncryption\u003dfalse\nspark.driver.cores\u003d14\nspark.driver.extraClassPath\u003d/usr/lib/spark/conf\nspark.driver.extraJavaOptions\u003d-Djava.net.preferIPv4Stack\u003dtrue\nspark.driver.extraLibraryPath\u003d/usr/lib/hadoop2/lib/native\nspark.driver.memory\u003d38g\nspark.dynamicAllocation.enabled\u003dfalse\nspark.dynamicAllocation.maxExecutors\u003d2\nspark.dynamicAllocation.minExecutors\u003d2\nspark.eventLog.compress\u003dtrue\nspark.eventLog.dir\u003dhdfs://ec2-174-129-98-222.compute-1.amazonaws.com:9000/spark-history\nspark.eventLog.enabled\u003dtrue\nspark.executor.cores\u003d1\nspark.executor.extraJavaOptions\u003d-Djava.net.preferIPv4Stack\u003dtrue\nspark.executor.instances\u003d2\nspark.executor.memory\u003d38g\nspark.hadoop.hive.qubole.consistent.loadpartition\u003dfalse\nspark.hadoop.mapred.output.committer.class\u003dorg.apache.hadoop.mapred.DirectFileOutputCommitter\nspark.hadoop.mapreduce.use.directfileoutputcommitter\u003dtrue\nspark.hadoop.spark.sql.parquet.output.committer.class\u003dorg.apache.spark.sql.parquet.DirectParquetOutputCommitter\nspark.history.fs.update.interval\u003d10\nspark.history.retainedApplications\u003d5\nspark.logConf\u003dtrue\nspark.master\u003dyarn\nspark.network.sasl.serverAlwaysEncrypt\u003dfalse\nspark.pyspark.python\u003d/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/bin/python\nspark.qubole.eventLog.hdfs.async\u003dfalse\nspark.qubole.sendsql\u003dfalse\nspark.qubole.spotloss.handle\u003dfalse\nspark.qubole.sql.hive.useDirectWrites\u003dfalse\nspark.rdd.compress\u003dTrue\nspark.scheduler.listenerbus.eventqueue.size\u003d20000\nspark.serializer.objectStreamReset\u003d100\nspark.shuffle.service.enabled\u003dfalse\nspark.speculation\u003dfalse\nspark.sql.hive.metastore.jars\u003d/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/etc/hadoop:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/common/lib/*:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/common/*:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/hdfs:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/hdfs/lib/*:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/hdfs/*:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/yarn/lib/*:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/yarn/*:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/mapreduce/*:/share/hadoop/tools:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/tools/lib/*:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/tools/*:/share/hadoop/qubole:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/qubole/*:/contrib/capacity-scheduler/*.jar:/usr/lib/spark/lib/hive2/*\nspark.sql.hive.metastore.version\u003d0.13.1\nspark.sql.qubole.catalyst.normalizePredicates\u003dfalse\nspark.sql.qubole.handleCommentsWithSemicolon\u003dfalse\nspark.sql.qubole.metrics.enable\u003dfalse\nspark.sql.qubole.partitionDiscoverer\u003dfalse\nspark.sql.qubole.recover.partitions\u003dfalse\nspark.sql.qubole.split.computation\u003dfalse\nspark.sql.streaming.showStreamingTab\u003dfalse\nspark.submit.deployMode\u003dclient\nspark.ui.retainedJobs\u003d33\nspark.ui.retainedStages\u003d100\nspark.yarn.dist.archives\u003dfile:/usr/lib/deep-learning/examples/tfos/mnist/mnist.zip#mnist\nspark.yarn.dist.jars\u003dfile:/usr/lib/deep-learning/tensorflow-hadoop-plugin/tensorflow-hadoop-1.0-SNAPSHOT.jar\nspark.yarn.driver.memoryOverhead\u003d9g\nspark.yarn.executor.memoryOverhead\u003d9g\nspark.yarn.historyServer.address\u003dec2-174-129-98-222.compute-1.amazonaws.com:18080\nspark.yarn.isPython\u003dtrue\nspark.yarn.maxAppAttempts\u003d1\n18/02/21 01:05:12 Thread-4 INFO SecurityManager: Changing view acls to: root\n18/02/21 01:05:12 Thread-4 INFO SecurityManager: Changing modify acls to: root\n18/02/21 01:05:12 Thread-4 INFO SecurityManager: Changing view acls groups to: \n18/02/21 01:05:12 Thread-4 INFO SecurityManager: Changing modify acls groups to: \n18/02/21 01:05:12 Thread-4 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n18/02/21 01:05:13 Thread-4 INFO Utils: Successfully started service \u0027sparkDriver\u0027 on port 40461.\n18/02/21 01:05:13 Thread-4 INFO SparkEnv: Registering MapOutputTracker\n18/02/21 01:05:13 Thread-4 INFO SparkEnv: Registering BlockManagerMaster\n18/02/21 01:05:13 Thread-4 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n18/02/21 01:05:13 Thread-4 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n18/02/21 01:05:13 Thread-4 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-bc8b37d7-a33b-4dbc-8b77-cb687b2df96d\n18/02/21 01:05:14 Thread-4 INFO MemoryStore: MemoryStore started with capacity 20.1 GB\n18/02/21 01:05:14 Thread-4 INFO SparkEnv: Registering OutputCommitCoordinator\n18/02/21 01:05:14 Thread-4 INFO log: Logging initialized @101437ms\n18/02/21 01:05:14 Thread-4 INFO Server: jetty-9.2.z-SNAPSHOT\n18/02/21 01:05:14 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@56e4fd38{/jobs,null,AVAILABLE,@Spark}\n18/02/21 01:05:14 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@410baa07{/jobs/json,null,AVAILABLE,@Spark}\n18/02/21 01:05:14 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3de4b0e0{/jobs/job,null,AVAILABLE,@Spark}\n18/02/21 01:05:14 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@30852536{/jobs/job/json,null,AVAILABLE,@Spark}\n18/02/21 01:05:14 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@41791323{/stages,null,AVAILABLE,@Spark}\n18/02/21 01:05:14 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5d3ec299{/stages/json,null,AVAILABLE,@Spark}\n18/02/21 01:05:14 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@45c50a6e{/stages/stage,null,AVAILABLE,@Spark}\n18/02/21 01:05:14 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5c703b75{/stages/stage/json,null,AVAILABLE,@Spark}\n18/02/21 01:05:14 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5599dbd3{/stages/pool,null,AVAILABLE,@Spark}\n18/02/21 01:05:14 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7e98f1f3{/stages/pool/json,null,AVAILABLE,@Spark}\n18/02/21 01:05:14 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@13631234{/storage,null,AVAILABLE,@Spark}\n18/02/21 01:05:14 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4e1528be{/storage/json,null,AVAILABLE,@Spark}\n18/02/21 01:05:14 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@55c935ec{/storage/rdd,null,AVAILABLE,@Spark}\n18/02/21 01:05:14 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6c6741cf{/storage/rdd/json,null,AVAILABLE,@Spark}\n18/02/21 01:05:14 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@679e1cc5{/environment,null,AVAILABLE,@Spark}\n18/02/21 01:05:14 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@a98850{/environment/json,null,AVAILABLE,@Spark}\n18/02/21 01:05:14 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1cd3abfc{/executors,null,AVAILABLE,@Spark}\n18/02/21 01:05:14 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4a6ce890{/executors/json,null,AVAILABLE,@Spark}\n18/02/21 01:05:14 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5d03ad2c{/executors/threadDump,null,AVAILABLE,@Spark}\n18/02/21 01:05:14 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@112445d4{/executors/threadDump/json,null,AVAILABLE,@Spark}\n18/02/21 01:05:14 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@10a99047{/static,null,AVAILABLE,@Spark}\n18/02/21 01:05:14 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3dacc2aa{/,null,AVAILABLE,@Spark}\n18/02/21 01:05:14 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7d8672fe{/api,null,AVAILABLE,@Spark}\n18/02/21 01:05:14 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4e3f827f{/jobs/job/kill,null,AVAILABLE,@Spark}\n18/02/21 01:05:14 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@12c80857{/stages/stage/kill,null,AVAILABLE,@Spark}\n18/02/21 01:05:14 Thread-4 INFO ServerConnector: Started Spark@52e8cc74{HTTP/1.1}{0.0.0.0:4040}\n18/02/21 01:05:14 Thread-4 INFO Server: Started @101567ms\n18/02/21 01:05:14 Thread-4 INFO Utils: Successfully started service \u0027SparkUI\u0027 on port 4040.\n18/02/21 01:05:14 Thread-4 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.0.72:4040\n18/02/21 01:05:14 Thread-4 INFO deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS\n18/02/21 01:05:17 Thread-4 INFO TimelineClientImpl: Timeline service address: http://ec2-174-129-98-222.compute-1.amazonaws.com:8188/ws/v1/timeline/\n18/02/21 01:05:17 Thread-4 INFO RMProxy: Connecting to ResourceManager at ec2-174-129-98-222.compute-1.amazonaws.com/10.0.0.72:8032\n18/02/21 01:05:18 Thread-4 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n18/02/21 01:05:18 Thread-4 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (49971 MB per container)\n18/02/21 01:05:18 Thread-4 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n18/02/21 01:05:18 Thread-4 INFO Client: Setting up container launch context for our AM\n18/02/21 01:05:18 Thread-4 INFO Client: Setting up the launch environment for our AM container\n18/02/21 01:05:18 Thread-4 INFO Client: Preparing resources for our AM container\n18/02/21 01:05:19 Thread-4 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n18/02/21 01:05:36 Thread-4 INFO deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS\n18/02/21 01:05:36 Thread-4 INFO Client: Uploading resource file:/tmp/spark-6f44c423-c5a9-44c7-886a-9590f30257b8/__spark_libs__7285013461346424742.zip -\u003e hdfs://ec2-174-129-98-222.compute-1.amazonaws.com:9000/user/root/.sparkStaging/application_1519174037520_0003/__spark_libs__7285013461346424742.zip\n18/02/21 01:05:45 Thread-4 INFO deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS\n18/02/21 01:05:45 Thread-4 INFO Client: Uploading resource file:/usr/lib/deep-learning/tensorflow-hadoop-plugin/tensorflow-hadoop-1.0-SNAPSHOT.jar -\u003e hdfs://ec2-174-129-98-222.compute-1.amazonaws.com:9000/user/root/.sparkStaging/application_1519174037520_0003/tensorflow-hadoop-1.0-SNAPSHOT.jar\n18/02/21 01:05:45 Thread-4 INFO Client: Uploading resource file:/usr/lib/deep-learning/examples/tfos/mnist/mnist.zip#mnist -\u003e hdfs://ec2-174-129-98-222.compute-1.amazonaws.com:9000/user/root/.sparkStaging/application_1519174037520_0003/mnist.zip\n18/02/21 01:05:45 Thread-4 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -\u003e hdfs://ec2-174-129-98-222.compute-1.amazonaws.com:9000/user/root/.sparkStaging/application_1519174037520_0003/pyspark.zip\n18/02/21 01:05:45 Thread-4 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.4-src.zip -\u003e hdfs://ec2-174-129-98-222.compute-1.amazonaws.com:9000/user/root/.sparkStaging/application_1519174037520_0003/py4j-0.10.4-src.zip\n18/02/21 01:05:45 Thread-4 INFO Client: Uploading resource file:/tmp/spark-6f44c423-c5a9-44c7-886a-9590f30257b8/__spark_conf__5230814779096418014.zip -\u003e hdfs://ec2-174-129-98-222.compute-1.amazonaws.com:9000/user/root/.sparkStaging/application_1519174037520_0003/__spark_conf__.zip\n18/02/21 01:05:45 Thread-4 INFO SecurityManager: Changing view acls to: root\n18/02/21 01:05:45 Thread-4 INFO SecurityManager: Changing modify acls to: root\n18/02/21 01:05:45 Thread-4 INFO SecurityManager: Changing view acls groups to: \n18/02/21 01:05:45 Thread-4 INFO SecurityManager: Changing modify acls groups to: \n18/02/21 01:05:45 Thread-4 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n18/02/21 01:05:45 Thread-4 INFO Client: Submitting application application_1519174037520_0003 to ResourceManager\n18/02/21 01:05:45 Thread-4 INFO YarnClientImpl: Submitted application application_1519174037520_0003\n18/02/21 01:05:45 Thread-4 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1519174037520_0003 and attemptId None\n18/02/21 01:05:46 Thread-4 INFO Client: Application report for application_1519174037520_0003 (state: ACCEPTED)\n18/02/21 01:05:46 Thread-4 INFO Client: \n\t client token: N/A\n\t diagnostics: N/A\n\t ApplicationMaster host: N/A\n\t ApplicationMaster RPC port: -1\n\t queue: root.root\n\t start time: 1519175145198\n\t final status: UNDEFINED\n\t tracking URL: http://ec2-174-129-98-222.compute-1.amazonaws.com:8088/proxy/application_1519174037520_0003/?spark\u003dtrue\n\t user: root\n18/02/21 01:05:47 Thread-4 INFO Client: Application report for application_1519174037520_0003 (state: ACCEPTED)\n18/02/21 01:05:48 Thread-4 INFO Client: Application report for application_1519174037520_0003 (state: ACCEPTED)\n18/02/21 01:05:49 Thread-4 INFO Client: Application report for application_1519174037520_0003 (state: ACCEPTED)\n18/02/21 01:05:50 Thread-4 INFO Client: Application report for application_1519174037520_0003 (state: ACCEPTED)\n18/02/21 01:05:50 dispatcher-event-loop-5 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(null)\n18/02/21 01:05:50 dispatcher-event-loop-0 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: Received AMStart(container_1519174037520_0003_01_000001, ip-10-0-0-48.ec2.internal)\n18/02/21 01:05:50 dispatcher-event-loop-7 INFO YarnClientSchedulerBackend: addWebUIFilter: Setting spark.ui.proxyBase to /proxy/application_1519174037520_0003\n18/02/21 01:05:50 dispatcher-event-loop-7 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -\u003e ec2-174-129-98-222.compute-1.amazonaws.com, PROXY_URI_BASES -\u003e http://ec2-174-129-98-222.compute-1.amazonaws.com:8088/proxy/application_1519174037520_0003), /proxy/application_1519174037520_0003\n18/02/21 01:05:50 dispatcher-event-loop-7 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n18/02/21 01:05:51 Thread-4 INFO Client: Application report for application_1519174037520_0003 (state: RUNNING)\n18/02/21 01:05:51 Thread-4 INFO Client: \n\t client token: N/A\n\t diagnostics: N/A\n\t ApplicationMaster host: 10.0.0.48\n\t ApplicationMaster RPC port: 0\n\t queue: root.root\n\t start time: 1519175145198\n\t final status: UNDEFINED\n\t tracking URL: http://ec2-174-129-98-222.compute-1.amazonaws.com:8088/proxy/application_1519174037520_0003/?spark\u003dtrue\n\t user: root\n18/02/21 01:05:51 Thread-4 INFO YarnClientSchedulerBackend: Application application_1519174037520_0003 has started running.\n18/02/21 01:05:51 Thread-4 INFO Utils: Successfully started service \u0027org.apache.spark.network.netty.NettyBlockTransferService\u0027 on port 45855.\n18/02/21 01:05:51 Thread-4 INFO NettyBlockTransferService: Server created on 10.0.0.72:45855\n18/02/21 01:05:51 Thread-4 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n18/02/21 01:05:51 Thread-4 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.0.72, 45855, None)\n18/02/21 01:05:51 dispatcher-event-loop-2 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.0.72:45855 with 20.1 GB RAM, BlockManagerId(driver, 10.0.0.72, 45855, None)\n18/02/21 01:05:51 Thread-4 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.0.72, 45855, None)\n18/02/21 01:05:51 Thread-4 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.0.72, 45855, None)\n18/02/21 01:05:51 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1a06f73c{/metrics/json,null,AVAILABLE,@Spark}\n18/02/21 01:05:51 Thread-4 INFO deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS\n18/02/21 01:05:51 Thread-4 INFO EventLoggingListener: Logging events to hdfs://ec2-174-129-98-222.compute-1.amazonaws.com:9000/spark-history/application_1519174037520_0003.lz4\n18/02/21 01:05:51 Thread-4 INFO YarnScheduler$$anon$1: Adding shutdown hook for context org.apache.spark.SparkContext@7dcdb710.\n18/02/21 01:05:51 SparkListenerBus INFO ExecutorsListener: onAMStart(Some(container_1519174037520_0003_01_000001), Some(ip-10-0-0-48.ec2.internal))\n18/02/21 01:05:51 Thread-4 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)\n18/02/21 01:05:51 Thread-4 INFO YarnScheduler: YarnClientClusterScheduler.postStartHook done.\nExtracting /usr/lib/deep-learning/examples/tfos/mnist/train-images-idx3-ubyte.gz\nExtracting /usr/lib/deep-learning/examples/tfos/mnist/train-labels-idx1-ubyte.gz\nimages.shape: (60000, 28, 28, 1)\nlabels.shape: (60000, 10)\n18/02/21 01:05:53 Thread-4 INFO SparkContext: Starting job: take at SerDeUtil.scala:233\n18/02/21 01:05:53 dag-scheduler-event-loop INFO DAGScheduler: Got job 0 (take at SerDeUtil.scala:233) with 1 output partitions\n18/02/21 01:05:53 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 0 (take at SerDeUtil.scala:233)\n18/02/21 01:05:53 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()\n18/02/21 01:05:53 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()\n18/02/21 01:05:53 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at mapPartitions at SerDeUtil.scala:148), which has no missing parents\n18/02/21 01:05:53 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.7 KB, free 20.1 GB)\n18/02/21 01:05:53 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.6 KB, free 20.1 GB)\n18/02/21 01:05:53 dispatcher-event-loop-1 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.0.72:45855 (size: 3.6 KB, free: 20.1 GB)\n18/02/21 01:05:53 dag-scheduler-event-loop INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1022\n18/02/21 01:05:53 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at mapPartitions at SerDeUtil.scala:148)\n18/02/21 01:05:53 dag-scheduler-event-loop INFO YarnScheduler: Adding task set 0.0 with 1 tasks\n18/02/21 01:05:53 dispatcher-event-loop-4 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(null) (10.0.0.48:47316) with ID 1 and container_1519174037520_0003_01_000002 with size 50465865728\n18/02/21 01:05:53 dispatcher-event-loop-4 WARN TaskSetManager: Stage 0 contains a task of very large size (4880 KB). The maximum recommended task size is 100 KB.\n18/02/21 01:05:53 dispatcher-event-loop-4 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, ip-10-0-0-48.ec2.internal, executor 1, partition 0, PROCESS_LOCAL, 4997806 bytes)\n18/02/21 01:05:53 dispatcher-event-loop-2 INFO BlockManagerMasterEndpoint: Registering block manager ip-10-0-0-48.ec2.internal:34817 with 20.1 GB RAM, BlockManagerId(1, ip-10-0-0-48.ec2.internal, 34817, None)\n18/02/21 01:05:53 dispatcher-event-loop-3 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-10-0-0-48.ec2.internal:34817 (size: 3.6 KB, free: 20.1 GB)\n18/02/21 01:07:14 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 80968 ms on ip-10-0-0-48.ec2.internal (executor 1) (1/1)\n18/02/21 01:07:14 task-result-getter-0 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n18/02/21 01:07:14 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 0 (take at SerDeUtil.scala:233) finished in 81.129 s\n18/02/21 01:07:14 Thread-4 INFO DAGScheduler: Job 0 finished: take at SerDeUtil.scala:233, took 81.491539 s\n18/02/21 01:07:14 Thread-4 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n18/02/21 01:07:14 Thread-4 INFO deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS\n18/02/21 01:07:14 Thread-4 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n18/02/21 01:07:14 dispatcher-event-loop-0 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.0.72:45855 in memory (size: 3.6 KB, free: 20.1 GB)\n18/02/21 01:07:14 dispatcher-event-loop-1 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ip-10-0-0-48.ec2.internal:34817 in memory (size: 3.6 KB, free: 20.1 GB)\n18/02/21 01:07:14 Thread-4 INFO SparkContext: Starting job: saveAsNewAPIHadoopFile at PythonRDD.scala:834\n18/02/21 01:07:14 dag-scheduler-event-loop INFO DAGScheduler: Got job 1 (saveAsNewAPIHadoopFile at PythonRDD.scala:834) with 10 output partitions\n18/02/21 01:07:14 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 1 (saveAsNewAPIHadoopFile at PythonRDD.scala:834)\n18/02/21 01:07:14 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()\n18/02/21 01:07:14 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()\n18/02/21 01:07:14 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at map at PythonHadoopUtil.scala:181), which has no missing parents\n18/02/21 01:07:14 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 77.6 KB, free 20.1 GB)\n18/02/21 01:07:14 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 27.3 KB, free 20.1 GB)\n18/02/21 01:07:14 dispatcher-event-loop-6 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.0.72:45855 (size: 27.3 KB, free: 20.1 GB)\n18/02/21 01:07:14 dag-scheduler-event-loop INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1022\n18/02/21 01:07:14 dag-scheduler-event-loop INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at map at PythonHadoopUtil.scala:181)\n18/02/21 01:07:14 dag-scheduler-event-loop INFO YarnScheduler: Adding task set 1.0 with 10 tasks\n18/02/21 01:07:14 dispatcher-event-loop-3 WARN TaskSetManager: Stage 1 contains a task of very large size (4880 KB). The maximum recommended task size is 100 KB.\n18/02/21 01:07:14 dispatcher-event-loop-3 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, ip-10-0-0-48.ec2.internal, executor 1, partition 0, PROCESS_LOCAL, 4997825 bytes)\n18/02/21 01:07:14 dispatcher-event-loop-2 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ip-10-0-0-48.ec2.internal:34817 (size: 27.3 KB, free: 20.1 GB)\n18/02/21 01:07:17 dispatcher-event-loop-6 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2, ip-10-0-0-48.ec2.internal, executor 1, partition 1, PROCESS_LOCAL, 5996045 bytes)\n18/02/21 01:07:17 task-result-getter-1 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2934 ms on ip-10-0-0-48.ec2.internal (executor 1) (1/10)\n18/02/21 01:07:19 dispatcher-event-loop-5 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3, ip-10-0-0-48.ec2.internal, executor 1, partition 2, PROCESS_LOCAL, 5996045 bytes)\n18/02/21 01:07:19 task-result-getter-2 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 1628 ms on ip-10-0-0-48.ec2.internal (executor 1) (2/10)\n18/02/21 01:07:20 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4, ip-10-0-0-48.ec2.internal, executor 1, partition 3, PROCESS_LOCAL, 5996045 bytes)\n18/02/21 01:07:20 task-result-getter-3 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 1631 ms on ip-10-0-0-48.ec2.internal (executor 1) (3/10)\n18/02/21 01:07:22 dispatcher-event-loop-4 INFO TaskSetManager: Starting task 4.0 in stage 1.0 (TID 5, ip-10-0-0-48.ec2.internal, executor 1, partition 4, PROCESS_LOCAL, 5996045 bytes)\n18/02/21 01:07:22 task-result-getter-0 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 1633 ms on ip-10-0-0-48.ec2.internal (executor 1) (4/10)\n18/02/21 01:07:24 dispatcher-event-loop-6 INFO TaskSetManager: Starting task 5.0 in stage 1.0 (TID 6, ip-10-0-0-48.ec2.internal, executor 1, partition 5, PROCESS_LOCAL, 5996045 bytes)\n18/02/21 01:07:24 task-result-getter-1 INFO TaskSetManager: Finished task 4.0 in stage 1.0 (TID 5) in 1668 ms on ip-10-0-0-48.ec2.internal (executor 1) (5/10)\n18/02/21 01:07:25 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 6.0 in stage 1.0 (TID 7, ip-10-0-0-48.ec2.internal, executor 1, partition 6, PROCESS_LOCAL, 5996045 bytes)\n18/02/21 01:07:25 task-result-getter-2 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID 6) in 1642 ms on ip-10-0-0-48.ec2.internal (executor 1) (6/10)\n18/02/21 01:07:27 dispatcher-event-loop-6 INFO TaskSetManager: Starting task 7.0 in stage 1.0 (TID 8, ip-10-0-0-48.ec2.internal, executor 1, partition 7, PROCESS_LOCAL, 5996045 bytes)\n18/02/21 01:07:27 task-result-getter-3 INFO TaskSetManager: Finished task 6.0 in stage 1.0 (TID 7) in 1620 ms on ip-10-0-0-48.ec2.internal (executor 1) (7/10)\n18/02/21 01:07:28 dispatcher-event-loop-5 INFO TaskSetManager: Starting task 8.0 in stage 1.0 (TID 9, ip-10-0-0-48.ec2.internal, executor 1, partition 8, PROCESS_LOCAL, 5996045 bytes)\n18/02/21 01:07:28 task-result-getter-0 INFO TaskSetManager: Finished task 7.0 in stage 1.0 (TID 8) in 1625 ms on ip-10-0-0-48.ec2.internal (executor 1) (8/10)\n18/02/21 01:07:30 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 9.0 in stage 1.0 (TID 10, ip-10-0-0-48.ec2.internal, executor 1, partition 9, PROCESS_LOCAL, 5590025 bytes)\n18/02/21 01:07:30 task-result-getter-1 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID 9) in 1607 ms on ip-10-0-0-48.ec2.internal (executor 1) (9/10)\n18/02/21 01:07:32 task-result-getter-2 INFO TaskSetManager: Finished task 9.0 in stage 1.0 (TID 10) in 1499 ms on ip-10-0-0-48.ec2.internal (executor 1) (10/10)\n18/02/21 01:07:32 task-result-getter-2 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \n18/02/21 01:07:32 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at PythonRDD.scala:834) finished in 17.405 s\n18/02/21 01:07:32 Thread-4 INFO DAGScheduler: Job 1 finished: saveAsNewAPIHadoopFile at PythonRDD.scala:834, took 17.423628 s\nExtracting /usr/lib/deep-learning/examples/tfos/mnist/t10k-images-idx3-ubyte.gz\nExtracting /usr/lib/deep-learning/examples/tfos/mnist/t10k-labels-idx1-ubyte.gz\nimages.shape: (10000, 28, 28, 1)\nlabels.shape: (10000, 10)\n18/02/21 01:07:32 Thread-4 INFO SparkContext: Starting job: take at SerDeUtil.scala:233\n18/02/21 01:07:32 dag-scheduler-event-loop INFO DAGScheduler: Got job 2 (take at SerDeUtil.scala:233) with 1 output partitions\n18/02/21 01:07:32 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 2 (take at SerDeUtil.scala:233)\n18/02/21 01:07:32 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()\n18/02/21 01:07:32 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()\n18/02/21 01:07:32 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at mapPartitions at SerDeUtil.scala:148), which has no missing parents\n18/02/21 01:07:32 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 5.7 KB, free 20.1 GB)\n18/02/21 01:07:32 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.6 KB, free 20.1 GB)\n18/02/21 01:07:32 dispatcher-event-loop-0 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.0.72:45855 (size: 3.6 KB, free: 20.1 GB)\n18/02/21 01:07:32 dag-scheduler-event-loop INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1022\n18/02/21 01:07:32 dag-scheduler-event-loop INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at mapPartitions at SerDeUtil.scala:148)\n18/02/21 01:07:32 dag-scheduler-event-loop INFO YarnScheduler: Adding task set 2.0 with 1 tasks\n18/02/21 01:07:32 dispatcher-event-loop-1 WARN TaskSetManager: Stage 2 contains a task of very large size (958 KB). The maximum recommended task size is 100 KB.\n18/02/21 01:07:32 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 11, ip-10-0-0-48.ec2.internal, executor 1, partition 0, PROCESS_LOCAL, 981499 bytes)\n18/02/21 01:07:32 dispatcher-event-loop-7 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ip-10-0-0-48.ec2.internal:34817 (size: 3.6 KB, free: 20.1 GB)\n18/02/21 01:07:32 task-result-getter-3 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 11) in 59 ms on ip-10-0-0-48.ec2.internal (executor 1) (1/1)\n18/02/21 01:07:32 task-result-getter-3 INFO YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \n18/02/21 01:07:32 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 2 (take at SerDeUtil.scala:233) finished in 0.059 s\n18/02/21 01:07:32 Thread-4 INFO DAGScheduler: Job 2 finished: take at SerDeUtil.scala:233, took 0.066819 s\n18/02/21 01:07:32 dispatcher-event-loop-0 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.0.72:45855 in memory (size: 3.6 KB, free: 20.1 GB)\n18/02/21 01:07:32 dispatcher-event-loop-1 INFO BlockManagerInfo: Removed broadcast_2_piece0 on ip-10-0-0-48.ec2.internal:34817 in memory (size: 3.6 KB, free: 20.1 GB)\n18/02/21 01:07:32 dispatcher-event-loop-7 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.0.72:45855 in memory (size: 27.3 KB, free: 20.1 GB)\n18/02/21 01:07:32 dispatcher-event-loop-5 INFO BlockManagerInfo: Removed broadcast_1_piece0 on ip-10-0-0-48.ec2.internal:34817 in memory (size: 27.3 KB, free: 20.1 GB)\n18/02/21 01:07:32 Thread-4 INFO deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS\n18/02/21 01:07:32 Thread-4 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n18/02/21 01:07:32 Thread-4 INFO SparkContext: Starting job: saveAsNewAPIHadoopFile at PythonRDD.scala:834\n18/02/21 01:07:32 dag-scheduler-event-loop INFO DAGScheduler: Got job 3 (saveAsNewAPIHadoopFile at PythonRDD.scala:834) with 10 output partitions\n18/02/21 01:07:32 dag-scheduler-event-loop INFO DAGScheduler: Final stage: ResultStage 3 (saveAsNewAPIHadoopFile at PythonRDD.scala:834)\n18/02/21 01:07:32 dag-scheduler-event-loop INFO DAGScheduler: Parents of final stage: List()\n18/02/21 01:07:32 dag-scheduler-event-loop INFO DAGScheduler: Missing parents: List()\n18/02/21 01:07:32 dag-scheduler-event-loop INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at map at PythonHadoopUtil.scala:181), which has no missing parents\n18/02/21 01:07:32 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 77.6 KB, free 20.1 GB)\n18/02/21 01:07:32 dag-scheduler-event-loop INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 27.3 KB, free 20.1 GB)\n18/02/21 01:07:32 dispatcher-event-loop-4 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.0.72:45855 (size: 27.3 KB, free: 20.1 GB)\n18/02/21 01:07:32 dag-scheduler-event-loop INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1022\n18/02/21 01:07:32 dag-scheduler-event-loop INFO DAGScheduler: Submitting 10 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at map at PythonHadoopUtil.scala:181)\n18/02/21 01:07:32 dag-scheduler-event-loop INFO YarnScheduler: Adding task set 3.0 with 10 tasks\n18/02/21 01:07:32 dispatcher-event-loop-2 WARN TaskSetManager: Stage 3 contains a task of very large size (958 KB). The maximum recommended task size is 100 KB.\n18/02/21 01:07:32 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 12, ip-10-0-0-48.ec2.internal, executor 1, partition 0, PROCESS_LOCAL, 981517 bytes)\n18/02/21 01:07:32 dispatcher-event-loop-3 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ip-10-0-0-48.ec2.internal:34817 (size: 27.3 KB, free: 20.1 GB)\n18/02/21 01:07:34 dispatcher-event-loop-7 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 13, ip-10-0-0-48.ec2.internal, executor 1, partition 1, PROCESS_LOCAL, 981517 bytes)\n18/02/21 01:07:34 task-result-getter-0 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 12) in 1718 ms on ip-10-0-0-48.ec2.internal (executor 1) (1/10)\n18/02/21 01:07:34 dispatcher-event-loop-2 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 14, ip-10-0-0-48.ec2.internal, executor 1, partition 2, PROCESS_LOCAL, 981517 bytes)\n18/02/21 01:07:34 task-result-getter-1 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 13) in 321 ms on ip-10-0-0-48.ec2.internal (executor 1) (2/10)\n18/02/21 01:07:34 dispatcher-event-loop-1 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 15, ip-10-0-0-48.ec2.internal, executor 1, partition 3, PROCESS_LOCAL, 981517 bytes)\n18/02/21 01:07:34 task-result-getter-2 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 14) in 327 ms on ip-10-0-0-48.ec2.internal (executor 1) (3/10)\n18/02/21 01:07:35 dispatcher-event-loop-6 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 16, ip-10-0-0-48.ec2.internal, executor 1, partition 4, PROCESS_LOCAL, 981517 bytes)\n18/02/21 01:07:35 task-result-getter-3 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 15) in 322 ms on ip-10-0-0-48.ec2.internal (executor 1) (4/10)\n18/02/21 01:07:35 dispatcher-event-loop-4 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 17, ip-10-0-0-48.ec2.internal, executor 1, partition 5, PROCESS_LOCAL, 981517 bytes)\n18/02/21 01:07:35 task-result-getter-0 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 16) in 323 ms on ip-10-0-0-48.ec2.internal (executor 1) (5/10)\n18/02/21 01:07:35 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 18, ip-10-0-0-48.ec2.internal, executor 1, partition 6, PROCESS_LOCAL, 981517 bytes)\n18/02/21 01:07:35 task-result-getter-1 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 17) in 321 ms on ip-10-0-0-48.ec2.internal (executor 1) (6/10)\n18/02/21 01:07:36 dispatcher-event-loop-3 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 19, ip-10-0-0-48.ec2.internal, executor 1, partition 7, PROCESS_LOCAL, 981517 bytes)\n18/02/21 01:07:36 task-result-getter-2 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 18) in 325 ms on ip-10-0-0-48.ec2.internal (executor 1) (7/10)\n18/02/21 01:07:36 dispatcher-event-loop-5 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 20, ip-10-0-0-48.ec2.internal, executor 1, partition 8, PROCESS_LOCAL, 981517 bytes)\n18/02/21 01:07:36 task-result-getter-3 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 19) in 325 ms on ip-10-0-0-48.ec2.internal (executor 1) (8/10)\n18/02/21 01:07:38 dispatcher-event-loop-0 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 21, ip-10-0-0-48.ec2.internal, executor 1, partition 9, PROCESS_LOCAL, 981517 bytes)\n18/02/21 01:07:38 task-result-getter-0 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 20) in 1608 ms on ip-10-0-0-48.ec2.internal (executor 1) (9/10)\n18/02/21 01:07:38 task-result-getter-1 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 21) in 282 ms on ip-10-0-0-48.ec2.internal (executor 1) (10/10)\n18/02/21 01:07:38 task-result-getter-1 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \n18/02/21 01:07:38 dag-scheduler-event-loop INFO DAGScheduler: ResultStage 3 (saveAsNewAPIHadoopFile at PythonRDD.scala:834) finished in 5.856 s\n18/02/21 01:07:38 Thread-4 INFO DAGScheduler: Job 3 finished: saveAsNewAPIHadoopFile at PythonRDD.scala:834, took 5.877715 s\n18/02/21 01:07:44 main INFO SparkContext: sc.stop called from [SparkSubmit.successfulExitHook[success]]\n18/02/21 01:07:44 main INFO ServerConnector: Stopped Spark@52e8cc74{HTTP/1.1}{0.0.0.0:4040}\n18/02/21 01:07:44 main INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@12c80857{/stages/stage/kill,null,UNAVAILABLE,@Spark}\n18/02/21 01:07:44 main INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@4e3f827f{/jobs/job/kill,null,UNAVAILABLE,@Spark}\n18/02/21 01:07:44 main INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@7d8672fe{/api,null,UNAVAILABLE,@Spark}\n18/02/21 01:07:44 main INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@3dacc2aa{/,null,UNAVAILABLE,@Spark}\n18/02/21 01:07:44 main INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@10a99047{/static,null,UNAVAILABLE,@Spark}\n18/02/21 01:07:44 main INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@112445d4{/executors/threadDump/json,null,UNAVAILABLE,@Spark}\n18/02/21 01:07:44 main INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@5d03ad2c{/executors/threadDump,null,UNAVAILABLE,@Spark}\n18/02/21 01:07:44 main INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@4a6ce890{/executors/json,null,UNAVAILABLE,@Spark}\n18/02/21 01:07:44 main INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@1cd3abfc{/executors,null,UNAVAILABLE,@Spark}\n18/02/21 01:07:44 main INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@a98850{/environment/json,null,UNAVAILABLE,@Spark}\n18/02/21 01:07:44 main INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@679e1cc5{/environment,null,UNAVAILABLE,@Spark}\n18/02/21 01:07:44 main INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@6c6741cf{/storage/rdd/json,null,UNAVAILABLE,@Spark}\n18/02/21 01:07:44 main INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@55c935ec{/storage/rdd,null,UNAVAILABLE,@Spark}\n18/02/21 01:07:44 main INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@4e1528be{/storage/json,null,UNAVAILABLE,@Spark}\n18/02/21 01:07:44 main INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@13631234{/storage,null,UNAVAILABLE,@Spark}\n18/02/21 01:07:44 main INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@7e98f1f3{/stages/pool/json,null,UNAVAILABLE,@Spark}\n18/02/21 01:07:44 main INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@5599dbd3{/stages/pool,null,UNAVAILABLE,@Spark}\n18/02/21 01:07:44 main INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@5c703b75{/stages/stage/json,null,UNAVAILABLE,@Spark}\n18/02/21 01:07:44 main INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@45c50a6e{/stages/stage,null,UNAVAILABLE,@Spark}\n18/02/21 01:07:44 main INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@5d3ec299{/stages/json,null,UNAVAILABLE,@Spark}\n18/02/21 01:07:44 main INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@41791323{/stages,null,UNAVAILABLE,@Spark}\n18/02/21 01:07:44 main INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@30852536{/jobs/job/json,null,UNAVAILABLE,@Spark}\n18/02/21 01:07:44 main INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@3de4b0e0{/jobs/job,null,UNAVAILABLE,@Spark}\n18/02/21 01:07:44 main INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@410baa07{/jobs/json,null,UNAVAILABLE,@Spark}\n18/02/21 01:07:44 main INFO ContextHandler: Stopped o.s.j.s.ServletContextHandler@56e4fd38{/jobs,null,UNAVAILABLE,@Spark}\n18/02/21 01:07:44 main INFO SparkUI: Stopped Spark web UI at http://10.0.0.72:4040\n18/02/21 01:07:44 Yarn application state monitor INFO YarnClientSchedulerBackend: Interrupting monitor thread\n18/02/21 01:07:44 main INFO YarnClientSchedulerBackend: Shutting down all executors\n18/02/21 01:07:44 dispatcher-event-loop-4 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: Sending StopAM(true) to AppMaster\n18/02/21 01:07:44 dispatcher-event-loop-2 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\n18/02/21 01:07:44 main INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices\n(serviceOption\u003dNone,\n services\u003dList(),\n started\u003dfalse)\n18/02/21 01:07:44 main INFO YarnClientSchedulerBackend: Stopped\n18/02/21 01:07:44 main INFO JobProgressListener: Counters\u003dFileSystemCounters.S3N_BYTES_READ:0,FileSystemCounters.S3N_BYTES_WRITTEN:0,FileSystemCounters.S3N_RECORDS_READ:0,FileSystemCounters.S3N_RECORDS_WRITTEN:0,Job Counters.SQL_EXECUTION_COUNT:0\n18/02/21 01:07:44 dispatcher-event-loop-6 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n18/02/21 01:07:44 main INFO MemoryStore: MemoryStore cleared\n18/02/21 01:07:44 main INFO BlockManager: BlockManager stopped\n18/02/21 01:07:44 main INFO BlockManagerMaster: BlockManagerMaster stopped\n18/02/21 01:07:44 dispatcher-event-loop-2 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n18/02/21 01:07:44 main INFO SparkContext: Successfully stopped SparkContext\n18/02/21 01:07:44 Thread-1 INFO ShutdownHookManager: Shutdown hook called\n18/02/21 01:07:44 Thread-1 INFO ShutdownHookManager: Deleting directory /tmp/spark-6ead82a6-590a-4778-ad7c-216af8c556a2\n18/02/21 01:07:44 Thread-1 INFO ShutdownHookManager: Deleting directory /tmp/spark-6f44c423-c5a9-44c7-886a-9590f30257b8\n18/02/21 01:07:44 Asks YARN to kill this spark job INFO YarnClientImpl: Killed application application_1519174037520_0003\n18/02/21 01:07:44 Thread-1 INFO ShutdownHookManager: Deleting directory /tmp/spark-6f44c423-c5a9-44c7-886a-9590f30257b8/pyspark-3aa85c49-7b40-496d-88ba-9926b2faf322\n18/02/21 01:07:44 Thread-1 INFO YarnScheduler$$anon$1: Invoing sc.stop from shutdown hook.\n18/02/21 01:07:44 Thread-1 INFO SparkContext: sc.stop called from [YarnClientClusterScheduler shutdown hook]\n18/02/21 01:07:44 Thread-1 INFO SparkContext: SparkContext already stopped.\n"
      },
      "dateCreated": "Feb 20, 2018 8:56:30 AM",
      "dateStarted": "Feb 21, 2018 1:03:21 AM",
      "dateFinished": "Feb 21, 2018 1:07:44 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom pyspark.context import SparkContext\nfrom pyspark.conf import SparkConf\nimport sys\nimport argparse\nimport os\nimport numpy\nimport sys\nimport tensorflow as tf\nimport threading\nfrom datetime import datetime\nfrom tensorflowonspark import TFCluster\n \n \ndef print_log(worker_num, arg):\n    print(\"{}: \".format(worker_num))\n    print(arg)\n \n \ndef map_fun(args, ctx):\n    from tensorflowonspark import TFNode\n    from datetime import datetime\n    import getpass\n    import math\n    import numpy\n    import os\n    import signal\n    import tensorflow as tf\n    import time\n \n    IMAGE_PIXELS \u003d 28\n    worker_num \u003d ctx.worker_num\n    job_name \u003d ctx.job_name\n    task_index \u003d ctx.task_index\n    cluster_spec \u003d ctx.cluster_spec\n    num_workers \u003d len(cluster_spec[\u0027worker\u0027])\n \n    # Delay PS nodes a bit, since workers seem to reserve GPUs more quickly/reliably (w/o conflict)\n    if job_name \u003d\u003d \"ps\":\n        time.sleep((worker_num + 1) * 5)\n \n    # Parameters\n    hidden_units \u003d 128\n    batch_size \u003d 100\n \n    # Get TF cluster and server instances\n    cluster, server \u003d TFNode.start_cluster_server(ctx, 1, args[\"rdma\"])\n \n    def read_csv_examples(image_dir, label_dir, batch_size\u003d100, num_epochs\u003dNone, task_index\u003dNone, num_workers\u003dNone):\n        print_log(worker_num, \"num_epochs: {0}\".format(num_epochs))\n        # Setup queue of csv image filenames\n        tf_record_pattern \u003d os.path.join(image_dir, \u0027part-*\u0027)\n        images \u003d tf.gfile.Glob(tf_record_pattern)\n        print_log(worker_num, \"images: {0}\".format(images))\n        image_queue \u003d tf.train.string_input_producer(\n            images, shuffle\u003dFalse, capacity\u003d1000, num_epochs\u003dnum_epochs, name\u003d\"image_queue\")\n \n        # Setup queue of csv label filenames\n        tf_record_pattern \u003d os.path.join(label_dir, \u0027part-*\u0027)\n        labels \u003d tf.gfile.Glob(tf_record_pattern)\n        print_log(worker_num, \"labels: {0}\".format(labels))\n        label_queue \u003d tf.train.string_input_producer(\n            labels, shuffle\u003dFalse, capacity\u003d1000, num_epochs\u003dnum_epochs, name\u003d\"label_queue\")\n \n        # Setup reader for image queue\n        img_reader \u003d tf.TextLineReader(name\u003d\"img_reader\")\n        _, img_csv \u003d img_reader.read(image_queue)\n        image_defaults \u003d [[1.0] for col in range(784)]\n        img \u003d tf.pack(tf.decode_csv(img_csv, image_defaults))\n        # Normalize values to [0,1]\n        norm \u003d tf.constant(255, dtype\u003dtf.float32, shape\u003d(784,))\n        image \u003d tf.div(img, norm)\n        print_log(worker_num, \"image: {0}\".format(image))\n \n        # Setup reader for label queue\n        label_reader \u003d tf.TextLineReader(name\u003d\"label_reader\")\n        _, label_csv \u003d label_reader.read(label_queue)\n        label_defaults \u003d [[1.0] for col in range(10)]\n        label \u003d tf.pack(tf.decode_csv(label_csv, label_defaults))\n        print_log(worker_num, \"label: {0}\".format(label))\n \n        # Return a batch of examples\n        return tf.train.batch([image, label], batch_size, num_threads\u003dargs[\"readers\"], name\u003d\"batch_csv\")\n \n    def read_tfr_examples(path, batch_size\u003d100, num_epochs\u003dNone, task_index\u003dNone, num_workers\u003dNone):\n        print_log(worker_num, \"num_epochs: {0}\".format(num_epochs))\n \n        # Setup queue of TFRecord filenames\n        tf_record_pattern \u003d os.path.join(path, \u0027part-*\u0027)\n        files \u003d tf.gfile.Glob(tf_record_pattern)\n        queue_name \u003d \"file_queue\"\n \n        # split input files across workers, if specified\n        if task_index is not None and num_workers is not None:\n            num_files \u003d len(files)\n            files \u003d files[task_index:num_files:num_workers]\n            queue_name \u003d \"file_queue_{0}\".format(task_index)\n \n        print_log(worker_num, \"files: {0}\".format(files))\n        file_queue \u003d tf.train.string_input_producer(\n            files, shuffle\u003dFalse, capacity\u003d1000, num_epochs\u003dnum_epochs, name\u003dqueue_name)\n \n        # Setup reader for examples\n        reader \u003d tf.TFRecordReader(name\u003d\"reader\")\n        _, serialized \u003d reader.read(file_queue)\n        feature_def \u003d {\u0027label\u0027: tf.FixedLenFeature(\n            [10], tf.int64), \u0027image\u0027: tf.FixedLenFeature([784], tf.int64)}\n        features \u003d tf.parse_single_example(serialized, feature_def)\n        norm \u003d tf.constant(255, dtype\u003dtf.float32, shape\u003d(784,))\n        image \u003d tf.div(tf.to_float(features[\u0027image\u0027]), norm)\n        print_log(worker_num, \"image: {0}\".format(image))\n        label \u003d tf.to_float(features[\u0027label\u0027])\n        print_log(worker_num, \"label: {0}\".format(label))\n \n        # Return a batch of examples\n        return tf.train.batch([image, label], batch_size, num_threads\u003dargs[\"readers\"], name\u003d\"batch\")\n \n    if job_name \u003d\u003d \"ps\":\n        server.join()\n    elif job_name \u003d\u003d \"worker\":\n        # Assigns ops to the local worker by default.\n        with tf.device(tf.train.replica_device_setter(\n                worker_device\u003d\"/job:worker/task:%d\" % task_index,\n                cluster\u003dcluster)):\n \n            # Variables of the hidden layer\n            hid_w \u003d tf.Variable(tf.truncated_normal([IMAGE_PIXELS * IMAGE_PIXELS, hidden_units],\n                                                    stddev\u003d1.0 / IMAGE_PIXELS), name\u003d\"hid_w\")\n            hid_b \u003d tf.Variable(tf.zeros([hidden_units]), name\u003d\"hid_b\")\n            tf.summary.histogram(\"hidden_weights\", hid_w)\n \n            # Variables of the softmax layer\n            sm_w \u003d tf.Variable(tf.truncated_normal([hidden_units, 10],\n                                                   stddev\u003d1.0 / math.sqrt(hidden_units)), name\u003d\"sm_w\")\n            sm_b \u003d tf.Variable(tf.zeros([10]), name\u003d\"sm_b\")\n            tf.summary.histogram(\"softmax_weights\", sm_w)\n \n            # Placeholders or QueueRunner/Readers for input data\n            num_epochs \u003d 1 if args[\"mode\"] \u003d\u003d \"inference\" else None if args[\"epochs\"] \u003d\u003d 0 else args[\"epochs\"]\n            index \u003d task_index if args[\"mode\"] \u003d\u003d \"inference\" else None\n            workers \u003d num_workers if args[\"mode\"] \u003d\u003d \"inference\" else None\n \n            if args[\"format\"] \u003d\u003d \"csv\":\n                images \u003d TFNode.hdfs_path(args[\"images\"], ctx.defaultFS, ctx.working_dir)\n                labels \u003d TFNode.hdfs_path(args[\"labels\"], ctx.defaultFS, ctx.working_dir)\n                x, y_ \u003d read_csv_examples(\n                    images, labels, 100, num_epochs, index, workers)\n            elif args[\"format\"] \u003d\u003d \"tfr\":\n                images \u003d TFNode.hdfs_path(args[\"images\"], ctx.defaultFS, ctx.working_dir)\n                x, y_ \u003d read_tfr_examples(\n                    images, 100, num_epochs, index, workers)\n            else:\n                raise(\"{0} format not supported for tf input mode\".format(\n                    args[\"format\"]))\n \n            x_img \u003d tf.reshape(x, [-1, IMAGE_PIXELS, IMAGE_PIXELS, 1])\n            tf.summary.image(\"x_img\", x_img)\n \n            hid_lin \u003d tf.nn.xw_plus_b(x, hid_w, hid_b)\n            hid \u003d tf.nn.relu(hid_lin)\n \n            y \u003d tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\n \n            global_step \u003d tf.Variable(0)\n \n            loss \u003d -tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n            tf.summary.scalar(\"loss\", loss)\n            train_op \u003d tf.train.AdagradOptimizer(0.01).minimize(\n                loss, global_step\u003dglobal_step)\n \n            # Test trained model\n            label \u003d tf.argmax(y_, 1, name\u003d\"label\")\n            prediction \u003d tf.argmax(y, 1, name\u003d\"prediction\")\n            correct_prediction \u003d tf.equal(prediction, label)\n            accuracy \u003d tf.reduce_mean(\n                tf.cast(correct_prediction, tf.float32), name\u003d\"accuracy\")\n            tf.summary.scalar(\"acc\", accuracy)\n \n            saver \u003d tf.train.Saver()\n            summary_op \u003d tf.summary.merge_all()\n            init_op \u003d tf.global_variables_initializer()\n \n        # Create a \"supervisor\", which oversees the training process and stores model state into HDFS\n        logdir \u003d TFNode.hdfs_path(args[\"model\"], ctx.defaultFS, ctx.working_dir)\n        print(\"tensorflow model path: {0}\".format(logdir))\n \n        summary_writer \u003d TFNode.get_summary_writer(ctx)\n         \n        if args[\"mode\"] \u003d\u003d \"train\":\n            sv \u003d tf.train.Supervisor(is_chief\u003d(task_index \u003d\u003d 0),\n                                     logdir\u003dlogdir,\n                                     init_op\u003dinit_op,\n                                     summary_op\u003dNone,\n                                     saver\u003dsaver,\n                                     global_step\u003dglobal_step,\n                                     stop_grace_secs\u003d300,\n                                     save_model_secs\u003d10)\n        else:\n            sv \u003d tf.train.Supervisor(is_chief\u003d(task_index \u003d\u003d 0),\n                                     logdir\u003dlogdir,\n                                     summary_op\u003dNone,\n                                     saver\u003dsaver,\n                                     global_step\u003dglobal_step,\n                                     stop_grace_secs\u003d300,\n                                     save_model_secs\u003d0)\n            output_dir \u003d TFNode.hdfs_path(args[\"output\"], ctx.defaultFS, ctx.working_dir)\n            output_file \u003d tf.gfile.Open(\n                \"{0}/part-{1:05d}\".format(output_dir, worker_num), mode\u003d\u0027w\u0027)\n \n        # The supervisor takes care of session initialization, restoring from\n        # a checkpoint, and closing when done or an error occurs.\n        with sv.managed_session(server.target) as sess:\n            print(\"{0} session ready\".format(datetime.now().isoformat()))\n \n            # Loop until the supervisor shuts down or 1000000 steps have completed.\n            step \u003d 0\n            count \u003d 0\n            while not sv.should_stop() and step \u003c args[\"steps\"]:\n                # Run a training step asynchronously.\n                # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n                # perform *synchronous* training.\n \n                # using QueueRunners/Readers\n                if args[\"mode\"] \u003d\u003d \"train\":\n                    if (step % 100 \u003d\u003d 0):\n                        print(\"{0} step: {1} accuracy: {2}\".format(\n                            datetime.now().isoformat(), step, sess.run(accuracy)))\n                    _, summary, step \u003d sess.run(\n                        [train_op, summary_op, global_step])\n                    if sv.is_chief:\n                        summary_writer.add_summary(summary, step)\n                else:  # args[\"mode\"] \u003d\u003d \"inference\"\n                    labels, pred, acc \u003d sess.run([label, prediction, accuracy])\n                    #print(\"label: {0}, pred: {1}\".format(labels, pred))\n                    print(\"acc: {0}\".format(acc))\n                    for i in range(len(labels)):\n                        count +\u003d 1\n                        output_file.write(\n                            \"{0} {1}\\n\".format(labels[i], pred[i]))\n                    print(\"count: {0}\".format(count))\n \n        if args[\"mode\"] \u003d\u003d \"inference\":\n            output_file.close()\n            # Delay chief worker from shutting down supervisor during inference, since it can load model, start session,\n            # run inference and request stop before the other workers even start/sync their sessions.\n            if task_index \u003d\u003d 0:\n                time.sleep(60)\n \n        # Ask for all the services to stop.\n        print(\"{0} stopping supervisor\".format(datetime.now().isoformat()))\n        sv.stop()\n \n \nnum_executors \u003d 2\nnum_ps \u003d 1\n \nargs \u003d {\"epochs\": 1,\n        \"format\": \"tfr\",\n        \"images\":\"/deep-learning/examples/tfos/mnist_data/tfr/train\",\n        \"labels\": None,\n        \"model\": \"mnist_model\",\n        \"cluster_size\": num_executors,\n        \"output\": \"predictions\",\n        \"readers\": 1,\n        \"steps\": 1000,\n        \"mode\": \"train\",\n        \"rdma\": False\n        }\n \nprint(\"{0} \u003d\u003d\u003d\u003d\u003d Start\".format(datetime.now().isoformat()))\ncluster \u003d TFCluster.run(sc, map_fun, args, args[\"cluster_size\"],\n                        num_ps, TFCluster.InputMode.TENSORFLOW)\ncluster.shutdown()\n \nprint(\"{0} \u003d\u003d\u003d\u003d\u003d Stop\".format(datetime.now().isoformat()))",
      "user": "somyak@qubole.com",
      "dateUpdated": "Feb 21, 2018 1:10:35 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [
          {
            "id": 0,
            "jobUrl": "https://somyak.qubole.net/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2Fec2-174-129-98-222.compute-1.amazonaws.com%3A8088%2Fproxy%2Fapplication_1519174037520_0002/jobs/job?spark\u003dtrue\u0026id\u003d0",
            "numTasks": 2,
            "numCompletedTasks": 2,
            "stages": [
              {
                "id": 0,
                "completed": true,
                "stageUrl": "https://somyak.qubole.net/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2Fec2-174-129-98-222.compute-1.amazonaws.com%3A8088%2Fproxy%2Fapplication_1519174037520_0002/stages/stage/?id\u003d0\u0026attempt\u003d0",
                "numCompleteTasks": 2,
                "numActiveTasks": 0,
                "numFailedTasks": 0,
                "numTotalTasks": 2
              }
            ],
            "status": "Success"
          }
        ],
        "numCompletedTasks": 2,
        "numTasks": 2,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1519117389690_-953675293",
      "id": "20180220-090309_911015111_q_REHQJ9HEG71519116979",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "2018-02-21T01:10:35.866526 \u003d\u003d\u003d\u003d\u003d Start\n2018-02-21 01:10:35,867 INFO (MainThread-9924) Reserving TFSparkNodes.\n\n2018-02-21 01:10:35,869 INFO (MainThread-9924) listening for reservations at (\u0027ip-10-0-0-164\u0027, 41443)\n\n2018-02-21 01:10:35,870 INFO (MainThread-9924) Starting TensorFlow on executors\n\n2018-02-21 01:10:35,947 INFO (MainThread-9924) Waiting for TFSparkNodes to start\n\n2018-02-21 01:10:35,949 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:10:36,951 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:10:37,954 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:10:38,955 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:10:39,958 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:10:40,959 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:10:41,961 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:10:42,963 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:10:43,965 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:10:44,967 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:10:45,969 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:10:46,970 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:10:47,972 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:10:48,974 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:10:49,976 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:10:50,978 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:10:51,980 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:10:52,981 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:10:53,983 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:10:54,985 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:10:55,987 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:10:56,989 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:10:57,990 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:10:58,992 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:10:59,994 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:00,996 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:01,998 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:03,000 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:04,001 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:05,003 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:06,005 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:07,007 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:08,009 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:09,011 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:10,013 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:11,015 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:12,016 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:13,018 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:14,019 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:15,021 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:16,023 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:17,025 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:18,027 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:19,029 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:20,030 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:21,032 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:22,034 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:23,036 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:24,038 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:25,040 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:26,042 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:27,044 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:28,046 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:29,047 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:30,049 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:31,051 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:32,052 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:33,054 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:34,056 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:35,058 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:36,060 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:37,061 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:38,062 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:39,064 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:40,066 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:41,067 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:42,069 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:43,070 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:44,072 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:45,074 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:46,075 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:47,077 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:48,079 INFO (MainThread-9924) waiting for 2 reservations\n\n2018-02-21 01:11:49,081 INFO (MainThread-9924) all reservations completed\n\n2018-02-21 01:11:49,082 INFO (MainThread-9924) All TFSparkNodes started\n\n2018-02-21 01:11:49,082 INFO (MainThread-9924) {\u0027job_name\u0027: \u0027worker\u0027, \u0027addr\u0027: \u0027/tmp/pymp-b8cxpv3l/listener-o1ov9dnj\u0027, \u0027worker_num\u0027: 1, \u0027task_index\u0027: 0, \u0027host\u0027: \u0027ip-10-0-0-34\u0027, \u0027authkey\u0027: b\u0027\\xae\\x97\\xd9%\\xeb\\xfaO\\x07\\xa8\\x95\\xd6\\xa5\\x8c\\xe64Y\u0027, \u0027port\u0027: 33199, \u0027ppid\u0027: 9491}\n\n2018-02-21 01:11:49,082 INFO (MainThread-9924) {\u0027job_name\u0027: \u0027ps\u0027, \u0027addr\u0027: (\u0027ip-10-0-0-131\u0027, 45547), \u0027worker_num\u0027: 0, \u0027task_index\u0027: 0, \u0027host\u0027: \u0027ip-10-0-0-131\u0027, \u0027authkey\u0027: b\u0027A\\xe4\\x1e\\xa6\\xc1\\x0bA^\\xb5\\xab\\xe4\\x9b\\xeaD\\x97\\xc2\u0027, \u0027port\u0027: 41535, \u0027ppid\u0027: 9411}\n\n2018-02-21 01:11:49,083 INFO (MainThread-9924) Stopping TensorFlow nodes\n\n2018-02-21 01:14:54,396 INFO (MainThread-9924) Shutting down cluster\n\n2018-02-21T01:14:59.845736 \u003d\u003d\u003d\u003d\u003d Stop\n"
      },
      "dateCreated": "Feb 20, 2018 9:03:09 AM",
      "dateStarted": "Feb 21, 2018 1:10:35 AM",
      "dateFinished": "Feb 21, 2018 1:14:59 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1519176880080_1815963775",
      "id": "20180221-013440_1118967240_q_REHQJ9HEG71519116979",
      "dateCreated": "Feb 21, 2018 1:34:40 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "dl-ami-test",
  "id": "REHQJ9HEG71519116979",
  "angularObjects": {
    "2D657NJSQ241519116622529:shared_process": [],
    "2D9E8V8SA241519128535986:shared_process": [],
    "2D93YKS5E241519116622539:shared_process": [],
    "2D81XKGNR241519116622525:shared_process": []
  },
  "config": {
    "isDashboard": false
  },
  "info": {},
  "source": "FCN"
}