{
  "paragraphs": [
    {
      "text": "%sh\ncd  /usr/lib/deep-learning/examples/tfos/mnist\nunzip /usr/lib/deep-learning/examples/tfos/mnist/mnist.zip",
      "dateUpdated": "Dec 28, 2017 6:07:44 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1514230583238_-1219183836",
      "id": "20171225-193623_1286573074_q_QUB2UFN5CM1523327479",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "Archive:  /usr/lib/deep-learning/examples/tfos/mnist/mnist.zip\n  inflating: t10k-images-idx3-ubyte.gz  \n extracting: t10k-labels-idx1-ubyte.gz  \n  inflating: train-images-idx3-ubyte.gz  \n extracting: train-labels-idx1-ubyte.gz  \n"
      },
      "dateCreated": "Dec 25, 2017 7:36:23 PM",
      "dateSubmitted": "Apr 10, 2018 2:31:57 AM",
      "dateStarted": "Apr 10, 2018 2:31:58 AM",
      "dateFinished": "Apr 10, 2018 2:32:00 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\n/usr/lib/spark/bin/spark-submit \\\n--master yarn \\\n--num-executors 2 \\\n--archives /usr/lib/deep-learning/examples/tfos/mnist/mnist.zip#mnist \\\n--jars /usr/lib/deep-learning/tensorflow-hadoop-plugin/tensorflow-hadoop-1.6.0.jar \\\n/usr/lib/deep-learning/examples/tfos/mnist/mnist_data_setup.py \\\n--output /deep-learning/examples/tfos/mnist_data/tfr \\\n--format tfr",
      "user": "somyak@qubole.com",
      "dateUpdated": "Apr 10, 2018 7:26:33 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1514218555123_-791596092",
      "id": "20171225-161555_635354518_q_QUB2UFN5CM1523327479",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Warning: Skipping download of JAR file:/usr/lib/deep-learning/tensorflow-hadoop-plugin/tensorflow-hadoop-1.0-SNAPSHOT.jar. Scheme file not supported\nDefault value for autoscalingv2 : false\nWarning: Local jar /usr/lib/deep-learning/tensorflow-hadoop-plugin/tensorflow-hadoop-1.0-SNAPSHOT.jar does not exist, skipping.\n18/04/10 02:32:02 main INFO Utils: Registered signal handlers for exception exit hook [TERM, HUP, INT]\n/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 \u003d\u003d np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nWARNING:tensorflow:From /usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse the retry module or similar alternatives.\nargs: Namespace(format\u003d\u0027tfr\u0027, num_partitions\u003d10, output\u003d\u0027/deep-learning/examples/tfos/mnist_data/tfr\u0027, read\u003dFalse, verify\u003dFalse)\n18/04/10 02:33:17 Thread-4 INFO SparkContext: Running Spark version 2.2.1\n18/04/10 02:33:20 Thread-4 INFO SparkContext: Submitted application: mnist_parallelize\n18/04/10 02:33:20 Thread-4 INFO SparkContext: Spark configuration:\nspark.R.cmd\u003d/usr/lib/a-4.2.0-r-3.3.2/bin/R\nspark.app.name\u003dmnist_parallelize\nspark.authenticate\u003dfalse\nspark.authenticate.enableSaslEncryption\u003dfalse\nspark.driver.cores\u003d6\nspark.driver.extraClassPath\u003d/usr/lib/spark/conf\nspark.driver.extraJavaOptions\u003d-Djava.net.preferIPv4Stack\u003dtrue -XX:ReservedCodeCacheSize\u003d100m -XX:+UseCodeCacheFlushing\nspark.driver.extraLibraryPath\u003d/usr/lib/hadoop2/lib/native\nspark.driver.memory\u003d38g\nspark.dynamicAllocation.enabled\u003dfalse\nspark.dynamicAllocation.minExecutors\u003d2\nspark.eventLog.compress\u003dtrue\nspark.eventLog.dir\u003dhdfs://172.30.0.26:9000/spark-history\nspark.eventLog.enabled\u003dtrue\nspark.executor.cores\u003d1\nspark.executor.extraJavaOptions\u003d-Djava.net.preferIPv4Stack\u003dtrue -XX:ReservedCodeCacheSize\u003d100m -XX:+UseCodeCacheFlushing\nspark.executor.instances\u003d2\nspark.executor.memory\u003d38g\nspark.executorEnv.JAVA_HOME\u003d/usr/lib/jvm/jre\nspark.executorEnv.LD_LIBRARY_PATH\u003d/usr/lib/hadoop2.6/lib/native/:/usr/local/cuda-8.0/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/lib/jvm/jre/lib/amd64/server\nspark.hadoop.fs.s3a.connection.establish.timeout\u003d5000\nspark.hadoop.fs.s3a.connection.maximum\u003d200\nspark.hadoop.hive.qubole.consistent.loadpartition\u003dfalse\nspark.hadoop.mapred.output.committer.class\u003dorg.apache.hadoop.mapred.DirectFileOutputCommitter\nspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\u003d2\nspark.hadoop.mapreduce.output.textoutputformat.overwrite\u003dfalse\nspark.hadoop.mapreduce.use.directfileoutputcommitter\u003dtrue\nspark.hadoop.mapreduce.use.parallelmergepaths\u003dtrue\nspark.hadoop.parquet.enable.summary-metadata\u003dfalse\nspark.hadoop.spark.sql.parquet.output.committer.class\u003dorg.apache.spark.sql.parquet.DirectParquetOutputCommitter\nspark.history.fs.update.interval\u003d10\nspark.history.retainedApplications\u003d5\nspark.logConf\u003dtrue\nspark.master\u003dyarn\nspark.network.sasl.serverAlwaysEncrypt\u003dfalse\nspark.network.timeout\u003d1200s\nspark.private.ip\u003dfalse\nspark.pyspark.python\u003d/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/bin/python\nspark.qubole.dynamicAllocation.estimateRequiredExecutorsV2\u003dfalse\nspark.qubole.eventLog.hdfs.async\u003dfalse\nspark.qubole.internal.default.maxExecutors\u003d2\nspark.qubole.killApplicationOnEventDrop\u003dfalse\nspark.qubole.outputformat.overwriteFileInWrite\u003dfalse\nspark.qubole.sendsql\u003dfalse\nspark.qubole.setIdleShutdownThreadAsDaemon\u003dfalse\nspark.qubole.spotloss.handle\u003dfalse\nspark.qubole.sql.hive.useDirectWrites\u003dfalse\nspark.rdd.compress\u003dTrue\nspark.repl.local.jars\u003dfile:/usr/lib/deep-learning/tensorflow-hadoop-plugin/tensorflow-hadoop-1.0-SNAPSHOT.jar\nspark.scheduler.listenerbus.eventqueue.size\u003d20000\nspark.serializer.objectStreamReset\u003d100\nspark.shs.publish.metrics\u003dfalse\nspark.shs.running.on.cluster\u003dtrue\nspark.shs.use.appcache\u003dtrue\nspark.shs.use.cluster.credentials\u003dtrue\nspark.shuffle.reduceLocality.enabled\u003dfalse\nspark.shuffle.service.enabled\u003dfalse\nspark.speculation\u003dfalse\nspark.sql.hive.metastore.jars\u003d/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/etc/hadoop:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/common/lib/*:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/common/*:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/hdfs:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/hdfs/lib/*:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/hdfs/*:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/yarn/lib/*:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/yarn/*:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/mapreduce/*:/share/hadoop/tools:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/tools/lib/*:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/tools/*:/share/hadoop/qubole:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/qubole/*:/contrib/capacity-scheduler/*.jar:/usr/lib/spark/lib/hive2/*\nspark.sql.hive.metastore.version\u003d0.13.1\nspark.sql.qubole.catalyst.normalizePredicates\u003dfalse\nspark.sql.qubole.handleCommentsWithSemicolon\u003dfalse\nspark.sql.qubole.ignoreFNFExceptions\u003dtrue\nspark.sql.qubole.metrics.enable\u003dfalse\nspark.sql.qubole.optimizer.mv.enable\u003dfalse\nspark.sql.qubole.partitionDiscoverer\u003dfalse\nspark.sql.qubole.recover.partitions\u003dfalse\nspark.sql.qubole.split.computation\u003dfalse\nspark.sql.streaming.showStreamingTab\u003dfalse\nspark.submit.deployMode\u003dclient\nspark.ui.retainedJobs\u003d33\nspark.ui.retainedStages\u003d100\nspark.yarn.dist.archives\u003dfile:/usr/lib/deep-learning/examples/tfos/mnist/mnist.zip#mnist\nspark.yarn.dist.jars\u003dfile:/usr/lib/deep-learning/tensorflow-hadoop-plugin/tensorflow-hadoop-1.0-SNAPSHOT.jar\nspark.yarn.driver.memoryOverhead\u003d9g\nspark.yarn.executor.memoryOverhead\u003d9g\nspark.yarn.historyServer.address\u003d172.30.0.26:18080\nspark.yarn.isPython\u003dtrue\nspark.yarn.maxAppAttempts\u003d1\n18/04/10 02:33:20 Thread-4 INFO SecurityManager: Changing view acls to: root\n18/04/10 02:33:20 Thread-4 INFO SecurityManager: Changing modify acls to: root\n18/04/10 02:33:20 Thread-4 INFO SecurityManager: Changing view acls groups to: \n18/04/10 02:33:20 Thread-4 INFO SecurityManager: Changing modify acls groups to: \n18/04/10 02:33:20 Thread-4 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n18/04/10 02:33:23 Thread-4 INFO Utils: Successfully started service \u0027sparkDriver\u0027 on port 38381.\n18/04/10 02:33:23 Thread-4 INFO SparkEnv: Registering MapOutputTracker\n18/04/10 02:33:23 Thread-4 INFO SparkEnv: Registering BlockManagerMaster\n18/04/10 02:33:23 Thread-4 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n18/04/10 02:33:23 Thread-4 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n18/04/10 02:33:23 Thread-4 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6caf6388-2e7a-464e-89ef-c36e48eff1e4\n18/04/10 02:33:23 Thread-4 INFO MemoryStore: MemoryStore started with capacity 20.1 GB\n18/04/10 02:33:24 Thread-4 INFO SparkEnv: Registering OutputCommitCoordinator\n18/04/10 02:33:25 Thread-4 INFO log: Logging initialized @86075ms\n18/04/10 02:33:25 Thread-4 INFO Server: jetty-9.3.z-SNAPSHOT\n18/04/10 02:33:26 Thread-4 INFO Server: Started @86732ms\n18/04/10 02:33:26 Thread-4 INFO AbstractConnector: Started ServerConnector@758c7a87{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n18/04/10 02:33:26 Thread-4 INFO Utils: Successfully started service \u0027SparkUI\u0027 on port 4040.\n18/04/10 02:33:26 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3c3f2707{/jobs,null,AVAILABLE,@Spark}\n18/04/10 02:33:26 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5d14acc4{/jobs/json,null,AVAILABLE,@Spark}\n18/04/10 02:33:26 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5e2b449c{/jobs/job,null,AVAILABLE,@Spark}\n18/04/10 02:33:26 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2fb2a5e5{/jobs/job/json,null,AVAILABLE,@Spark}\n18/04/10 02:33:26 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4f60970e{/stages,null,AVAILABLE,@Spark}\n18/04/10 02:33:26 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3ed94b28{/stages/json,null,AVAILABLE,@Spark}\n18/04/10 02:33:26 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@16494bd9{/stages/stage,null,AVAILABLE,@Spark}\n18/04/10 02:33:26 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4608596f{/stages/stage/json,null,AVAILABLE,@Spark}\n18/04/10 02:33:26 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@16158082{/stages/pool,null,AVAILABLE,@Spark}\n18/04/10 02:33:26 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@16e67491{/stages/pool/json,null,AVAILABLE,@Spark}\n18/04/10 02:33:26 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@21a1f231{/storage,null,AVAILABLE,@Spark}\n18/04/10 02:33:26 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1a5d86d8{/storage/json,null,AVAILABLE,@Spark}\n18/04/10 02:33:26 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7c032da9{/storage/rdd,null,AVAILABLE,@Spark}\n18/04/10 02:33:26 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6430f29f{/storage/rdd/json,null,AVAILABLE,@Spark}\n18/04/10 02:33:26 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@38f2fb2e{/environment,null,AVAILABLE,@Spark}\n18/04/10 02:33:26 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@278ee005{/environment/json,null,AVAILABLE,@Spark}\n18/04/10 02:33:26 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1a3be228{/executors,null,AVAILABLE,@Spark}\n18/04/10 02:33:26 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4412d22c{/executors/json,null,AVAILABLE,@Spark}\n18/04/10 02:33:26 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4e2c49e2{/executors/threadDump,null,AVAILABLE,@Spark}\n18/04/10 02:33:26 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6c725f53{/executors/threadDump/json,null,AVAILABLE,@Spark}\n18/04/10 02:33:26 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@55eb6492{/static,null,AVAILABLE,@Spark}\n18/04/10 02:33:26 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@31595714{/,null,AVAILABLE,@Spark}\n18/04/10 02:33:26 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@13973{/api,null,AVAILABLE,@Spark}\n18/04/10 02:33:26 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@7a4e8b7a{/jobs/job/kill,null,AVAILABLE,@Spark}\n18/04/10 02:33:26 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6899f1a0{/stages/stage/kill,null,AVAILABLE,@Spark}\n18/04/10 02:33:26 Thread-4 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.30.0.26:4040\n18/04/10 02:33:27 Thread-4 INFO QuboleUtils: Overload detection enabled: false \n18/04/10 02:33:28 Thread-4 INFO deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS\n18/04/10 02:33:35 Thread-4 INFO TimelineClientImpl: Timeline service address: http://172.30.0.26:8188/ws/v1/timeline/\n18/04/10 02:33:35 Thread-4 INFO RMProxy: Connecting to ResourceManager at 172.30.0.26/172.30.0.26:8032\n18/04/10 02:33:35 Thread-4 INFO Client: Requesting a new application from cluster with 6 NodeManagers\n18/04/10 02:33:36 Thread-4 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (49971 MB per container)\n18/04/10 02:33:36 Thread-4 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n18/04/10 02:33:36 Thread-4 INFO Client: Setting up container launch context for our AM\n18/04/10 02:33:36 Thread-4 INFO Client: Setting up the launch environment for our AM container\n18/04/10 02:33:36 Thread-4 INFO Client: Preparing resources for our AM container\n18/04/10 02:33:38 Thread-4 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n18/04/10 02:33:56 Thread-4 INFO deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS\n18/04/10 02:33:56 Thread-4 INFO Client: Uploading resource file:/tmp/spark-06f9bc5e-b8e0-4258-a2f1-99fe49d4d2fc/__spark_libs__1803435807704863426.zip -\u003e hdfs://172.30.0.26:9000/user/root/.sparkStaging/application_1523322990942_0167/__spark_libs__1803435807704863426.zip\n18/04/10 02:33:58 Thread-4 INFO deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS\n18/04/10 02:33:58 Thread-4 INFO Client: Uploading resource file:/usr/lib/deep-learning/tensorflow-hadoop-plugin/tensorflow-hadoop-1.0-SNAPSHOT.jar -\u003e hdfs://172.30.0.26:9000/user/root/.sparkStaging/application_1523322990942_0167/tensorflow-hadoop-1.0-SNAPSHOT.jar\n18/04/10 02:33:58 Thread-4 INFO Client: Deleted staging directory hdfs://172.30.0.26:9000/user/root/.sparkStaging/application_1523322990942_0167\n18/04/10 02:33:58 Thread-4 ERROR SparkContext: Error initializing SparkContext.\njava.io.FileNotFoundException: File file:/usr/lib/deep-learning/tensorflow-hadoop-plugin/tensorflow-hadoop-1.0-SNAPSHOT.jar does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:539)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:765)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:529)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)\n\tat org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:371)\n\tat org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:490)\n\tat org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$11$$anonfun$apply$6.apply(Client.scala:612)\n\tat org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$11$$anonfun$apply$6.apply(Client.scala:611)\n\tat scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:74)\n\tat org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$11.apply(Client.scala:611)\n\tat org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$11.apply(Client.scala:610)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:610)\n\tat org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:841)\n\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:170)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:85)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:173)\n\tat org.apache.spark.SparkContext.\u003cinit\u003e(SparkContext.scala:618)\n\tat org.apache.spark.api.java.JavaSparkContext.\u003cinit\u003e(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:236)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n18/04/10 02:33:58 Thread-4 INFO AbstractConnector: Stopped Spark@758c7a87{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n18/04/10 02:33:58 Thread-4 INFO SparkUI: Stopped Spark web UI at http://172.30.0.26:4040\n18/04/10 02:33:58 dispatcher-event-loop-1 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n18/04/10 02:33:58 Thread-4 ERROR Utils: Neglecting uncaught exception in thread Thread-4\njava.util.NoSuchElementException: None.get\n\tat scala.None$.get(Option.scala:347)\n\tat scala.None$.get(Option.scala:345)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.stop(YarnClientSchedulerBackend.scala:184)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:527)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1701)\n\tat org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:2101)\n\tat org.apache.spark.util.Utils$.tryQuboleLogError(Utils.scala:1372)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2100)\n\tat org.apache.spark.SparkContext.\u003cinit\u003e(SparkContext.scala:697)\n\tat org.apache.spark.api.java.JavaSparkContext.\u003cinit\u003e(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:236)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n18/04/10 02:33:58 dispatcher-event-loop-2 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: Sending StopAM(true) to AppMaster\n18/04/10 02:33:58 dispatcher-event-loop-0 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n18/04/10 02:33:58 Thread-4 INFO MemoryStore: MemoryStore cleared\n18/04/10 02:33:58 Thread-4 INFO BlockManager: BlockManager stopped\n18/04/10 02:33:58 Thread-4 INFO BlockManagerMaster: BlockManagerMaster stopped\n18/04/10 02:33:58 Thread-4 WARN MetricsSystem: Stopping a MetricsSystem that is not running\n18/04/10 02:33:58 dispatcher-event-loop-1 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n18/04/10 02:33:59 Thread-4 INFO SparkContext: Successfully stopped SparkContext\nTraceback (most recent call last):\n  File \"/usr/lib/deep-learning/examples/tfos/mnist/mnist_data_setup.py\", line 149, in \u003cmodule\u003e\n    sc \u003d SparkContext(conf\u003dSparkConf().setAppName(\"mnist_parallelize\"))\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py\", line 118, in __init__\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py\", line 180, in _do_init\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py\", line 273, in _initialize_context\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1401, in __call__\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.io.FileNotFoundException: File file:/usr/lib/deep-learning/tensorflow-hadoop-plugin/tensorflow-hadoop-1.0-SNAPSHOT.jar does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:539)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:765)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:529)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)\n\tat org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:371)\n\tat org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:490)\n\tat org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$11$$anonfun$apply$6.apply(Client.scala:612)\n\tat org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$11$$anonfun$apply$6.apply(Client.scala:611)\n\tat scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:74)\n\tat org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$11.apply(Client.scala:611)\n\tat org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$11.apply(Client.scala:610)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:610)\n\tat org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:841)\n\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:170)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:85)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:173)\n\tat org.apache.spark.SparkContext.\u003cinit\u003e(SparkContext.scala:618)\n\tat org.apache.spark.api.java.JavaSparkContext.\u003cinit\u003e(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:236)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n\n18/04/10 02:34:04 main INFO SparkContext: sc.stop called from [SparkSubmit.exceptionExitHook[failure]]\n18/04/10 02:34:04 main INFO SparkContext: SparkContext already stopped.\n18/04/10 02:34:04 main INFO YarnClientSchedulerBackend: yarn-client securityManager checkExit, exit code: 1\n18/04/10 02:34:04 main INFO SparkContext: sc.stop called from [yarn-client App exit with exit code: 1]\n18/04/10 02:34:04 main INFO SparkContext: SparkContext already stopped.\n18/04/10 02:34:04 Thread-1 INFO ShutdownHookManager: Shutdown hook called\n18/04/10 02:34:04 Thread-1 INFO ShutdownHookManager: Deleting directory /tmp/spark-244f2a31-2673-4cca-9e20-2717ed6484da\n18/04/10 02:34:04 Thread-1 INFO ShutdownHookManager: Deleting directory /tmp/spark-06f9bc5e-b8e0-4258-a2f1-99fe49d4d2fc\nExitValue: 1"
      },
      "dateCreated": "Dec 25, 2017 4:15:55 PM",
      "dateSubmitted": "Apr 10, 2018 2:31:58 AM",
      "dateStarted": "Apr 10, 2018 2:31:58 AM",
      "dateFinished": "Apr 10, 2018 2:34:04 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\nexport LD_LIBRARY_PATH\u003d/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.151-1.b12.35.amzn1.x86_64/jre/lib/amd64/server:$LD_LIBRARY_PATH",
      "dateUpdated": "Dec 28, 2017 6:10:30 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1514441361561_-1137784858",
      "id": "20171228-060921_201869850_q_QUB2UFN5CM1523327479",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Dec 28, 2017 6:09:21 AM",
      "dateSubmitted": "Apr 10, 2018 2:31:58 AM",
      "dateStarted": "Apr 10, 2018 2:31:59 AM",
      "dateFinished": "Apr 10, 2018 2:31:59 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\necho $LD_LIBRARY_PATH",
      "dateUpdated": "Dec 28, 2017 6:10:49 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1514441435373_-594211283",
      "id": "20171228-061035_1905143372_q_QUB2UFN5CM1523327479",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "/usr/java/jdk1.8.0_121/jre/lib/amd64/server:/usr/java/jdk1.8.0_121/jre/lib/amd64:/usr/java/jdk1.8.0_121/jre/../lib/amd64:/usr/lib/hadoop2.6/lib/native/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/java/jdk1.8.0_121//jre/lib/amd64/server:\n"
      },
      "dateCreated": "Dec 28, 2017 6:10:35 AM",
      "dateSubmitted": "Apr 10, 2018 2:31:59 AM",
      "dateStarted": "Apr 10, 2018 2:31:59 AM",
      "dateFinished": "Apr 10, 2018 2:31:59 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom pyspark.context import SparkContext\nfrom pyspark.conf import SparkConf\n\nimport sys\nprint(sys.executable)\n\nimport argparse\nimport os\nimport numpy\nimport sys\nimport tensorflow as tf\nimport threading\nfrom datetime import datetime\n\nfrom tensorflowonspark import TFCluster",
      "dateUpdated": "Dec 28, 2017 9:23:13 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [],
        "numCompletedTasks": 0,
        "numTasks": 0,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1514228606679_475062617",
      "id": "20171225-190326_1095226201_q_QUB2UFN5CM1523327479",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/bin/python\n/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 \u003d\u003d np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n"
      },
      "dateCreated": "Dec 25, 2017 7:03:26 PM",
      "dateSubmitted": "Apr 10, 2018 2:31:57 AM",
      "dateStarted": "Apr 10, 2018 2:32:17 AM",
      "dateFinished": "Apr 10, 2018 2:33:14 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1514446450135_-149404172",
      "id": "20171228-073410_1049259817_q_QUB2UFN5CM1523327479",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Dec 28, 2017 7:34:10 AM",
      "dateSubmitted": "Apr 10, 2018 2:32:17 AM",
      "dateStarted": "Apr 10, 2018 2:33:15 AM",
      "dateFinished": "Apr 10, 2018 2:33:15 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\necho $LD_LIBRARY_PATH",
      "dateUpdated": "Dec 28, 2017 7:34:09 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1514446440509_-1037804707",
      "id": "20171228-073400_1850734434_q_QUB2UFN5CM1523327479",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "/usr/java/jdk1.8.0_121/jre/lib/amd64/server:/usr/java/jdk1.8.0_121/jre/lib/amd64:/usr/java/jdk1.8.0_121/jre/../lib/amd64:/usr/lib/hadoop2.6/lib/native/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/java/jdk1.8.0_121//jre/lib/amd64/server:\n"
      },
      "dateCreated": "Dec 28, 2017 7:34:00 AM",
      "dateSubmitted": "Apr 10, 2018 2:31:59 AM",
      "dateStarted": "Apr 10, 2018 2:31:59 AM",
      "dateFinished": "Apr 10, 2018 2:31:59 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "def print_log(worker_num, arg):\n    print(\"{}: \".format(worker_num))\n    print(arg)\n\n\ndef map_fun(args, ctx):\n    from tensorflowonspark import TFNode\n    from datetime import datetime\n    import getpass\n    import math\n    import numpy\n    import os\n    import signal\n    import tensorflow as tf\n    import time\n\n    IMAGE_PIXELS \u003d 28\n    worker_num \u003d ctx.worker_num\n    job_name \u003d ctx.job_name\n    task_index \u003d ctx.task_index\n    cluster_spec \u003d ctx.cluster_spec\n    num_workers \u003d len(cluster_spec[\u0027worker\u0027])\n\n    # Delay PS nodes a bit, since workers seem to reserve GPUs more quickly/reliably (w/o conflict)\n    if job_name \u003d\u003d \"ps\":\n        time.sleep((worker_num + 1) * 5)\n\n    # Parameters\n    hidden_units \u003d 128\n    batch_size \u003d 100\n\n    # Get TF cluster and server instances\n    cluster, server \u003d TFNode.start_cluster_server(ctx, 1, args[\"rdma\"])\n\n    def read_csv_examples(image_dir, label_dir, batch_size\u003d100, num_epochs\u003dNone, task_index\u003dNone, num_workers\u003dNone):\n        print_log(worker_num, \"num_epochs: {0}\".format(num_epochs))\n        # Setup queue of csv image filenames\n        tf_record_pattern \u003d os.path.join(image_dir, \u0027part-*\u0027)\n        images \u003d tf.gfile.Glob(tf_record_pattern)\n        print_log(worker_num, \"images: {0}\".format(images))\n        image_queue \u003d tf.train.string_input_producer(\n            images, shuffle\u003dFalse, capacity\u003d1000, num_epochs\u003dnum_epochs, name\u003d\"image_queue\")\n\n        # Setup queue of csv label filenames\n        tf_record_pattern \u003d os.path.join(label_dir, \u0027part-*\u0027)\n        labels \u003d tf.gfile.Glob(tf_record_pattern)\n        print_log(worker_num, \"labels: {0}\".format(labels))\n        label_queue \u003d tf.train.string_input_producer(\n            labels, shuffle\u003dFalse, capacity\u003d1000, num_epochs\u003dnum_epochs, name\u003d\"label_queue\")\n\n        # Setup reader for image queue\n        img_reader \u003d tf.TextLineReader(name\u003d\"img_reader\")\n        _, img_csv \u003d img_reader.read(image_queue)\n        image_defaults \u003d [[1.0] for col in range(784)]\n        img \u003d tf.pack(tf.decode_csv(img_csv, image_defaults))\n        # Normalize values to [0,1]\n        norm \u003d tf.constant(255, dtype\u003dtf.float32, shape\u003d(784,))\n        image \u003d tf.div(img, norm)\n        print_log(worker_num, \"image: {0}\".format(image))\n\n        # Setup reader for label queue\n        label_reader \u003d tf.TextLineReader(name\u003d\"label_reader\")\n        _, label_csv \u003d label_reader.read(label_queue)\n        label_defaults \u003d [[1.0] for col in range(10)]\n        label \u003d tf.pack(tf.decode_csv(label_csv, label_defaults))\n        print_log(worker_num, \"label: {0}\".format(label))\n\n        # Return a batch of examples\n        return tf.train.batch([image, label], batch_size, num_threads\u003dargs[\"readers\"], name\u003d\"batch_csv\")\n\n    def read_tfr_examples(path, batch_size\u003d100, num_epochs\u003dNone, task_index\u003dNone, num_workers\u003dNone):\n        print_log(worker_num, \"num_epochs: {0}\".format(num_epochs))\n\n        # Setup queue of TFRecord filenames\n        tf_record_pattern \u003d os.path.join(path, \u0027part-*\u0027)\n        files \u003d tf.gfile.Glob(tf_record_pattern)\n        queue_name \u003d \"file_queue\"\n\n        # split input files across workers, if specified\n        if task_index is not None and num_workers is not None:\n            num_files \u003d len(files)\n            files \u003d files[task_index:num_files:num_workers]\n            queue_name \u003d \"file_queue_{0}\".format(task_index)\n\n        print_log(worker_num, \"files: {0}\".format(files))\n        file_queue \u003d tf.train.string_input_producer(\n            files, shuffle\u003dFalse, capacity\u003d1000, num_epochs\u003dnum_epochs, name\u003dqueue_name)\n\n        # Setup reader for examples\n        reader \u003d tf.TFRecordReader(name\u003d\"reader\")\n        _, serialized \u003d reader.read(file_queue)\n        feature_def \u003d {\u0027label\u0027: tf.FixedLenFeature(\n            [10], tf.int64), \u0027image\u0027: tf.FixedLenFeature([784], tf.int64)}\n        features \u003d tf.parse_single_example(serialized, feature_def)\n        norm \u003d tf.constant(255, dtype\u003dtf.float32, shape\u003d(784,))\n        image \u003d tf.div(tf.to_float(features[\u0027image\u0027]), norm)\n        print_log(worker_num, \"image: {0}\".format(image))\n        label \u003d tf.to_float(features[\u0027label\u0027])\n        print_log(worker_num, \"label: {0}\".format(label))\n\n        # Return a batch of examples\n        return tf.train.batch([image, label], batch_size, num_threads\u003dargs[\"readers\"], name\u003d\"batch\")\n\n    if job_name \u003d\u003d \"ps\":\n        server.join()\n    elif job_name \u003d\u003d \"worker\":\n        # Assigns ops to the local worker by default.\n        with tf.device(tf.train.replica_device_setter(\n                worker_device\u003d\"/job:worker/task:%d\" % task_index,\n                cluster\u003dcluster)):\n\n            # Variables of the hidden layer\n            hid_w \u003d tf.Variable(tf.truncated_normal([IMAGE_PIXELS * IMAGE_PIXELS, hidden_units],\n                                                    stddev\u003d1.0 / IMAGE_PIXELS), name\u003d\"hid_w\")\n            hid_b \u003d tf.Variable(tf.zeros([hidden_units]), name\u003d\"hid_b\")\n            tf.summary.histogram(\"hidden_weights\", hid_w)\n\n            # Variables of the softmax layer\n            sm_w \u003d tf.Variable(tf.truncated_normal([hidden_units, 10],\n                                                   stddev\u003d1.0 / math.sqrt(hidden_units)), name\u003d\"sm_w\")\n            sm_b \u003d tf.Variable(tf.zeros([10]), name\u003d\"sm_b\")\n            tf.summary.histogram(\"softmax_weights\", sm_w)\n\n            # Placeholders or QueueRunner/Readers for input data\n            num_epochs \u003d 1 if args[\"mode\"] \u003d\u003d \"inference\" else None if args[\"epochs\"] \u003d\u003d 0 else args[\"epochs\"]\n            index \u003d task_index if args[\"mode\"] \u003d\u003d \"inference\" else None\n            workers \u003d num_workers if args[\"mode\"] \u003d\u003d \"inference\" else None\n\n            if args[\"format\"] \u003d\u003d \"csv\":\n                images \u003d TFNode.hdfs_path(args[\"images\"], ctx.defaultFS, ctx.working_dir)\n                labels \u003d TFNode.hdfs_path(args[\"labels\"], ctx.defaultFS, ctx.working_dir)\n                x, y_ \u003d read_csv_examples(\n                    images, labels, 100, num_epochs, index, workers)\n            elif args[\"format\"] \u003d\u003d \"tfr\":\n                images \u003d TFNode.hdfs_path(args[\"images\"], ctx.defaultFS, ctx.working_dir)\n                x, y_ \u003d read_tfr_examples(\n                    images, 100, num_epochs, index, workers)\n            else:\n                raise(\"{0} format not supported for tf input mode\".format(\n                    args[\"format\"]))\n\n            x_img \u003d tf.reshape(x, [-1, IMAGE_PIXELS, IMAGE_PIXELS, 1])\n            tf.summary.image(\"x_img\", x_img)\n\n            hid_lin \u003d tf.nn.xw_plus_b(x, hid_w, hid_b)\n            hid \u003d tf.nn.relu(hid_lin)\n\n            y \u003d tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\n\n            global_step \u003d tf.Variable(0)\n\n            loss \u003d -tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n            tf.summary.scalar(\"loss\", loss)\n            train_op \u003d tf.train.AdagradOptimizer(0.01).minimize(\n                loss, global_step\u003dglobal_step)\n\n            # Test trained model\n            label \u003d tf.argmax(y_, 1, name\u003d\"label\")\n            prediction \u003d tf.argmax(y, 1, name\u003d\"prediction\")\n            correct_prediction \u003d tf.equal(prediction, label)\n            accuracy \u003d tf.reduce_mean(\n                tf.cast(correct_prediction, tf.float32), name\u003d\"accuracy\")\n            tf.summary.scalar(\"acc\", accuracy)\n\n            saver \u003d tf.train.Saver()\n            summary_op \u003d tf.summary.merge_all()\n            init_op \u003d tf.global_variables_initializer()\n\n        # Create a \"supervisor\", which oversees the training process and stores model state into HDFS\n        logdir \u003d TFNode.hdfs_path(args[\"model\"], ctx.defaultFS, ctx.working_dir)\n        print(\"tensorflow model path: {0}\".format(logdir))\n\n        summary_writer \u003d TFNode.get_summary_writer(ctx)\n        \n        if args[\"mode\"] \u003d\u003d \"train\":\n            sv \u003d tf.train.Supervisor(is_chief\u003d(task_index \u003d\u003d 0),\n                                     logdir\u003dlogdir,\n                                     init_op\u003dinit_op,\n                                     summary_op\u003dNone,\n                                     saver\u003dsaver,\n                                     global_step\u003dglobal_step,\n                                     stop_grace_secs\u003d300,\n                                     save_model_secs\u003d10)\n        else:\n            sv \u003d tf.train.Supervisor(is_chief\u003d(task_index \u003d\u003d 0),\n                                     logdir\u003dlogdir,\n                                     summary_op\u003dNone,\n                                     saver\u003dsaver,\n                                     global_step\u003dglobal_step,\n                                     stop_grace_secs\u003d300,\n                                     save_model_secs\u003d0)\n            output_dir \u003d TFNode.hdfs_path(args[\"output\"], ctx.defaultFS, ctx.working_dir)\n            output_file \u003d tf.gfile.Open(\n                \"{0}/part-{1:05d}\".format(output_dir, worker_num), mode\u003d\u0027w\u0027)\n\n        # The supervisor takes care of session initialization, restoring from\n        # a checkpoint, and closing when done or an error occurs.\n        with sv.managed_session(server.target) as sess:\n            print(\"{0} session ready\".format(datetime.now().isoformat()))\n\n            # Loop until the supervisor shuts down or 1000000 steps have completed.\n            step \u003d 0\n            count \u003d 0\n            while not sv.should_stop() and step \u003c args[\"steps\"]:\n                # Run a training step asynchronously.\n                # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n                # perform *synchronous* training.\n\n                # using QueueRunners/Readers\n                if args[\"mode\"] \u003d\u003d \"train\":\n                    if (step % 100 \u003d\u003d 0):\n                        print(\"{0} step: {1} accuracy: {2}\".format(\n                            datetime.now().isoformat(), step, sess.run(accuracy)))\n                    _, summary, step \u003d sess.run(\n                        [train_op, summary_op, global_step])\n                    if sv.is_chief:\n                        summary_writer.add_summary(summary, step)\n                else:  # args[\"mode\"] \u003d\u003d \"inference\"\n                    labels, pred, acc \u003d sess.run([label, prediction, accuracy])\n                    #print(\"label: {0}, pred: {1}\".format(labels, pred))\n                    print(\"acc: {0}\".format(acc))\n                    for i in range(len(labels)):\n                        count +\u003d 1\n                        output_file.write(\n                            \"{0} {1}\\n\".format(labels[i], pred[i]))\n                    print(\"count: {0}\".format(count))\n\n        if args[\"mode\"] \u003d\u003d \"inference\":\n            output_file.close()\n            # Delay chief worker from shutting down supervisor during inference, since it can load model, start session,\n            # run inference and request stop before the other workers even start/sync their sessions.\n            if task_index \u003d\u003d 0:\n                time.sleep(60)\n\n        # Ask for all the services to stop.\n        print(\"{0} stopping supervisor\".format(datetime.now().isoformat()))\n        sv.stop()",
      "dateUpdated": "Dec 28, 2017 9:23:22 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [],
        "numCompletedTasks": 0,
        "numTasks": 0,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1514231345671_-693348105",
      "id": "20171225-194905_967595234_q_QUB2UFN5CM1523327479",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Dec 25, 2017 7:49:05 PM",
      "dateSubmitted": "Apr 10, 2018 2:33:15 AM",
      "dateStarted": "Apr 10, 2018 2:33:15 AM",
      "dateFinished": "Apr 10, 2018 2:33:15 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "num_executors \u003d 2\nnum_ps \u003d 1\n\nargs \u003d {\"epochs\": 1,\n        \"format\": \"tfr\",\n        \"images\":\"/deep-learning/examples/tfos/mnist_data/tfr/train\",\n        \"labels\": None,\n        \"model\": \"mnist_model\",\n        \"cluster_size\": num_executors,\n        \"output\": \"predictions\",\n        \"readers\": 1,\n        \"steps\": 1000,\n        \"mode\": \"train\",\n        \"rdma\": False\n        }\n\nprint(\"{0} \u003d\u003d\u003d\u003d\u003d Start\".format(datetime.now().isoformat()))\ncluster \u003d TFCluster.run(sc, map_fun, args, args[\"cluster_size\"],\n                        num_ps, TFCluster.InputMode.TENSORFLOW)\ncluster.shutdown()\n\nprint(\"{0} \u003d\u003d\u003d\u003d\u003d Stop\".format(datetime.now().isoformat()))",
      "dateUpdated": "Dec 28, 2017 9:23:28 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [],
        "numCompletedTasks": 0,
        "numTasks": 0,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1514231275467_-910220603",
      "id": "20171225-194755_2015858841_q_QUB2UFN5CM1523327479",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "2017-12-28T09:23:28.362021 \u003d\u003d\u003d\u003d\u003d Start\n2017-12-28 09:23:28,362 INFO (MainThread-5992) Reserving TFSparkNodes.\n\n2017-12-28 09:23:28,364 INFO (MainThread-5992) listening for reservations at (\u0027ip-10-16-178-17\u0027, 35013)\n\n2017-12-28 09:23:28,366 INFO (MainThread-5992) Starting TensorFlow on executors\n\n2017-12-28 09:23:28,383 INFO (MainThread-5992) Waiting for TFSparkNodes to start\n\n2017-12-28 09:23:28,384 INFO (MainThread-5992) waiting for 2 reservations\n\n2017-12-28 09:23:29,387 INFO (MainThread-5992) waiting for 2 reservations\n\n2017-12-28 09:23:30,389 INFO (MainThread-5992) waiting for 2 reservations\n\n2017-12-28 09:23:31,391 INFO (MainThread-5992) waiting for 2 reservations\n\n2017-12-28 09:23:32,393 INFO (MainThread-5992) waiting for 2 reservations\n\n2017-12-28 09:23:33,395 INFO (MainThread-5992) waiting for 2 reservations\n\n2017-12-28 09:23:34,398 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:23:35,400 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:23:36,401 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:23:37,404 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:23:38,406 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:23:39,407 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:23:40,410 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:23:41,412 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:23:42,414 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:23:43,416 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:23:44,418 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:23:45,420 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:23:46,422 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:23:47,424 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:23:48,426 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:23:49,428 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:23:50,430 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:23:51,433 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:23:52,435 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:23:53,437 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:23:54,439 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:23:55,441 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:23:56,443 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:23:57,445 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:23:58,447 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:23:59,449 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:00,451 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:01,453 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:02,455 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:03,457 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:04,459 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:05,461 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:06,463 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:07,466 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:08,468 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:09,470 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:10,472 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:11,474 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:12,476 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:13,478 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:14,480 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:15,482 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:16,484 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:17,486 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:18,488 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:19,490 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:20,492 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:21,494 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:22,496 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:23,498 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:24,500 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:25,502 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:26,504 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:27,506 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:28,508 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:29,510 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:30,512 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:31,514 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:32,516 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:33,518 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:34,520 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:35,522 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:36,524 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:37,526 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:38,527 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:39,529 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:40,531 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:41,533 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:42,535 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:43,537 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:44,540 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:45,542 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:46,544 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:47,546 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:48,548 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:49,550 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:50,552 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:51,554 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:52,556 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:53,558 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:54,560 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:55,562 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:56,565 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:57,567 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:58,568 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:24:59,570 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:00,572 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:01,574 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:02,576 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:03,578 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:04,580 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:05,582 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:06,583 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:07,586 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:08,588 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:09,590 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:10,592 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:11,594 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:12,596 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:13,597 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:14,599 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:15,602 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:16,604 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:17,606 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:18,608 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:19,610 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:20,612 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:21,614 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:22,616 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:23,618 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:24,620 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:25,622 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:26,623 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:27,625 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:28,627 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:29,629 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:30,631 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:31,633 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:32,635 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:33,637 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:34,639 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:35,641 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:36,643 INFO (MainThread-5992) waiting for 1 reservations\n\n2017-12-28 09:25:37,645 INFO (MainThread-5992) all reservations completed\n\n2017-12-28 09:25:37,646 INFO (MainThread-5992) All TFSparkNodes started\n\n2017-12-28 09:25:37,647 INFO (MainThread-5992) {\u0027host\u0027: \u0027ip-10-138-195-160\u0027, \u0027addr\u0027: (\u0027ip-10-138-195-160\u0027, 43219), \u0027worker_num\u0027: 0, \u0027task_index\u0027: 0, \u0027job_name\u0027: \u0027ps\u0027, \u0027authkey\u0027: b\u0027\\xae\\x8f\\x8dp\\xa2\\xdcJB\\x9d\\xc3\\r\\x8e+\\x1d\\xf7\\xd9\u0027, \u0027port\u0027: 42935, \u0027ppid\u0027: 4614}\n\n2017-12-28 09:25:37,647 INFO (MainThread-5992) {\u0027host\u0027: \u0027ip-10-93-184-101\u0027, \u0027addr\u0027: \u0027/tmp/pymp-bcdpo_4w/listener-o1aic10r\u0027, \u0027worker_num\u0027: 1, \u0027task_index\u0027: 0, \u0027job_name\u0027: \u0027worker\u0027, \u0027authkey\u0027: b\u0027\\xf6\\xa3\\xa9\\x7f\\xb1\\xb6M,\\x99r\\xfa\\xf6oXJ\\xce\u0027, \u0027port\u0027: 43737, \u0027ppid\u0027: 1875}\n\n2017-12-28 09:25:37,648 INFO (MainThread-5992) Stopping TensorFlow nodes\n\n2017-12-28 09:29:17,374 INFO (MainThread-5992) Shutting down cluster\n\n2017-12-28T09:29:22.826088 \u003d\u003d\u003d\u003d\u003d Stop\n"
      },
      "dateCreated": "Dec 25, 2017 7:47:55 PM",
      "dateSubmitted": "Apr 10, 2018 2:33:15 AM",
      "dateStarted": "Apr 10, 2018 2:33:15 AM",
      "dateFinished": "Dec 28, 2017 9:29:22 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%knitr",
      "dateUpdated": "Dec 28, 2017 7:56:19 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/r"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1514231533734_1028127387",
      "id": "20171225-195213_1718542748_q_QUB2UFN5CM1523327479",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cscript type\u003d\"text/javascript\"\u003e\nwindow.onload \u003d function() {\n  var imgs \u003d document.getElementsByTagName(\u0027img\u0027), i, img;\n  for (i \u003d 0; i \u003c imgs.length; i++) {\n    img \u003d imgs[i];\n    // center an image if it is the only element of its parent\n    if (img.parentElement.childElementCount \u003d\u003d\u003d 1)\n      img.parentElement.style.textAlign \u003d \u0027center\u0027;\n  }\n};\n\u003c/script\u003e"
      },
      "dateCreated": "Dec 25, 2017 7:52:13 PM",
      "dateSubmitted": "Apr 10, 2018 2:33:15 AM",
      "dateStarted": "Apr 10, 2018 2:33:15 AM",
      "dateFinished": "Apr 10, 2018 2:33:24 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nimport sys.process._\nsys.env(\"LD_LIBRARY_PATH\")",
      "dateUpdated": "Dec 28, 2017 8:53:37 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [],
        "numCompletedTasks": 0,
        "numTasks": 0,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1514446704507_667000701",
      "id": "20171228-073824_1818877258_q_QUB2UFN5CM1523327479",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nimport sys.process._\n\nres0: String \u003d /usr/java/jdk1.8.0_121/jre/lib/amd64/server:/usr/java/jdk1.8.0_121/jre/lib/amd64:/usr/java/jdk1.8.0_121/jre/../lib/amd64:/usr/lib/hadoop2.6/lib/native/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/java/jdk1.8.0_121//jre/lib/amd64/server:/media/ebs2/yarn/local/usercache/root/appcache/application_1523322990942_0161/container_1523322990942_0161_01_000004:/usr/lib/hadoop2/lib/native:/usr/lib/hadoop2/lib/native\n"
      },
      "dateCreated": "Dec 28, 2017 7:38:24 AM",
      "dateSubmitted": "Apr 10, 2018 2:33:16 AM",
      "dateStarted": "Apr 10, 2018 2:33:16 AM",
      "dateFinished": "Apr 10, 2018 2:33:17 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nprint(sc.parallelize(1 to 10).map(x \u003d\u003e sys.env(\"LD_LIBRARY_PATH\")).collect)",
      "dateUpdated": "Dec 28, 2017 9:09:30 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "JOB UI",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "https://qa3.qubole.net/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F172.30.0.26%3A8088%2Fproxy%2Fapplication_1523322990942_0163/jobs/job?spark\u003dtrue\u0026id\u003d1"
          ],
          "interpreterSettingId": "2DDPX12U5283971523327498530"
        }
      },
      "paragraphProgress": {
        "jobs": [
          {
            "id": 1,
            "jobUrl": "https://qa3.qubole.net/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F172.30.0.26%3A8088%2Fproxy%2Fapplication_1523322990942_0163/jobs/job?spark\u003dtrue\u0026id\u003d1",
            "numTasks": 2,
            "numCompletedTasks": 0,
            "stages": [
              {
                "id": 1,
                "completed": false,
                "stageUrl": "https://qa3.qubole.net/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F172.30.0.26%3A8088%2Fproxy%2Fapplication_1523322990942_0163/stages/stage/?id\u003d1\u0026attempt\u003d0",
                "numCompleteTasks": 0,
                "numActiveTasks": 0,
                "numFailedTasks": 0,
                "numTotalTasks": 2
              }
            ],
            "status": "Running"
          }
        ],
        "numCompletedTasks": 0,
        "numTasks": 2,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1514451012304_-1648109528",
      "id": "20171228-085012_14013020_q_QUB2UFN5CM1523327479",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "[Ljava.lang.String;@53e18e1e"
      },
      "dateCreated": "Dec 28, 2017 8:50:12 AM",
      "dateSubmitted": "Apr 10, 2018 2:33:16 AM",
      "dateStarted": "Apr 10, 2018 2:33:17 AM",
      "dateFinished": "Dec 28, 2017 9:09:34 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nvar a \u003d sc.parallelize(1 to 10).map(x \u003d\u003e sys.env(\"LD_LIBRARY_PATH\")).collect\na.foreach(println)",
      "dateUpdated": "Dec 28, 2017 9:22:03 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [
          {
            "id": 0,
            "jobUrl": "https://qa.qubole.net/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F10.184.236.203%3A8088%2Fproxy%2Fapplication_1514440521907_0017/jobs/job?spark\u003dtrue\u0026id\u003d0",
            "numTasks": 2,
            "numCompletedTasks": 2,
            "stages": [
              {
                "id": 0,
                "completed": true,
                "stageUrl": "https://qa.qubole.net/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F10.184.236.203%3A8088%2Fproxy%2Fapplication_1514440521907_0017/stages/stage/?id\u003d0\u0026attempt\u003d0",
                "numCompleteTasks": 2,
                "numActiveTasks": 0,
                "numFailedTasks": 0,
                "numTotalTasks": 2
              }
            ],
            "status": "Success"
          }
        ],
        "numCompletedTasks": 2,
        "numTasks": 2,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1514451576545_1901042544",
      "id": "20171228-085936_1984757508_q_QUB2UFN5CM1523327479",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "a: Array[String] \u003d Array(/usr/java/jdk1.8.0_60/jre/lib/amd64/server:/usr/java/jdk1.8.0_60/jre/lib/amd64:/usr/java/jdk1.8.0_60/jre/../lib/amd64:/usr/java/jdk1.8.0_121/jre/lib/amd64/server:/usr/lib/hadoop2.6/lib/native/:/usr/local/cuda-8.0/lib64:/usr/local/cuda/extras/CUPTI/lib64/, /usr/java/jdk1.8.0_60/jre/lib/amd64/server:/usr/java/jdk1.8.0_60/jre/lib/amd64:/usr/java/jdk1.8.0_60/jre/../lib/amd64:/usr/java/jdk1.8.0_121/jre/lib/amd64/server:/usr/lib/hadoop2.6/lib/native/:/usr/local/cuda-8.0/lib64:/usr/local/cuda/extras/CUPTI/lib64/, /usr/java/jdk1.8.0_60/jre/lib/amd64/server:/usr/java/jdk1.8.0_60/jre/lib/amd64:/usr/java/jdk1.8.0_60/jre/../lib/amd64:/usr/java/jdk1.8.0_121/jre/lib/amd64/server:/usr/lib/hadoop2.6/lib/native/:/usr/local/cuda-8.0/lib64:/usr/local/cuda/extras/CUPTI/lib64/, /usr.../usr/java/jdk1.8.0_60/jre/lib/amd64/server:/usr/java/jdk1.8.0_60/jre/lib/amd64:/usr/java/jdk1.8.0_60/jre/../lib/amd64:/usr/java/jdk1.8.0_121/jre/lib/amd64/server:/usr/lib/hadoop2.6/lib/native/:/usr/local/cuda-8.0/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n/usr/java/jdk1.8.0_60/jre/lib/amd64/server:/usr/java/jdk1.8.0_60/jre/lib/amd64:/usr/java/jdk1.8.0_60/jre/../lib/amd64:/usr/java/jdk1.8.0_121/jre/lib/amd64/server:/usr/lib/hadoop2.6/lib/native/:/usr/local/cuda-8.0/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n/usr/java/jdk1.8.0_60/jre/lib/amd64/server:/usr/java/jdk1.8.0_60/jre/lib/amd64:/usr/java/jdk1.8.0_60/jre/../lib/amd64:/usr/java/jdk1.8.0_121/jre/lib/amd64/server:/usr/lib/hadoop2.6/lib/native/:/usr/local/cuda-8.0/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n/usr/java/jdk1.8.0_60/jre/lib/amd64/server:/usr/java/jdk1.8.0_60/jre/lib/amd64:/usr/java/jdk1.8.0_60/jre/../lib/amd64:/usr/java/jdk1.8.0_121/jre/lib/amd64/server:/usr/lib/hadoop2.6/lib/native/:/usr/local/cuda-8.0/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n/usr/java/jdk1.8.0_60/jre/lib/amd64/server:/usr/java/jdk1.8.0_60/jre/lib/amd64:/usr/java/jdk1.8.0_60/jre/../lib/amd64:/usr/java/jdk1.8.0_121/jre/lib/amd64/server:/usr/lib/hadoop2.6/lib/native/:/usr/local/cuda-8.0/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n/usr/java/jdk1.8.0_60/jre/lib/amd64/server:/usr/java/jdk1.8.0_60/jre/lib/amd64:/usr/java/jdk1.8.0_60/jre/../lib/amd64:/usr/java/jdk1.8.0_121/jre/lib/amd64/server:/usr/lib/hadoop2.6/lib/native/:/usr/local/cuda-8.0/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n/usr/java/jdk1.8.0_60/jre/lib/amd64/server:/usr/java/jdk1.8.0_60/jre/lib/amd64:/usr/java/jdk1.8.0_60/jre/../lib/amd64:/usr/java/jdk1.8.0_121/jre/lib/amd64/server:/usr/lib/hadoop2.6/lib/native/:/usr/local/cuda-8.0/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n/usr/java/jdk1.8.0_60/jre/lib/amd64/server:/usr/java/jdk1.8.0_60/jre/lib/amd64:/usr/java/jdk1.8.0_60/jre/../lib/amd64:/usr/java/jdk1.8.0_121/jre/lib/amd64/server:/usr/lib/hadoop2.6/lib/native/:/usr/local/cuda-8.0/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n/usr/java/jdk1.8.0_60/jre/lib/amd64/server:/usr/java/jdk1.8.0_60/jre/lib/amd64:/usr/java/jdk1.8.0_60/jre/../lib/amd64:/usr/java/jdk1.8.0_121/jre/lib/amd64/server:/usr/lib/hadoop2.6/lib/native/:/usr/local/cuda-8.0/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n/usr/java/jdk1.8.0_60/jre/lib/amd64/server:/usr/java/jdk1.8.0_60/jre/lib/amd64:/usr/java/jdk1.8.0_60/jre/../lib/amd64:/usr/java/jdk1.8.0_121/jre/lib/amd64/server:/usr/lib/hadoop2.6/lib/native/:/usr/local/cuda-8.0/lib64:/usr/local/cuda/extras/CUPTI/lib64/\n"
      },
      "dateCreated": "Dec 28, 2017 8:59:36 AM",
      "dateSubmitted": "Apr 10, 2018 2:33:17 AM",
      "dateFinished": "Dec 28, 2017 9:22:21 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nvar a \u003d sc.parallelize(1 to 10).map(x \u003d\u003e sys.env(\"JAVA_HOME\")).collect\na.foreach(println)",
      "dateUpdated": "Dec 28, 2017 9:12:50 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [
          {
            "id": 3,
            "jobUrl": "https://qa.qubole.net/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F10.184.236.203%3A8088%2Fproxy%2Fapplication_1514440521907_0015/jobs/job?spark\u003dtrue\u0026id\u003d3",
            "numTasks": 2,
            "numCompletedTasks": 2,
            "stages": [
              {
                "id": 3,
                "completed": true,
                "stageUrl": "https://qa.qubole.net/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F10.184.236.203%3A8088%2Fproxy%2Fapplication_1514440521907_0015/stages/stage/?id\u003d3\u0026attempt\u003d0",
                "numCompleteTasks": 2,
                "numActiveTasks": 0,
                "numFailedTasks": 0,
                "numTotalTasks": 2
              }
            ],
            "status": "Success"
          }
        ],
        "numCompletedTasks": 2,
        "numTasks": 2,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1514452339321_-636901884",
      "id": "20171228-091219_1495025578_q_QUB2UFN5CM1523327479",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\na: Array[String] \u003d Array(/usr/lib/jvm/jre, /usr/lib/jvm/jre, /usr/lib/jvm/jre, /usr/lib/jvm/jre, /usr/lib/jvm/jre, /usr/lib/jvm/jre, /usr/lib/jvm/jre, /usr/lib/jvm/jre, /usr/lib/jvm/jre, /usr/lib/jvm/jre)\n/usr/lib/jvm/jre\n/usr/lib/jvm/jre\n/usr/lib/jvm/jre\n/usr/lib/jvm/jre\n/usr/lib/jvm/jre\n/usr/lib/jvm/jre\n/usr/lib/jvm/jre\n/usr/lib/jvm/jre\n/usr/lib/jvm/jre\n/usr/lib/jvm/jre\n"
      },
      "dateCreated": "Dec 28, 2017 9:12:19 AM",
      "dateStarted": "Dec 28, 2017 9:12:50 AM",
      "dateFinished": "Dec 28, 2017 9:12:50 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1514452370008_2050446590",
      "id": "20171228-091250_1648118415_q_QUB2UFN5CM1523327479",
      "dateCreated": "Dec 28, 2017 9:12:50 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "tfos-123",
  "id": "QUB2UFN5CM1523327479",
  "angularObjects": {
    "2DCSFHNKX283751522976640389:shared_process": [],
    "2DAXJBHH6283751522976646364:shared_process": [],
    "2DD14D1NA283751522976640392:shared_process": [],
    "2DC3356M7283751522976640395:shared_process": []
  },
  "config": {
    "isDashboard": false,
    "defaultLang": "pyspark"
  },
  "info": {},
  "source": "FCN"
}