{
  "paragraphs": [
    {
      "text": "%sh\ncd  /usr/lib/deep-learning/examples/tfos/mnist\nunzip /usr/lib/deep-learning/examples/tfos/mnist/mnist.zip",
      "dateUpdated": "Dec 28, 2017 6:07:44 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1514230583238_-1219183836",
      "id": "20171225-193623_1286573074_q_6BSRX622QQ1530181423",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "Archive:  /usr/lib/deep-learning/examples/tfos/mnist/mnist.zip\n  inflating: t10k-images-idx3-ubyte.gz  \n extracting: t10k-labels-idx1-ubyte.gz  \n  inflating: train-images-idx3-ubyte.gz  \n extracting: train-labels-idx1-ubyte.gz  \n"
      },
      "dateCreated": "Dec 25, 2017 7:36:23 PM",
      "dateSubmitted": "Jun 28, 2018 10:24:26 AM",
      "dateStarted": "Jun 28, 2018 10:24:29 AM",
      "dateFinished": "Jun 28, 2018 10:24:31 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\n/usr/lib/spark/bin/spark-submit \\\n--master yarn \\\n--num-executors 2 \\\n--archives /usr/lib/deep-learning/examples/tfos/mnist/mnist.zip#mnist \\\n--jars /usr/lib/deep-learning/tensorflow-hadoop-plugin/tensorflow-hadoop-1.8.0.jar \\\n/usr/lib/deep-learning/examples/tfos/mnist/mnist_data_setup.py \\\n--output /deep-learning/examples/tfos/mnist_data/tfr \\\n--format tfr",
      "user": "quboleregression+notebooks@qu-test.com",
      "dateUpdated": "Jun 29, 2018 10:10:08 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1514218555123_-791596092",
      "id": "20171225-161555_635354518_q_6BSRX622QQ1530181423",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Warning: Skipping download of JAR file:/usr/lib/deep-learning/tensorflow-hadoop-plugin/tensorflow-hadoop-1.6.0.jar. Scheme file not supported\nDefault value for autoscalingv2 : false\nWarning: Local jar /usr/lib/deep-learning/tensorflow-hadoop-plugin/tensorflow-hadoop-1.6.0.jar does not exist, skipping.\n18/06/28 10:24:31 main INFO Utils: Registered signal handlers for exception exit hook [TERM, HUP, INT]\nargs: Namespace(format\u003d\u0027tfr\u0027, num_partitions\u003d10, output\u003d\u0027/deep-learning/examples/tfos/mnist_data/tfr\u0027, read\u003dFalse, verify\u003dFalse)\n18/06/28 10:26:14 Thread-4 INFO SparkContext: Running Spark version 2.2.1\n18/06/28 10:26:15 Thread-4 INFO SparkContext: Submitted application: mnist_parallelize\n18/06/28 10:26:15 Thread-4 INFO SparkContext: Spark configuration:\nspark.R.cmd\u003d/usr/lib/a-4.2.0-r-3.3.2/bin/R\nspark.app.name\u003dmnist_parallelize\nspark.authenticate\u003dfalse\nspark.authenticate.enableSaslEncryption\u003dfalse\nspark.driver.cores\u003d6\nspark.driver.extraClassPath\u003d/usr/lib/spark/conf\nspark.driver.extraJavaOptions\u003d-Djava.net.preferIPv4Stack\u003dtrue -XX:ReservedCodeCacheSize\u003d100m -XX:+UseCodeCacheFlushing\nspark.driver.extraLibraryPath\u003d/usr/lib/hadoop2/lib/native\nspark.driver.memory\u003d37g\nspark.dynamicAllocation.enabled\u003dfalse\nspark.dynamicAllocation.minExecutors\u003d2\nspark.eventLog.compress\u003dtrue\nspark.eventLog.dir\u003dhdfs://172.30.0.31:9000/spark-history\nspark.eventLog.enabled\u003dtrue\nspark.executor.cores\u003d1\nspark.executor.extraJavaOptions\u003d-Djava.net.preferIPv4Stack\u003dtrue -XX:ReservedCodeCacheSize\u003d100m -XX:+UseCodeCacheFlushing\nspark.executor.instances\u003d2\nspark.executor.memory\u003d37g\nspark.executorEnv.JAVA_HOME\u003d/usr/lib/jvm/java-1.8.0/jre/\nspark.executorEnv.LD_LIBRARY_PATH\u003d/usr/lib/hadoop2/lib/native/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/lib/jvm/java-1.8.0/jre/lib/amd64/server\nspark.hadoop.fs.s3a.connection.establish.timeout\u003d5000\nspark.hadoop.fs.s3a.connection.maximum\u003d200\nspark.hadoop.hive.metastore.dml.events\u003dfalse\nspark.hadoop.hive.qubole.consistent.loadpartition\u003dfalse\nspark.hadoop.mapred.output.committer.class\u003dorg.apache.hadoop.mapred.DirectFileOutputCommitter\nspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\u003d2\nspark.hadoop.mapreduce.output.textoutputformat.overwrite\u003dfalse\nspark.hadoop.mapreduce.use.directfileoutputcommitter\u003dtrue\nspark.hadoop.mapreduce.use.parallelmergepaths\u003dfalse\nspark.hadoop.parquet.enable.summary-metadata\u003dfalse\nspark.hadoop.spark.sql.parquet.output.committer.class\u003dorg.apache.spark.sql.parquet.DirectParquetOutputCommitter\nspark.history.fs.update.interval\u003d10\nspark.history.retainedApplications\u003d5\nspark.logConf\u003dtrue\nspark.master\u003dyarn\nspark.network.sasl.serverAlwaysEncrypt\u003dfalse\nspark.private.ip\u003dfalse\nspark.pyspark.python\u003d/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/bin/python\nspark.qubole.dynamicAllocation.estimateRequiredExecutorsV2\u003dfalse\nspark.qubole.eventLog.hdfs.async\u003dfalse\nspark.qubole.forceRefreshIdleExecutor.enabled\u003dfalse\nspark.qubole.forceRefreshIdleExecutor.interval\u003d600s\nspark.qubole.forceRefreshIdleExecutor.percentage\u003d50\nspark.qubole.internal.default.maxExecutors\u003d2\nspark.qubole.killApplicationOnEventDrop\u003dfalse\nspark.qubole.outputformat.overwriteFileInWrite\u003dfalse\nspark.qubole.sendsql\u003dfalse\nspark.qubole.setIdleShutdownThreadAsDaemon\u003dfalse\nspark.qubole.spotloss.handle\u003dfalse\nspark.qubole.sql.hive.useDirectWrites\u003dfalse\nspark.rdd.compress\u003dTrue\nspark.repl.local.jars\u003dfile:/usr/lib/deep-learning/tensorflow-hadoop-plugin/tensorflow-hadoop-1.6.0.jar\nspark.rolling.log4j.properties\u003d/usr/lib/spark/conf/log4j_rolling.properties\nspark.scheduler.listenerbus.eventqueue.size\u003d20000\nspark.serializer.objectStreamReset\u003d100\nspark.shs.publish.metrics\u003dfalse\nspark.shs.running.on.cluster\u003dtrue\nspark.shs.use.appcache\u003dtrue\nspark.shs.use.cluster.credentials\u003dtrue\nspark.shuffle.reduceLocality.enabled\u003dfalse\nspark.shuffle.service.enabled\u003dfalse\nspark.speculation\u003dfalse\nspark.sql.hive.metastore.jars\u003d/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/etc/hadoop:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/common/lib/*:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/common/*:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/hdfs:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/hdfs/lib/*:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/hdfs/*:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/yarn/lib/*:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/yarn/*:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/mapreduce/*:/share/hadoop/tools:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/tools/lib/*:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/tools/*:/share/hadoop/qubole:/usr/lib/qubole/packages/hadoop2-2.6.0/hadoop2/share/hadoop/qubole/*:/contrib/capacity-scheduler/*.jar:/usr/lib/spark/lib/hive2/*\nspark.sql.hive.metastore.version\u003d0.13.1\nspark.sql.qubole.catalyst.normalizePredicates\u003dfalse\nspark.sql.qubole.createCustomPartitionDirInOverwrite\u003dfalse\nspark.sql.qubole.fireInsertEvents\u003dfalse\nspark.sql.qubole.handleCommentsWithSemicolon\u003dfalse\nspark.sql.qubole.ignoreFNFExceptions\u003dtrue\nspark.sql.qubole.metrics.enable\u003dfalse\nspark.sql.qubole.mv.enable\u003dfalse\nspark.sql.qubole.partitionDiscoverer\u003dfalse\nspark.sql.qubole.recover.partitions\u003dfalse\nspark.sql.qubole.split.computation\u003dfalse\nspark.sql.streaming.showStreamingTab\u003dfalse\nspark.submit.deployMode\u003dclient\nspark.ui.retainedJobs\u003d33\nspark.ui.retainedStages\u003d100\nspark.yarn.dist.archives\u003dfile:/usr/lib/deep-learning/examples/tfos/mnist/mnist.zip#mnist\nspark.yarn.dist.jars\u003dfile:/usr/lib/deep-learning/tensorflow-hadoop-plugin/tensorflow-hadoop-1.6.0.jar\nspark.yarn.driver.memoryOverhead\u003d9g\nspark.yarn.executor.memoryOverhead\u003d9g\nspark.yarn.historyServer.address\u003d172.30.0.31:18080\nspark.yarn.isPython\u003dtrue\nspark.yarn.maxAppAttempts\u003d1\n18/06/28 10:26:15 Thread-4 INFO SecurityManager: Changing view acls to: root\n18/06/28 10:26:15 Thread-4 INFO SecurityManager: Changing modify acls to: root\n18/06/28 10:26:15 Thread-4 INFO SecurityManager: Changing view acls groups to: \n18/06/28 10:26:15 Thread-4 INFO SecurityManager: Changing modify acls groups to: \n18/06/28 10:26:15 Thread-4 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n18/06/28 10:26:15 Thread-4 INFO Utils: Successfully started service \u0027sparkDriver\u0027 on port 42869.\n18/06/28 10:26:15 Thread-4 INFO SparkEnv: Registering MapOutputTracker\n18/06/28 10:26:15 Thread-4 INFO SparkEnv: Registering BlockManagerMaster\n18/06/28 10:26:15 Thread-4 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n18/06/28 10:26:15 Thread-4 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n18/06/28 10:26:15 Thread-4 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1e2ea623-74aa-4170-be2f-1754fab6dc75\n18/06/28 10:26:15 Thread-4 INFO MemoryStore: MemoryStore started with capacity 19.6 GB\n18/06/28 10:26:16 Thread-4 INFO SparkEnv: Registering OutputCommitCoordinator\n18/06/28 10:26:16 Thread-4 INFO log: Logging initialized @106069ms\n18/06/28 10:26:16 Thread-4 INFO Server: jetty-9.3.z-SNAPSHOT\n18/06/28 10:26:16 Thread-4 INFO Server: Started @106149ms\n18/06/28 10:26:16 Thread-4 INFO AbstractConnector: Started ServerConnector@3e7bf5f8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n18/06/28 10:26:16 Thread-4 INFO Utils: Successfully started service \u0027SparkUI\u0027 on port 4040.\n18/06/28 10:26:16 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@212ffd47{/jobs,null,AVAILABLE,@Spark}\n18/06/28 10:26:16 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@67c7a2a{/jobs/json,null,AVAILABLE,@Spark}\n18/06/28 10:26:16 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1617bff6{/jobs/job,null,AVAILABLE,@Spark}\n18/06/28 10:26:16 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2e2dd28a{/jobs/job/json,null,AVAILABLE,@Spark}\n18/06/28 10:26:16 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@49a60a8c{/stages,null,AVAILABLE,@Spark}\n18/06/28 10:26:16 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@64a1ba71{/stages/json,null,AVAILABLE,@Spark}\n18/06/28 10:26:16 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@69b55722{/stages/stage,null,AVAILABLE,@Spark}\n18/06/28 10:26:16 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@45c03ba1{/stages/stage/json,null,AVAILABLE,@Spark}\n18/06/28 10:26:16 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@76e3d1a0{/stages/pool,null,AVAILABLE,@Spark}\n18/06/28 10:26:16 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4f48a7c1{/stages/pool/json,null,AVAILABLE,@Spark}\n18/06/28 10:26:16 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@11f7371b{/storage,null,AVAILABLE,@Spark}\n18/06/28 10:26:16 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@1c5caebe{/storage/json,null,AVAILABLE,@Spark}\n18/06/28 10:26:16 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@4185fc1e{/storage/rdd,null,AVAILABLE,@Spark}\n18/06/28 10:26:16 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@2dc8d48c{/storage/rdd/json,null,AVAILABLE,@Spark}\n18/06/28 10:26:16 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@3854bfdb{/environment,null,AVAILABLE,@Spark}\n18/06/28 10:26:16 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@429573f5{/environment/json,null,AVAILABLE,@Spark}\n18/06/28 10:26:16 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@77355815{/executors,null,AVAILABLE,@Spark}\n18/06/28 10:26:16 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@228f2d19{/executors/json,null,AVAILABLE,@Spark}\n18/06/28 10:26:16 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@19c3212{/executors/threadDump,null,AVAILABLE,@Spark}\n18/06/28 10:26:16 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@764ad60c{/executors/threadDump/json,null,AVAILABLE,@Spark}\n18/06/28 10:26:16 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@6b0c84cc{/static,null,AVAILABLE,@Spark}\n18/06/28 10:26:16 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@5d03fe8b{/,null,AVAILABLE,@Spark}\n18/06/28 10:26:16 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@33dfeaa{/api,null,AVAILABLE,@Spark}\n18/06/28 10:26:16 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@27cb4a03{/jobs/job/kill,null,AVAILABLE,@Spark}\n18/06/28 10:26:16 Thread-4 INFO ContextHandler: Started o.s.j.s.ServletContextHandler@68b37a86{/stages/stage/kill,null,AVAILABLE,@Spark}\n18/06/28 10:26:16 Thread-4 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.30.0.31:4040\n18/06/28 10:26:16 Thread-4 INFO QuboleUtils: Overload detection enabled: false \n18/06/28 10:26:16 Thread-4 INFO deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS\n18/06/28 10:26:19 Thread-4 INFO TimelineClientImpl: Timeline service address: http://172.30.0.31:8188/ws/v1/timeline/\n18/06/28 10:26:19 Thread-4 INFO RMProxy: Connecting to ResourceManager at 172.30.0.31/172.30.0.31:8032\n18/06/28 10:26:20 Thread-4 INFO Client: Requesting a new application from cluster with 7 NodeManagers\n18/06/28 10:26:20 Thread-4 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (49971 MB per container)\n18/06/28 10:26:20 Thread-4 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n18/06/28 10:26:20 Thread-4 INFO Client: Setting up container launch context for our AM\n18/06/28 10:26:20 Thread-4 INFO Client: Setting up the launch environment for our AM container\n18/06/28 10:26:20 Thread-4 INFO Client: Preparing resources for our AM container\n18/06/28 10:26:22 Thread-4 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n18/06/28 10:26:44 Thread-4 INFO deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS\n18/06/28 10:26:44 Thread-4 INFO Client: Uploading resource file:/tmp/spark-1bb87355-98da-4ca3-ac26-f46f759589f1/__spark_libs__6408900223329563524.zip -\u003e hdfs://172.30.0.31:9000/user/root/.sparkStaging/application_1530175854947_0101/__spark_libs__6408900223329563524.zip\n18/06/28 10:26:45 Thread-4 INFO deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS\n18/06/28 10:26:45 Thread-4 INFO Client: Uploading resource file:/usr/lib/deep-learning/tensorflow-hadoop-plugin/tensorflow-hadoop-1.6.0.jar -\u003e hdfs://172.30.0.31:9000/user/root/.sparkStaging/application_1530175854947_0101/tensorflow-hadoop-1.6.0.jar\n18/06/28 10:26:45 Thread-4 INFO Client: Deleted staging directory hdfs://172.30.0.31:9000/user/root/.sparkStaging/application_1530175854947_0101\n18/06/28 10:26:45 Thread-4 ERROR SparkContext: Error initializing SparkContext.\njava.io.FileNotFoundException: File file:/usr/lib/deep-learning/tensorflow-hadoop-plugin/tensorflow-hadoop-1.6.0.jar does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:539)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:765)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:529)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)\n\tat org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:371)\n\tat org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:490)\n\tat org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$11$$anonfun$apply$6.apply(Client.scala:612)\n\tat org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$11$$anonfun$apply$6.apply(Client.scala:611)\n\tat scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:74)\n\tat org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$11.apply(Client.scala:611)\n\tat org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$11.apply(Client.scala:610)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:610)\n\tat org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:841)\n\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:170)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:85)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:173)\n\tat org.apache.spark.SparkContext.\u003cinit\u003e(SparkContext.scala:619)\n\tat org.apache.spark.api.java.JavaSparkContext.\u003cinit\u003e(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:236)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n18/06/28 10:26:45 Thread-4 INFO AbstractConnector: Stopped Spark@3e7bf5f8{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n18/06/28 10:26:45 Thread-4 INFO SparkUI: Stopped Spark web UI at http://172.30.0.31:4040\n18/06/28 10:26:45 dispatcher-event-loop-1 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n18/06/28 10:26:45 Thread-4 ERROR Utils: Neglecting uncaught exception in thread Thread-4\njava.util.NoSuchElementException: None.get\n\tat scala.None$.get(Option.scala:347)\n\tat scala.None$.get(Option.scala:345)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.stop(YarnClientSchedulerBackend.scala:184)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:527)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1701)\n\tat org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:2112)\n\tat org.apache.spark.util.Utils$.tryQuboleLogError(Utils.scala:1371)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2111)\n\tat org.apache.spark.SparkContext.\u003cinit\u003e(SparkContext.scala:708)\n\tat org.apache.spark.api.java.JavaSparkContext.\u003cinit\u003e(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:236)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n18/06/28 10:26:45 dispatcher-event-loop-0 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n18/06/28 10:26:45 dispatcher-event-loop-2 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: Sending StopAM(true) to AppMaster\n18/06/28 10:26:45 Thread-4 INFO MemoryStore: MemoryStore cleared\n18/06/28 10:26:45 Thread-4 INFO BlockManager: BlockManager stopped\n18/06/28 10:26:45 Thread-4 INFO BlockManagerMaster: BlockManagerMaster stopped\n18/06/28 10:26:45 Thread-4 WARN MetricsSystem: Stopping a MetricsSystem that is not running\n18/06/28 10:26:45 dispatcher-event-loop-2 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n18/06/28 10:26:45 Thread-4 INFO SparkContext: Successfully stopped SparkContext\nTraceback (most recent call last):\n  File \"/usr/lib/deep-learning/examples/tfos/mnist/mnist_data_setup.py\", line 149, in \u003cmodule\u003e\n    sc \u003d SparkContext(conf\u003dSparkConf().setAppName(\"mnist_parallelize\"))\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py\", line 118, in __init__\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py\", line 180, in _do_init\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py\", line 273, in _initialize_context\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1401, in __call__\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.io.FileNotFoundException: File file:/usr/lib/deep-learning/tensorflow-hadoop-plugin/tensorflow-hadoop-1.6.0.jar does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:539)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:765)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:529)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337)\n\tat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)\n\tat org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:371)\n\tat org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:490)\n\tat org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$11$$anonfun$apply$6.apply(Client.scala:612)\n\tat org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$11$$anonfun$apply$6.apply(Client.scala:611)\n\tat scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:74)\n\tat org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$11.apply(Client.scala:611)\n\tat org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$11.apply(Client.scala:610)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:610)\n\tat org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:841)\n\tat org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:170)\n\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:85)\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:173)\n\tat org.apache.spark.SparkContext.\u003cinit\u003e(SparkContext.scala:619)\n\tat org.apache.spark.api.java.JavaSparkContext.\u003cinit\u003e(JavaSparkContext.scala:58)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:236)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n\n18/06/28 10:26:50 main INFO SparkContext: sc.stop called from [SparkSubmit.exceptionExitHook[failure]]\n18/06/28 10:26:50 main INFO SparkContext: SparkContext already stopped.\n18/06/28 10:26:50 main INFO YarnClientSchedulerBackend: yarn-client securityManager checkExit, exit code: 1\n18/06/28 10:26:50 main INFO SparkContext: sc.stop called from [yarn-client App exit with exit code: 1]\n18/06/28 10:26:50 main INFO SparkContext: SparkContext already stopped.\n18/06/28 10:26:50 Thread-1 INFO ShutdownHookManager: Shutdown hook called\n18/06/28 10:26:50 Thread-1 INFO ShutdownHookManager: Deleting directory /tmp/spark-cfafe16c-c63b-49ff-a0a8-0be0cb6a7b33\n18/06/28 10:26:50 Thread-1 INFO ShutdownHookManager: Deleting directory /tmp/spark-1bb87355-98da-4ca3-ac26-f46f759589f1\nExitValue: 1"
      },
      "dateCreated": "Dec 25, 2017 4:15:55 PM",
      "dateSubmitted": "Jun 28, 2018 10:24:29 AM",
      "dateStarted": "Jun 28, 2018 10:24:29 AM",
      "dateFinished": "Jun 28, 2018 10:26:51 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\nexport LD_LIBRARY_PATH\u003d/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.151-1.b12.35.amzn1.x86_64/jre/lib/amd64/server:$LD_LIBRARY_PATH",
      "dateUpdated": "Dec 28, 2017 6:10:30 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1514441361561_-1137784858",
      "id": "20171228-060921_201869850_q_6BSRX622QQ1530181423",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Dec 28, 2017 6:09:21 AM",
      "dateSubmitted": "Jun 28, 2018 10:24:29 AM",
      "dateStarted": "Jun 28, 2018 10:24:29 AM",
      "dateFinished": "Jun 28, 2018 10:24:29 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\necho $LD_LIBRARY_PATH",
      "dateUpdated": "Dec 28, 2017 6:10:49 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1514441435373_-594211283",
      "id": "20171228-061035_1905143372_q_6BSRX622QQ1530181423",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\n"
      },
      "dateCreated": "Dec 28, 2017 6:10:35 AM",
      "dateSubmitted": "Jun 28, 2018 10:24:29 AM",
      "dateStarted": "Jun 28, 2018 10:24:30 AM",
      "dateFinished": "Jun 28, 2018 10:24:30 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom pyspark.context import SparkContext\nfrom pyspark.conf import SparkConf\n\nimport sys\nprint(sys.executable)\n\nimport argparse\nimport os\nimport numpy\nimport sys\nimport tensorflow as tf\nimport threading\nfrom datetime import datetime\n\nfrom tensorflowonspark import TFCluster",
      "dateUpdated": "Dec 28, 2017 9:23:13 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1514228606679_475062617",
      "id": "20171225-190326_1095226201_q_6BSRX622QQ1530181423",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/bin/python\n"
      },
      "dateCreated": "Dec 25, 2017 7:03:26 PM",
      "dateSubmitted": "Jun 28, 2018 10:24:26 AM",
      "dateStarted": "Jun 28, 2018 10:24:41 AM",
      "dateFinished": "Jun 28, 2018 10:25:38 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1514446450135_-149404172",
      "id": "20171228-073410_1049259817_q_6BSRX622QQ1530181423",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Dec 28, 2017 7:34:10 AM",
      "dateSubmitted": "Jun 28, 2018 10:24:41 AM",
      "dateStarted": "Jun 28, 2018 10:25:38 AM",
      "dateFinished": "Jun 28, 2018 10:25:38 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%sh\necho $LD_LIBRARY_PATH",
      "dateUpdated": "Dec 28, 2017 7:34:09 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/sh"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1514446440509_-1037804707",
      "id": "20171228-073400_1850734434_q_6BSRX622QQ1530181423",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\n"
      },
      "dateCreated": "Dec 28, 2017 7:34:00 AM",
      "dateSubmitted": "Jun 28, 2018 10:24:30 AM",
      "dateStarted": "Jun 28, 2018 10:24:30 AM",
      "dateFinished": "Jun 28, 2018 10:24:30 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "def print_log(worker_num, arg):\n    print(\"{}: \".format(worker_num))\n    print(arg)\n\n\ndef map_fun(args, ctx):\n    from tensorflowonspark import TFNode\n    from datetime import datetime\n    import getpass\n    import math\n    import numpy\n    import os\n    import signal\n    import tensorflow as tf\n    import time\n\n    IMAGE_PIXELS \u003d 28\n    worker_num \u003d ctx.worker_num\n    job_name \u003d ctx.job_name\n    task_index \u003d ctx.task_index\n    cluster_spec \u003d ctx.cluster_spec\n    num_workers \u003d len(cluster_spec[\u0027worker\u0027])\n\n    # Delay PS nodes a bit, since workers seem to reserve GPUs more quickly/reliably (w/o conflict)\n    if job_name \u003d\u003d \"ps\":\n        time.sleep((worker_num + 1) * 5)\n\n    # Parameters\n    hidden_units \u003d 128\n    batch_size \u003d 100\n\n    # Get TF cluster and server instances\n    cluster, server \u003d TFNode.start_cluster_server(ctx, 1, args[\"rdma\"])\n\n    def read_csv_examples(image_dir, label_dir, batch_size\u003d100, num_epochs\u003dNone, task_index\u003dNone, num_workers\u003dNone):\n        print_log(worker_num, \"num_epochs: {0}\".format(num_epochs))\n        # Setup queue of csv image filenames\n        tf_record_pattern \u003d os.path.join(image_dir, \u0027part-*\u0027)\n        images \u003d tf.gfile.Glob(tf_record_pattern)\n        print_log(worker_num, \"images: {0}\".format(images))\n        image_queue \u003d tf.train.string_input_producer(\n            images, shuffle\u003dFalse, capacity\u003d1000, num_epochs\u003dnum_epochs, name\u003d\"image_queue\")\n\n        # Setup queue of csv label filenames\n        tf_record_pattern \u003d os.path.join(label_dir, \u0027part-*\u0027)\n        labels \u003d tf.gfile.Glob(tf_record_pattern)\n        print_log(worker_num, \"labels: {0}\".format(labels))\n        label_queue \u003d tf.train.string_input_producer(\n            labels, shuffle\u003dFalse, capacity\u003d1000, num_epochs\u003dnum_epochs, name\u003d\"label_queue\")\n\n        # Setup reader for image queue\n        img_reader \u003d tf.TextLineReader(name\u003d\"img_reader\")\n        _, img_csv \u003d img_reader.read(image_queue)\n        image_defaults \u003d [[1.0] for col in range(784)]\n        img \u003d tf.pack(tf.decode_csv(img_csv, image_defaults))\n        # Normalize values to [0,1]\n        norm \u003d tf.constant(255, dtype\u003dtf.float32, shape\u003d(784,))\n        image \u003d tf.div(img, norm)\n        print_log(worker_num, \"image: {0}\".format(image))\n\n        # Setup reader for label queue\n        label_reader \u003d tf.TextLineReader(name\u003d\"label_reader\")\n        _, label_csv \u003d label_reader.read(label_queue)\n        label_defaults \u003d [[1.0] for col in range(10)]\n        label \u003d tf.pack(tf.decode_csv(label_csv, label_defaults))\n        print_log(worker_num, \"label: {0}\".format(label))\n\n        # Return a batch of examples\n        return tf.train.batch([image, label], batch_size, num_threads\u003dargs[\"readers\"], name\u003d\"batch_csv\")\n\n    def read_tfr_examples(path, batch_size\u003d100, num_epochs\u003dNone, task_index\u003dNone, num_workers\u003dNone):\n        print_log(worker_num, \"num_epochs: {0}\".format(num_epochs))\n\n        # Setup queue of TFRecord filenames\n        tf_record_pattern \u003d os.path.join(path, \u0027part-*\u0027)\n        files \u003d tf.gfile.Glob(tf_record_pattern)\n        queue_name \u003d \"file_queue\"\n\n        # split input files across workers, if specified\n        if task_index is not None and num_workers is not None:\n            num_files \u003d len(files)\n            files \u003d files[task_index:num_files:num_workers]\n            queue_name \u003d \"file_queue_{0}\".format(task_index)\n\n        print_log(worker_num, \"files: {0}\".format(files))\n        file_queue \u003d tf.train.string_input_producer(\n            files, shuffle\u003dFalse, capacity\u003d1000, num_epochs\u003dnum_epochs, name\u003dqueue_name)\n\n        # Setup reader for examples\n        reader \u003d tf.TFRecordReader(name\u003d\"reader\")\n        _, serialized \u003d reader.read(file_queue)\n        feature_def \u003d {\u0027label\u0027: tf.FixedLenFeature(\n            [10], tf.int64), \u0027image\u0027: tf.FixedLenFeature([784], tf.int64)}\n        features \u003d tf.parse_single_example(serialized, feature_def)\n        norm \u003d tf.constant(255, dtype\u003dtf.float32, shape\u003d(784,))\n        image \u003d tf.div(tf.to_float(features[\u0027image\u0027]), norm)\n        print_log(worker_num, \"image: {0}\".format(image))\n        label \u003d tf.to_float(features[\u0027label\u0027])\n        print_log(worker_num, \"label: {0}\".format(label))\n\n        # Return a batch of examples\n        return tf.train.batch([image, label], batch_size, num_threads\u003dargs[\"readers\"], name\u003d\"batch\")\n\n    if job_name \u003d\u003d \"ps\":\n        server.join()\n    elif job_name \u003d\u003d \"worker\":\n        # Assigns ops to the local worker by default.\n        with tf.device(tf.train.replica_device_setter(\n                worker_device\u003d\"/job:worker/task:%d\" % task_index,\n                cluster\u003dcluster)):\n\n            # Variables of the hidden layer\n            hid_w \u003d tf.Variable(tf.truncated_normal([IMAGE_PIXELS * IMAGE_PIXELS, hidden_units],\n                                                    stddev\u003d1.0 / IMAGE_PIXELS), name\u003d\"hid_w\")\n            hid_b \u003d tf.Variable(tf.zeros([hidden_units]), name\u003d\"hid_b\")\n            tf.summary.histogram(\"hidden_weights\", hid_w)\n\n            # Variables of the softmax layer\n            sm_w \u003d tf.Variable(tf.truncated_normal([hidden_units, 10],\n                                                   stddev\u003d1.0 / math.sqrt(hidden_units)), name\u003d\"sm_w\")\n            sm_b \u003d tf.Variable(tf.zeros([10]), name\u003d\"sm_b\")\n            tf.summary.histogram(\"softmax_weights\", sm_w)\n\n            # Placeholders or QueueRunner/Readers for input data\n            num_epochs \u003d 1 if args[\"mode\"] \u003d\u003d \"inference\" else None if args[\"epochs\"] \u003d\u003d 0 else args[\"epochs\"]\n            index \u003d task_index if args[\"mode\"] \u003d\u003d \"inference\" else None\n            workers \u003d num_workers if args[\"mode\"] \u003d\u003d \"inference\" else None\n\n            if args[\"format\"] \u003d\u003d \"csv\":\n                images \u003d TFNode.hdfs_path(args[\"images\"], ctx.defaultFS, ctx.working_dir)\n                labels \u003d TFNode.hdfs_path(args[\"labels\"], ctx.defaultFS, ctx.working_dir)\n                x, y_ \u003d read_csv_examples(\n                    images, labels, 100, num_epochs, index, workers)\n            elif args[\"format\"] \u003d\u003d \"tfr\":\n                images \u003d TFNode.hdfs_path(args[\"images\"], ctx.defaultFS, ctx.working_dir)\n                x, y_ \u003d read_tfr_examples(\n                    images, 100, num_epochs, index, workers)\n            else:\n                raise(\"{0} format not supported for tf input mode\".format(\n                    args[\"format\"]))\n\n            x_img \u003d tf.reshape(x, [-1, IMAGE_PIXELS, IMAGE_PIXELS, 1])\n            tf.summary.image(\"x_img\", x_img)\n\n            hid_lin \u003d tf.nn.xw_plus_b(x, hid_w, hid_b)\n            hid \u003d tf.nn.relu(hid_lin)\n\n            y \u003d tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\n\n            global_step \u003d tf.Variable(0)\n\n            loss \u003d -tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n            tf.summary.scalar(\"loss\", loss)\n            train_op \u003d tf.train.AdagradOptimizer(0.01).minimize(\n                loss, global_step\u003dglobal_step)\n\n            # Test trained model\n            label \u003d tf.argmax(y_, 1, name\u003d\"label\")\n            prediction \u003d tf.argmax(y, 1, name\u003d\"prediction\")\n            correct_prediction \u003d tf.equal(prediction, label)\n            accuracy \u003d tf.reduce_mean(\n                tf.cast(correct_prediction, tf.float32), name\u003d\"accuracy\")\n            tf.summary.scalar(\"acc\", accuracy)\n\n            saver \u003d tf.train.Saver()\n            summary_op \u003d tf.summary.merge_all()\n            init_op \u003d tf.global_variables_initializer()\n\n        # Create a \"supervisor\", which oversees the training process and stores model state into HDFS\n        logdir \u003d TFNode.hdfs_path(args[\"model\"], ctx.defaultFS, ctx.working_dir)\n        print(\"tensorflow model path: {0}\".format(logdir))\n\n        summary_writer \u003d TFNode.get_summary_writer(ctx)\n        \n        if args[\"mode\"] \u003d\u003d \"train\":\n            sv \u003d tf.train.Supervisor(is_chief\u003d(task_index \u003d\u003d 0),\n                                     logdir\u003dlogdir,\n                                     init_op\u003dinit_op,\n                                     summary_op\u003dNone,\n                                     saver\u003dsaver,\n                                     global_step\u003dglobal_step,\n                                     stop_grace_secs\u003d300,\n                                     save_model_secs\u003d10)\n        else:\n            sv \u003d tf.train.Supervisor(is_chief\u003d(task_index \u003d\u003d 0),\n                                     logdir\u003dlogdir,\n                                     summary_op\u003dNone,\n                                     saver\u003dsaver,\n                                     global_step\u003dglobal_step,\n                                     stop_grace_secs\u003d300,\n                                     save_model_secs\u003d0)\n            output_dir \u003d TFNode.hdfs_path(args[\"output\"], ctx.defaultFS, ctx.working_dir)\n            output_file \u003d tf.gfile.Open(\n                \"{0}/part-{1:05d}\".format(output_dir, worker_num), mode\u003d\u0027w\u0027)\n\n        # The supervisor takes care of session initialization, restoring from\n        # a checkpoint, and closing when done or an error occurs.\n        with sv.managed_session(server.target) as sess:\n            print(\"{0} session ready\".format(datetime.now().isoformat()))\n\n            # Loop until the supervisor shuts down or 1000000 steps have completed.\n            step \u003d 0\n            count \u003d 0\n            while not sv.should_stop() and step \u003c args[\"steps\"]:\n                # Run a training step asynchronously.\n                # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n                # perform *synchronous* training.\n\n                # using QueueRunners/Readers\n                if args[\"mode\"] \u003d\u003d \"train\":\n                    if (step % 100 \u003d\u003d 0):\n                        print(\"{0} step: {1} accuracy: {2}\".format(\n                            datetime.now().isoformat(), step, sess.run(accuracy)))\n                    _, summary, step \u003d sess.run(\n                        [train_op, summary_op, global_step])\n                    if sv.is_chief:\n                        summary_writer.add_summary(summary, step)\n                else:  # args[\"mode\"] \u003d\u003d \"inference\"\n                    labels, pred, acc \u003d sess.run([label, prediction, accuracy])\n                    #print(\"label: {0}, pred: {1}\".format(labels, pred))\n                    print(\"acc: {0}\".format(acc))\n                    for i in range(len(labels)):\n                        count +\u003d 1\n                        output_file.write(\n                            \"{0} {1}\\n\".format(labels[i], pred[i]))\n                    print(\"count: {0}\".format(count))\n\n        if args[\"mode\"] \u003d\u003d \"inference\":\n            output_file.close()\n            # Delay chief worker from shutting down supervisor during inference, since it can load model, start session,\n            # run inference and request stop before the other workers even start/sync their sessions.\n            if task_index \u003d\u003d 0:\n                time.sleep(60)\n\n        # Ask for all the services to stop.\n        print(\"{0} stopping supervisor\".format(datetime.now().isoformat()))\n        sv.stop()",
      "dateUpdated": "Dec 28, 2017 9:23:22 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1514231345671_-693348105",
      "id": "20171225-194905_967595234_q_6BSRX622QQ1530181423",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Dec 25, 2017 7:49:05 PM",
      "dateSubmitted": "Jun 28, 2018 10:25:38 AM",
      "dateStarted": "Jun 28, 2018 10:25:38 AM",
      "dateFinished": "Jun 28, 2018 10:25:38 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "num_executors \u003d 2\nnum_ps \u003d 1\n\nargs \u003d {\"epochs\": 1,\n        \"format\": \"tfr\",\n        \"images\":\"/deep-learning/examples/tfos/mnist_data/tfr/train\",\n        \"labels\": None,\n        \"model\": \"mnist_model\",\n        \"cluster_size\": num_executors,\n        \"output\": \"predictions\",\n        \"readers\": 1,\n        \"steps\": 1000,\n        \"mode\": \"train\",\n        \"rdma\": False\n        }\n\nprint(\"{0} \u003d\u003d\u003d\u003d\u003d Start\".format(datetime.now().isoformat()))\ncluster \u003d TFCluster.run(sc, map_fun, args, args[\"cluster_size\"],\n                        num_ps, TFCluster.InputMode.TENSORFLOW)\ncluster.shutdown()\n\nprint(\"{0} \u003d\u003d\u003d\u003d\u003d Stop\".format(datetime.now().isoformat()))",
      "dateUpdated": "Dec 28, 2017 9:23:28 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1514231275467_-910220603",
      "id": "20171225-194755_2015858841_q_6BSRX622QQ1530181423",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "2018-06-28T10:25:38.522737 \u003d\u003d\u003d\u003d\u003d Start\n2018-06-28 10:25:38,523 INFO (MainThread-9116) Reserving TFSparkNodes.\n\n2018-06-28 10:25:38,525 INFO (MainThread-9116) listening for reservations at (\u0027ip-172-30-0-13\u0027, 35061)\n\n2018-06-28 10:25:38,526 INFO (MainThread-9116) Starting TensorFlow on executors\n\n2018-06-28 10:25:38,605 INFO (MainThread-9116) Waiting for TFSparkNodes to start\n\n2018-06-28 10:25:38,607 INFO (MainThread-9116) waiting for 2 reservations\n\n2018-06-28 10:25:39,610 INFO (MainThread-9116) waiting for 2 reservations\n\n2018-06-28 10:25:40,612 INFO (MainThread-9116) waiting for 2 reservations\n\n2018-06-28 10:25:41,614 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:25:42,616 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:25:43,618 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:25:44,621 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:25:45,623 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:25:46,624 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:25:47,627 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:25:48,629 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:25:49,631 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:25:50,634 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:25:51,636 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:25:52,638 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:25:53,639 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:25:54,642 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:25:55,644 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:25:56,646 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:25:57,648 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:25:58,650 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:25:59,652 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:00,654 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:01,656 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:02,659 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:03,660 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:04,662 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:05,664 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:06,666 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:07,668 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:08,670 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:09,672 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:10,674 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:11,676 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:12,678 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:13,680 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:14,682 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:15,684 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:16,686 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:17,688 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:18,690 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:19,692 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:20,694 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:21,696 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:22,698 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:23,699 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:24,701 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:25,703 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:26,705 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:27,706 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:28,708 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:29,710 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:30,711 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:31,714 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:32,716 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:33,718 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:34,720 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:35,722 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:36,724 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:37,726 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:38,728 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:39,730 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:40,732 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:41,733 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:42,735 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:43,737 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:44,739 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:45,741 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:46,743 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:47,745 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:48,747 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:49,749 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:50,751 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:51,753 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:52,755 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:53,757 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:54,759 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:55,761 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:56,763 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:57,765 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:58,766 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:26:59,768 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:00,770 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:01,772 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:02,774 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:03,776 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:04,778 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:05,780 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:06,782 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:07,784 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:08,786 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:09,788 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:10,790 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:11,792 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:12,794 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:13,796 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:14,798 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:15,799 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:16,801 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:17,803 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:18,805 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:19,807 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:20,809 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:21,811 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:22,813 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:23,815 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:24,817 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:25,819 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:26,821 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:27,823 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:28,824 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:29,825 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:30,827 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:31,829 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:32,831 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:33,833 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:34,835 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:35,837 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:36,839 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:37,841 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:38,843 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:39,845 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:40,847 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:41,849 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:42,851 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:43,853 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:44,855 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:45,857 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:46,859 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:47,861 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:48,863 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:49,865 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:50,867 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:51,869 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:52,870 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:53,872 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:54,874 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:55,876 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:56,878 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:57,880 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:58,882 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:27:59,884 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:28:00,886 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:28:01,888 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:28:02,889 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:28:03,891 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:28:04,893 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:28:05,895 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:28:06,897 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:28:07,898 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:28:08,900 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:28:09,902 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:28:10,904 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:28:11,906 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:28:12,908 INFO (MainThread-9116) waiting for 1 reservations\n\n2018-06-28 10:28:13,910 INFO (MainThread-9116) all reservations completed\n\n2018-06-28 10:28:13,911 INFO (MainThread-9116) All TFSparkNodes started\n\n2018-06-28 10:28:13,911 INFO (MainThread-9116) {\u0027host\u0027: \u0027ip-172-30-0-192\u0027, \u0027addr\u0027: (\u0027ip-172-30-0-192\u0027, 34793), \u0027worker_num\u0027: 0, \u0027task_index\u0027: 0, \u0027job_name\u0027: \u0027ps\u0027, \u0027authkey\u0027: b\u0027\\x9e\\x7f\\x11\\x8e\\xf4\\xb6H\\x18\\xa0b\\xf2*RR\\xfe\\xd7\u0027, \u0027port\u0027: 46071, \u0027ppid\u0027: 13868}\n\n2018-06-28 10:28:13,912 INFO (MainThread-9116) {\u0027host\u0027: \u0027ip-172-30-0-247\u0027, \u0027addr\u0027: \u0027/tmp/pymp-912xwcyb/listener-edobs07a\u0027, \u0027worker_num\u0027: 1, \u0027task_index\u0027: 0, \u0027job_name\u0027: \u0027worker\u0027, \u0027authkey\u0027: b\u0027\\xfa\\xb7\\x00\\xb9\\r\\x94M$\\xa78DD\\xeaL\\xae\\x00\u0027, \u0027port\u0027: 44427, \u0027ppid\u0027: 915}\n\n2018-06-28 10:28:13,912 INFO (MainThread-9116) Stopping TensorFlow nodes\n\nException in thread Thread-3:\nTraceback (most recent call last):\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n    self.run()\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/threading.py\", line 862, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/tensorflowonspark-1.0.0-py3.5.egg/tensorflowonspark/TFCluster.py\", line 261, in _start\n    background\u003d(input_mode \u003d\u003d InputMode.SPARK)))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 799, in foreachPartition\n    self.mapPartitions(func).count()  # Force evaluation\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1041, in count\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1032, in sum\n    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 906, in fold\n    vals \u003d self.mapPartitions(func).collect()\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 809, in collect\n    port \u003d self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\", line 319, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 7, 172.30.0.247, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/lib/spark/python/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 794, in func\n    r \u003d f(it)\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/tensorflowonspark-1.0.0-py3.5.egg/tensorflowonspark/TFSparkNode.py\", line 402, in _mapfn\n    fn(tf_args, ctx)\n  File \"\u003cstdin\u003e\", line 98, in map_fun\n  File \"\u003cstdin\u003e\", line 53, in read_tfr_examples\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\", line 337, in get_matching_files\n    for single_filename in filename\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\n    c_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.NotFoundError: hdfs://172.30.0.31:9000/deep-learning/examples/tfos/mnist_data/tfr/train; No such file or directory\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.\u003cinit\u003e(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:339)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1548)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1536)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1535)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1535)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:845)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:845)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:845)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1763)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1718)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1707)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:640)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2266)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2287)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2306)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2331)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:467)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/lib/spark/python/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 794, in func\n    r \u003d f(it)\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/tensorflowonspark-1.0.0-py3.5.egg/tensorflowonspark/TFSparkNode.py\", line 402, in _mapfn\n    fn(tf_args, ctx)\n  File \"\u003cstdin\u003e\", line 98, in map_fun\n  File \"\u003cstdin\u003e\", line 53, in read_tfr_examples\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/tensorflow/python/lib/io/file_io.py\", line 337, in get_matching_files\n    for single_filename in filename\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\", line 516, in __exit__\n    c_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.NotFoundError: hdfs://172.30.0.31:9000/deep-learning/examples/tfos/mnist_data/tfr/train; No such file or directory\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.\u003cinit\u003e(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:339)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\n\n2018-06-28 10:28:32,092 INFO (MainThread-9116) Shutting down cluster\n\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2550257928077404448.py\", line 295, in \u003cmodule\u003e\n    exec(code)\n  File \"\u003cstdin\u003e\", line 18, in \u003cmodule\u003e\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/tensorflowonspark-1.0.0-py3.5.egg/tensorflowonspark/TFCluster.py\", line 163, in shutdown\n    q.join()\n  File \"\u003cstring\u003e\", line 2, in join\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/multiprocessing/managers.py\", line 717, in _callmethod\n    kind, result \u003d conn.recv()\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n    buf \u003d self._recv_bytes()\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf \u003d self._recv(4)\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n    chunk \u003d read(handle, remaining)\n  File \"/usr/lib/spark/python/pyspark/context.py\", line 237, in signal_handler\n    raise KeyboardInterrupt()\nKeyboardInterrupt\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2550257928077404448.py\", line 302, in \u003cmodule\u003e\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-2550257928077404448.py\", line 295, in \u003cmodule\u003e\n    exec(code)\n  File \"\u003cstdin\u003e\", line 18, in \u003cmodule\u003e\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/site-packages/tensorflowonspark-1.0.0-py3.5.egg/tensorflowonspark/TFCluster.py\", line 163, in shutdown\n    q.join()\n  File \"\u003cstring\u003e\", line 2, in join\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/multiprocessing/managers.py\", line 717, in _callmethod\n    kind, result \u003d conn.recv()\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/multiprocessing/connection.py\", line 250, in recv\n    buf \u003d self._recv_bytes()\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf \u003d self._recv(4)\n  File \"/usr/lib/a-4.2.0-py-3.5.3-dl-gpu-full/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n    chunk \u003d read(handle, remaining)\n  File \"/usr/lib/spark/python/pyspark/context.py\", line 237, in signal_handler\n    raise KeyboardInterrupt()\nKeyboardInterrupt\n\n"
      },
      "dateCreated": "Dec 25, 2017 7:47:55 PM",
      "dateSubmitted": "Jun 28, 2018 10:25:38 AM",
      "dateStarted": "Jun 28, 2018 10:25:38 AM",
      "dateFinished": "Jun 28, 2018 11:14:45 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%knitr",
      "dateUpdated": "Dec 28, 2017 7:56:19 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/r"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1514231533734_1028127387",
      "id": "20171225-195213_1718542748_q_6BSRX622QQ1530181423",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cscript type\u003d\"text/javascript\"\u003e\nwindow.onload \u003d function() {\n  var imgs \u003d document.getElementsByTagName(\u0027img\u0027), i, img;\n  for (i \u003d 0; i \u003c imgs.length; i++) {\n    img \u003d imgs[i];\n    // center an image if it is the only element of its parent\n    if (img.parentElement.childElementCount \u003d\u003d\u003d 1)\n      img.parentElement.style.textAlign \u003d \u0027center\u0027;\n  }\n};\n\u003c/script\u003e"
      },
      "dateCreated": "Dec 25, 2017 7:52:13 PM",
      "dateSubmitted": "Jun 28, 2018 10:25:39 AM",
      "dateStarted": "Jun 28, 2018 10:25:39 AM",
      "dateFinished": "Jun 28, 2018 10:25:47 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nimport sys.process._\nsys.env(\"LD_LIBRARY_PATH\")",
      "dateUpdated": "Dec 28, 2017 8:53:37 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1514446704507_667000701",
      "id": "20171228-073824_1818877258_q_6BSRX622QQ1530181423",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nimport sys.process._\n\nres0: String \u003d /media/ebs2/yarn/local/usercache/root/appcache/application_1530175854947_0099/container_1530175854947_0099_01_000004:/usr/lib/hadoop2/lib/native:/usr/lib/hadoop2/lib/native\n"
      },
      "dateCreated": "Dec 28, 2017 7:38:24 AM",
      "dateSubmitted": "Jun 28, 2018 10:25:39 AM",
      "dateStarted": "Jun 28, 2018 10:25:39 AM",
      "dateFinished": "Jun 28, 2018 10:25:40 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nprint(sc.parallelize(1 to 10).map(x \u003d\u003e sys.env(\"LD_LIBRARY_PATH\")).collect)",
      "dateUpdated": "Dec 28, 2017 9:09:30 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "JOB UI",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "https://qa3.qubole.net/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F172.30.0.31%3A8088%2Fproxy%2Fapplication_1530175854947_0100/jobs/job?spark\u003dtrue\u0026id\u003d1"
          ],
          "interpreterSettingId": "2DHYD2X9H326581530181442072"
        }
      },
      "version": "v0",
      "jobName": "paragraph_1514451012304_-1648109528",
      "id": "20171228-085012_14013020_q_6BSRX622QQ1530181423",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "[Ljava.lang.String;@27cc8a4a"
      },
      "dateCreated": "Dec 28, 2017 8:50:12 AM",
      "dateSubmitted": "Jun 28, 2018 10:25:40 AM",
      "dateStarted": "Jun 28, 2018 10:25:40 AM",
      "dateFinished": "Jun 28, 2018 10:28:21 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nvar a \u003d sc.parallelize(1 to 10).map(x \u003d\u003e sys.env(\"LD_LIBRARY_PATH\")).collect\na.foreach(println)",
      "dateUpdated": "Dec 28, 2017 9:22:03 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "JOB UI",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "https://qa3.qubole.net/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F172.30.0.31%3A8088%2Fproxy%2Fapplication_1530175854947_0100/jobs/job?spark\u003dtrue\u0026id\u003d2"
          ],
          "interpreterSettingId": "2DHYD2X9H326581530181442072"
        }
      },
      "version": "v0",
      "jobName": "paragraph_1514451576545_1901042544",
      "id": "20171228-085936_1984757508_q_6BSRX622QQ1530181423",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "a: Array[String] \u003d Array(/usr/java/jdk1.8.0_161/jre/lib/amd64/server:/usr/java/jdk1.8.0_161/jre/lib/amd64:/usr/java/jdk1.8.0_161/jre/../lib/amd64:/usr/lib/hadoop2/lib/native/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/lib/jvm/java-1.8.0/jre/lib/amd64/server, /usr/java/jdk1.8.0_161/jre/lib/amd64/server:/usr/java/jdk1.8.0_161/jre/lib/amd64:/usr/java/jdk1.8.0_161/jre/../lib/amd64:/usr/lib/hadoop2/lib/native/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/lib/jvm/java-1.8.0/jre/lib/amd64/server, /usr/java/jdk1.8.0_161/jre/lib/amd64/server:/usr/java/jdk1.8.0_161/jre/lib/amd64:/usr/java/jdk1.8.0_161/jre/../lib/amd64:/usr/lib/hadoop2/lib/native/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/lib/jvm/java-1.8.0/jre/lib/amd64/server, /usr/java/.../usr/java/jdk1.8.0_161/jre/lib/amd64/server:/usr/java/jdk1.8.0_161/jre/lib/amd64:/usr/java/jdk1.8.0_161/jre/../lib/amd64:/usr/lib/hadoop2/lib/native/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/lib/jvm/java-1.8.0/jre/lib/amd64/server\n/usr/java/jdk1.8.0_161/jre/lib/amd64/server:/usr/java/jdk1.8.0_161/jre/lib/amd64:/usr/java/jdk1.8.0_161/jre/../lib/amd64:/usr/lib/hadoop2/lib/native/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/lib/jvm/java-1.8.0/jre/lib/amd64/server\n/usr/java/jdk1.8.0_161/jre/lib/amd64/server:/usr/java/jdk1.8.0_161/jre/lib/amd64:/usr/java/jdk1.8.0_161/jre/../lib/amd64:/usr/lib/hadoop2/lib/native/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/lib/jvm/java-1.8.0/jre/lib/amd64/server\n/usr/java/jdk1.8.0_161/jre/lib/amd64/server:/usr/java/jdk1.8.0_161/jre/lib/amd64:/usr/java/jdk1.8.0_161/jre/../lib/amd64:/usr/lib/hadoop2/lib/native/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/lib/jvm/java-1.8.0/jre/lib/amd64/server\n/usr/java/jdk1.8.0_161/jre/lib/amd64/server:/usr/java/jdk1.8.0_161/jre/lib/amd64:/usr/java/jdk1.8.0_161/jre/../lib/amd64:/usr/lib/hadoop2/lib/native/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/lib/jvm/java-1.8.0/jre/lib/amd64/server\n/usr/java/jdk1.8.0_161/jre/lib/amd64/server:/usr/java/jdk1.8.0_161/jre/lib/amd64:/usr/java/jdk1.8.0_161/jre/../lib/amd64:/usr/lib/hadoop2/lib/native/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/lib/jvm/java-1.8.0/jre/lib/amd64/server\n/usr/java/jdk1.8.0_161/jre/lib/amd64/server:/usr/java/jdk1.8.0_161/jre/lib/amd64:/usr/java/jdk1.8.0_161/jre/../lib/amd64:/usr/lib/hadoop2/lib/native/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/lib/jvm/java-1.8.0/jre/lib/amd64/server\n/usr/java/jdk1.8.0_161/jre/lib/amd64/server:/usr/java/jdk1.8.0_161/jre/lib/amd64:/usr/java/jdk1.8.0_161/jre/../lib/amd64:/usr/lib/hadoop2/lib/native/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/lib/jvm/java-1.8.0/jre/lib/amd64/server\n/usr/java/jdk1.8.0_161/jre/lib/amd64/server:/usr/java/jdk1.8.0_161/jre/lib/amd64:/usr/java/jdk1.8.0_161/jre/../lib/amd64:/usr/lib/hadoop2/lib/native/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/lib/jvm/java-1.8.0/jre/lib/amd64/server\n/usr/java/jdk1.8.0_161/jre/lib/amd64/server:/usr/java/jdk1.8.0_161/jre/lib/amd64:/usr/java/jdk1.8.0_161/jre/../lib/amd64:/usr/lib/hadoop2/lib/native/:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:/usr/lib/jvm/java-1.8.0/jre/lib/amd64/server\n"
      },
      "dateCreated": "Dec 28, 2017 8:59:36 AM",
      "dateSubmitted": "Jun 28, 2018 10:25:41 AM",
      "dateStarted": "Jun 28, 2018 10:28:22 AM",
      "dateFinished": "Jun 28, 2018 10:28:30 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%spark\nvar a \u003d sc.parallelize(1 to 10).map(x \u003d\u003e sys.env(\"JAVA_HOME\")).collect\na.foreach(println)",
      "dateUpdated": "Dec 28, 2017 9:12:50 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "JOB UI",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            "https://qa3.qubole.net/cluster-proxy?encodedUrl\u003dhttp%3A%2F%2F172.30.0.31%3A8088%2Fproxy%2Fapplication_1530175854947_0100/jobs/job?spark\u003dtrue\u0026id\u003d4"
          ],
          "interpreterSettingId": "2DHYD2X9H326581530181442072"
        }
      },
      "version": "v0",
      "jobName": "paragraph_1514452339321_-636901884",
      "id": "20171228-091219_1495025578_q_6BSRX622QQ1530181423",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\na: Array[String] \u003d Array(/usr/lib/jvm/java-1.8.0/jre/, /usr/lib/jvm/java-1.8.0/jre/, /usr/lib/jvm/java-1.8.0/jre/, /usr/lib/jvm/java-1.8.0/jre/, /usr/lib/jvm/java-1.8.0/jre/, /usr/lib/jvm/java-1.8.0/jre/, /usr/lib/jvm/java-1.8.0/jre/, /usr/lib/jvm/java-1.8.0/jre/, /usr/lib/jvm/java-1.8.0/jre/, /usr/lib/jvm/java-1.8.0/jre/)\n/usr/lib/jvm/java-1.8.0/jre/\n/usr/lib/jvm/java-1.8.0/jre/\n/usr/lib/jvm/java-1.8.0/jre/\n/usr/lib/jvm/java-1.8.0/jre/\n/usr/lib/jvm/java-1.8.0/jre/\n/usr/lib/jvm/java-1.8.0/jre/\n/usr/lib/jvm/java-1.8.0/jre/\n/usr/lib/jvm/java-1.8.0/jre/\n/usr/lib/jvm/java-1.8.0/jre/\n/usr/lib/jvm/java-1.8.0/jre/\n"
      },
      "dateCreated": "Dec 28, 2017 9:12:19 AM",
      "dateSubmitted": "Jun 28, 2018 10:28:22 AM",
      "dateStarted": "Jun 28, 2018 10:28:30 AM",
      "dateFinished": "Jun 28, 2018 10:28:32 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1514452370008_2050446590",
      "id": "20171228-091250_1648118415_q_6BSRX622QQ1530181423",
      "dateCreated": "Dec 28, 2017 9:12:50 AM",
      "dateSubmitted": "Jun 28, 2018 10:28:30 AM",
      "dateStarted": "Jun 28, 2018 11:14:45 AM",
      "dateFinished": "Jun 28, 2018 11:14:45 AM",
      "status": "ABORT",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "tfos-123",
  "id": "6BSRX622QQ1530181423",
  "angularObjects": {
    "2DHYD2X9H326581530181442072:shared_process": [],
    "2DG81HXZT326581530175779354:shared_process": [],
    "2DG7XXNXZ326581530175779295:shared_process": [],
    "2DKD7WN1R326581530175779340:shared_process": [],
    "2DJWM1ZTZ326581530175779351:shared_process": []
  },
  "config": {
    "isDashboard": false,
    "defaultLang": "pyspark"
  },
  "info": {},
  "source": "FCN"
}