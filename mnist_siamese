{
  "paragraphs": [
    {
      "text": "os.environ[\"KERAS_BACKEND\"] \u003d \"theano\"\nos.environ[\u0027THEANO_FLAGS\u0027] \u003d \u0027floatX\u003dfloat32,device\u003dcuda,dnn.library_path\u003d/usr/local/cuda/lib64,dnn.include_path\u003d/usr/local/cuda/include/\u0027",
      "user": "prashantp@qubole.com",
      "dateUpdated": "Jan 22, 2018 1:34:25 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1516358450971_1498611767",
      "id": "20180119-104050_729682499_q_E2D5GV4MB61516358383",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Jan 19, 2018 10:40:50 AM",
      "dateStarted": "Jan 22, 2018 1:34:25 PM",
      "dateFinished": "Jan 22, 2018 1:34:25 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "# Confirming backend is changed to Theano\nfrom __future__ import print_function\nimport keras\nfrom keras import backend as K\nprint(K.backend())",
      "user": "prashantp@qubole.com",
      "dateUpdated": "Jan 22, 2018 1:34:25 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1516358458612_-1541289293",
      "id": "20180119-104058_1414842380_q_E2D5GV4MB61516358383",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "theano\n"
      },
      "dateCreated": "Jan 19, 2018 10:40:58 AM",
      "dateStarted": "Jan 22, 2018 1:34:25 PM",
      "dateFinished": "Jan 22, 2018 1:34:25 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "from __future__ import absolute_import\nfrom __future__ import print_function\nimport numpy as np\n\nimport random\nfrom keras.datasets import mnist\nfrom keras.models import Model\nfrom keras.layers import Input, Flatten, Dense, Dropout, Lambda\nfrom keras.optimizers import RMSprop\nfrom keras import backend as K\n\nnum_classes \u003d 10\nepochs \u003d 5\n\n\ndef euclidean_distance(vects):\n    x, y \u003d vects\n    return K.sqrt(K.maximum(K.sum(K.square(x - y), axis\u003d1, keepdims\u003dTrue), K.epsilon()))\n\n\ndef eucl_dist_output_shape(shapes):\n    shape1, shape2 \u003d shapes\n    return (shape1[0], 1)\n\n\ndef contrastive_loss(y_true, y_pred):\n    \u0027\u0027\u0027Contrastive loss from Hadsell-et-al.\u002706\n    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n    \u0027\u0027\u0027\n    margin \u003d 1\n    return K.mean(y_true * K.square(y_pred) +\n                  (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))\n\n\ndef create_pairs(x, digit_indices):\n    \u0027\u0027\u0027Positive and negative pair creation.\n    Alternates between positive and negative pairs.\n    \u0027\u0027\u0027\n    pairs \u003d []\n    labels \u003d []\n    n \u003d min([len(digit_indices[d]) for d in range(num_classes)]) - 1\n    for d in range(num_classes):\n        for i in range(n):\n            z1, z2 \u003d digit_indices[d][i], digit_indices[d][i + 1]\n            pairs +\u003d [[x[z1], x[z2]]]\n            inc \u003d random.randrange(1, num_classes)\n            dn \u003d (d + inc) % num_classes\n            z1, z2 \u003d digit_indices[d][i], digit_indices[dn][i]\n            pairs +\u003d [[x[z1], x[z2]]]\n            labels +\u003d [1, 0]\n    return np.array(pairs), np.array(labels)\n\n\ndef create_base_network(input_shape):\n    \u0027\u0027\u0027Base network to be shared (eq. to feature extraction).\n    \u0027\u0027\u0027\n    input \u003d Input(shape\u003dinput_shape)\n    x \u003d Flatten()(input)\n    x \u003d Dense(128, activation\u003d\u0027relu\u0027)(x)\n    x \u003d Dropout(0.1)(x)\n    x \u003d Dense(128, activation\u003d\u0027relu\u0027)(x)\n    x \u003d Dropout(0.1)(x)\n    x \u003d Dense(128, activation\u003d\u0027relu\u0027)(x)\n    return Model(input, x)\n\n\ndef compute_accuracy(y_true, y_pred):\n    \u0027\u0027\u0027Compute classification accuracy with a fixed threshold on distances.\n    \u0027\u0027\u0027\n    pred \u003d y_pred.ravel() \u003c 0.5\n    return np.mean(pred \u003d\u003d y_true)\n\n\ndef accuracy(y_true, y_pred):\n    \u0027\u0027\u0027Compute classification accuracy with a fixed threshold on distances.\n    \u0027\u0027\u0027\n    return K.mean(K.equal(y_true, K.cast(y_pred \u003c 0.5, y_true.dtype)))\n\n\n# the data, shuffled and split between train and test sets\n(x_train, y_train), (x_test, y_test) \u003d mnist.load_data()\nx_train \u003d x_train.astype(\u0027float32\u0027)\nx_test \u003d x_test.astype(\u0027float32\u0027)\nx_train /\u003d 255\nx_test /\u003d 255\ninput_shape \u003d x_train.shape[1:]\n\n# create training+test positive and negative pairs\ndigit_indices \u003d [np.where(y_train \u003d\u003d i)[0] for i in range(num_classes)]\ntr_pairs, tr_y \u003d create_pairs(x_train, digit_indices)\n\ndigit_indices \u003d [np.where(y_test \u003d\u003d i)[0] for i in range(num_classes)]\nte_pairs, te_y \u003d create_pairs(x_test, digit_indices)\n\n# network definition\nbase_network \u003d create_base_network(input_shape)\n\ninput_a \u003d Input(shape\u003dinput_shape)\ninput_b \u003d Input(shape\u003dinput_shape)\n\n# because we re-use the same instance `base_network`,\n# the weights of the network\n# will be shared across the two branches\nprocessed_a \u003d base_network(input_a)\nprocessed_b \u003d base_network(input_b)\n\ndistance \u003d Lambda(euclidean_distance,\n                  output_shape\u003deucl_dist_output_shape)([processed_a, processed_b])\n\nmodel \u003d Model([input_a, input_b], distance)\n\n# train\nrms \u003d RMSprop()\nmodel.compile(loss\u003dcontrastive_loss, optimizer\u003drms, metrics\u003d[accuracy])\nmodel.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n          batch_size\u003d128,\n          epochs\u003depochs,\n          validation_data\u003d([te_pairs[:, 0], te_pairs[:, 1]], te_y))\n\n# compute final accuracy on training and test sets\ny_pred \u003d model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\ntr_acc \u003d compute_accuracy(tr_y, y_pred)\ny_pred \u003d model.predict([te_pairs[:, 0], te_pairs[:, 1]])\nte_acc \u003d compute_accuracy(te_y, y_pred)\n\nprint(\u0027* Accuracy on training set: %0.2f%%\u0027 % (100 * tr_acc))\nprint(\u0027* Accuracy on test set: %0.2f%%\u0027 % (100 * te_acc))\n",
      "user": "prashantp@qubole.com",
      "dateUpdated": "Jan 22, 2018 1:34:25 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [],
        "numCompletedTasks": 0,
        "numTasks": 0,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1516097727976_525702703",
      "id": "20180116-101527_1797067755_q_E2D5GV4MB61516358383",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "Train on 108400 samples, validate on 17820 samples\nEpoch 1/5\n\n   128/108400 [..............................] - ETA: 5s - loss: 1.4899 - accuracy: 0.4844\n\n  2432/108400 [..............................] - ETA: 2s - loss: 0.3191 - accuracy: 0.5522\n\n  4736/108400 [\u003e.............................] - ETA: 2s - loss: 0.2744 - accuracy: 0.5857\n\n  7168/108400 [\u003e.............................] - ETA: 2s - loss: 0.2491 - accuracy: 0.6247\n\n  9600/108400 [\u003d\u003e............................] - ETA: 2s - loss: 0.2290 - accuracy: 0.6602\n\n 12032/108400 [\u003d\u003d\u003e...........................] - ETA: 2s - loss: 0.2148 - accuracy: 0.6853\n\n 14464/108400 [\u003d\u003d\u003d\u003e..........................] - ETA: 2s - loss: 0.2033 - accuracy: 0.7064\n\n 16896/108400 [\u003d\u003d\u003d\u003e..........................] - ETA: 1s - loss: 0.1936 - accuracy: 0.7238\n\n 19328/108400 [\u003d\u003d\u003d\u003d\u003e.........................] - ETA: 1s - loss: 0.1855 - accuracy: 0.7388\n\n 21760/108400 [\u003d\u003d\u003d\u003d\u003d\u003e........................] - ETA: 1s - loss: 0.1784 - accuracy: 0.7511\n\n 24192/108400 [\u003d\u003d\u003d\u003d\u003d\u003e........................] - ETA: 1s - loss: 0.1726 - accuracy: 0.7610\n\n 26624/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003e.......................] - ETA: 1s - loss: 0.1677 - accuracy: 0.7695\n\n 29056/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e......................] - ETA: 1s - loss: 0.1629 - accuracy: 0.7784\n\n 31488/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e......................] - ETA: 1s - loss: 0.1585 - accuracy: 0.7863\n\n 33920/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.....................] - ETA: 1s - loss: 0.1550 - accuracy: 0.7925\n\n 36352/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e....................] - ETA: 1s - loss: 0.1512 - accuracy: 0.7990\n\n 38784/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e....................] - ETA: 1s - loss: 0.1480 - accuracy: 0.8043\n\n 41216/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...................] - ETA: 1s - loss: 0.1449 - accuracy: 0.8099\n\n 43648/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..................] - ETA: 1s - loss: 0.1419 - accuracy: 0.8147\n\n 46080/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..................] - ETA: 1s - loss: 0.1395 - accuracy: 0.8191\n\n 48512/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.................] - ETA: 1s - loss: 0.1367 - accuracy: 0.8236\n\n 50944/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e................] - ETA: 1s - loss: 0.1342 - accuracy: 0.8277\n\n 53376/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e................] - ETA: 1s - loss: 0.1318 - accuracy: 0.8314\n\n 55808/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...............] - ETA: 1s - loss: 0.1295 - accuracy: 0.8351\n\n 58240/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..............] - ETA: 1s - loss: 0.1273 - accuracy: 0.8385\n\n 60672/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..............] - ETA: 1s - loss: 0.1253 - accuracy: 0.8418\n\n 63104/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.............] - ETA: 0s - loss: 0.1235 - accuracy: 0.8449\n\n 65536/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e............] - ETA: 0s - loss: 0.1216 - accuracy: 0.8478\n\n 67968/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e............] - ETA: 0s - loss: 0.1199 - accuracy: 0.8505\n\n 70400/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...........] - ETA: 0s - loss: 0.1183 - accuracy: 0.8528\n\n 72832/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..........] - ETA: 0s - loss: 0.1166 - accuracy: 0.8554\n\n 75264/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..........] - ETA: 0s - loss: 0.1149 - accuracy: 0.8581\n\n 77696/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.........] - ETA: 0s - loss: 0.1133 - accuracy: 0.8605\n\n 80128/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e........] - ETA: 0s - loss: 0.1118 - accuracy: 0.8627\n\n 82560/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e........] - ETA: 0s - loss: 0.1103 - accuracy: 0.8649\n\n 84992/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.......] - ETA: 0s - loss: 0.1090 - accuracy: 0.8669\n\n 87424/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e......] - ETA: 0s - loss: 0.1077 - accuracy: 0.8688\n\n 89856/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e......] - ETA: 0s - loss: 0.1065 - accuracy: 0.8707\n\n 92288/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.....] - ETA: 0s - loss: 0.1054 - accuracy: 0.8723\n\n 94720/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e....] - ETA: 0s - loss: 0.1042 - accuracy: 0.8740\n\n 97152/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e....] - ETA: 0s - loss: 0.1031 - accuracy: 0.8755\n\n 99584/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...] - ETA: 0s - loss: 0.1020 - accuracy: 0.8772\n\n102016/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..] - ETA: 0s - loss: 0.1008 - accuracy: 0.8790\n\n104448/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..] - ETA: 0s - loss: 0.0997 - accuracy: 0.8804\n\n106880/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.] - ETA: 0s - loss: 0.0986 - accuracy: 0.8819\n\n108400/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 3s 23us/step - loss: 0.0979 - accuracy: 0.8829 - val_loss: 0.0437 - val_accuracy: 0.9542\n\nEpoch 2/5\n\n   128/108400 [..............................] - ETA: 5s - loss: 0.0507 - accuracy: 0.9453\n\n  2560/108400 [..............................] - ETA: 2s - loss: 0.0502 - accuracy: 0.9473\n\n  4992/108400 [\u003e.............................] - ETA: 2s - loss: 0.0494 - accuracy: 0.9503\n\n  7424/108400 [\u003d\u003e............................] - ETA: 2s - loss: 0.0497 - accuracy: 0.9507\n\n  9856/108400 [\u003d\u003e............................] - ETA: 2s - loss: 0.0490 - accuracy: 0.9513\n\n 12288/108400 [\u003d\u003d\u003e...........................] - ETA: 2s - loss: 0.0488 - accuracy: 0.9522\n\n 14720/108400 [\u003d\u003d\u003d\u003e..........................] - ETA: 1s - loss: 0.0489 - accuracy: 0.9517\n\n 17152/108400 [\u003d\u003d\u003d\u003e..........................] - ETA: 1s - loss: 0.0483 - accuracy: 0.9523\n\n 19584/108400 [\u003d\u003d\u003d\u003d\u003e.........................] - ETA: 1s - loss: 0.0476 - accuracy: 0.9534\n\n 22016/108400 [\u003d\u003d\u003d\u003d\u003d\u003e........................] - ETA: 1s - loss: 0.0472 - accuracy: 0.9536\n\n 24448/108400 [\u003d\u003d\u003d\u003d\u003d\u003e........................] - ETA: 1s - loss: 0.0471 - accuracy: 0.9535\n\n 26880/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003e.......................] - ETA: 1s - loss: 0.0468 - accuracy: 0.9536\n\n 29312/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e......................] - ETA: 1s - loss: 0.0462 - accuracy: 0.9543\n\n 31744/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e......................] - ETA: 1s - loss: 0.0461 - accuracy: 0.9543\n\n 34176/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.....................] - ETA: 1s - loss: 0.0458 - accuracy: 0.9544\n\n 36608/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e....................] - ETA: 1s - loss: 0.0457 - accuracy: 0.9542\n\n 39040/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e....................] - ETA: 1s - loss: 0.0457 - accuracy: 0.9542\n\n 41472/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...................] - ETA: 1s - loss: 0.0457 - accuracy: 0.9544\n\n 43904/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..................] - ETA: 1s - loss: 0.0454 - accuracy: 0.9549\n\n 46336/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..................] - ETA: 1s - loss: 0.0449 - accuracy: 0.9556\n\n 48768/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.................] - ETA: 1s - loss: 0.0446 - accuracy: 0.9561\n\n 51200/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e................] - ETA: 1s - loss: 0.0442 - accuracy: 0.9565\n\n 53632/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e................] - ETA: 1s - loss: 0.0440 - accuracy: 0.9566\n\n 56064/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...............] - ETA: 1s - loss: 0.0438 - accuracy: 0.9570\n\n 58496/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..............] - ETA: 1s - loss: 0.0437 - accuracy: 0.9570\n\n 60928/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..............] - ETA: 1s - loss: 0.0434 - accuracy: 0.9573\n\n 63360/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.............] - ETA: 0s - loss: 0.0433 - accuracy: 0.9575\n\n 65792/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e............] - ETA: 0s - loss: 0.0430 - accuracy: 0.9578\n\n 68352/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e............] - ETA: 0s - loss: 0.0428 - accuracy: 0.9581\n\n 70784/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...........] - ETA: 0s - loss: 0.0426 - accuracy: 0.9582\n\n 73216/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..........] - ETA: 0s - loss: 0.0424 - accuracy: 0.9585\n\n 75648/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..........] - ETA: 0s - loss: 0.0422 - accuracy: 0.9588\n\n 78208/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.........] - ETA: 0s - loss: 0.0422 - accuracy: 0.9588\n\n 80768/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e........] - ETA: 0s - loss: 0.0421 - accuracy: 0.9590\n\n 83328/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.......] - ETA: 0s - loss: 0.0419 - accuracy: 0.9591\n\n 85760/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.......] - ETA: 0s - loss: 0.0418 - accuracy: 0.9592\n\n 88320/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e......] - ETA: 0s - loss: 0.0415 - accuracy: 0.9596\n\n 90752/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.....] - ETA: 0s - loss: 0.0413 - accuracy: 0.9600\n\n 92672/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.....] - ETA: 0s - loss: 0.0412 - accuracy: 0.9601\n\n 95104/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e....] - ETA: 0s - loss: 0.0411 - accuracy: 0.9602\n\n 97536/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e....] - ETA: 0s - loss: 0.0409 - accuracy: 0.9604\n\n 99968/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...] - ETA: 0s - loss: 0.0406 - accuracy: 0.9608\n\n102400/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..] - ETA: 0s - loss: 0.0405 - accuracy: 0.9609\n\n104960/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.] - ETA: 0s - loss: 0.0403 - accuracy: 0.9611\n\n107520/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.] - ETA: 0s - loss: 0.0401 - accuracy: 0.9613\n\n108400/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 2s 23us/step - loss: 0.0400 - accuracy: 0.9614 - val_loss: 0.0293 - val_accuracy: 0.9696\n\nEpoch 3/5\n\n   128/108400 [..............................] - ETA: 5s - loss: 0.0338 - accuracy: 0.9688\n\n  2688/108400 [..............................] - ETA: 2s - loss: 0.0302 - accuracy: 0.9736\n\n  5120/108400 [\u003e.............................] - ETA: 2s - loss: 0.0310 - accuracy: 0.9721\n\n  7680/108400 [\u003d\u003e............................] - ETA: 2s - loss: 0.0300 - accuracy: 0.9729\n\n 10240/108400 [\u003d\u003e............................] - ETA: 2s - loss: 0.0297 - accuracy: 0.9729\n\n 12800/108400 [\u003d\u003d\u003e...........................] - ETA: 1s - loss: 0.0300 - accuracy: 0.9727\n\n 15360/108400 [\u003d\u003d\u003d\u003e..........................] - ETA: 1s - loss: 0.0309 - accuracy: 0.9715\n\n 17920/108400 [\u003d\u003d\u003d\u003e..........................] - ETA: 1s - loss: 0.0309 - accuracy: 0.9712\n\n 20352/108400 [\u003d\u003d\u003d\u003d\u003e.........................] - ETA: 1s - loss: 0.0305 - accuracy: 0.9717\n\n 22912/108400 [\u003d\u003d\u003d\u003d\u003d\u003e........................] - ETA: 1s - loss: 0.0300 - accuracy: 0.9726\n\n 25472/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003e.......................] - ETA: 1s - loss: 0.0299 - accuracy: 0.9725\n\n 28032/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003e.......................] - ETA: 1s - loss: 0.0298 - accuracy: 0.9722\n\n 30336/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e......................] - ETA: 1s - loss: 0.0298 - accuracy: 0.9721\n\n 32896/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.....................] - ETA: 1s - loss: 0.0296 - accuracy: 0.9723\n\n 35456/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.....................] - ETA: 1s - loss: 0.0296 - accuracy: 0.9722\n\n 38016/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e....................] - ETA: 1s - loss: 0.0296 - accuracy: 0.9722\n\n 40448/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...................] - ETA: 1s - loss: 0.0295 - accuracy: 0.9722\n\n 42880/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...................] - ETA: 1s - loss: 0.0296 - accuracy: 0.9719\n\n 45312/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..................] - ETA: 1s - loss: 0.0295 - accuracy: 0.9720\n\n 47744/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.................] - ETA: 1s - loss: 0.0293 - accuracy: 0.9721\n\n 50176/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.................] - ETA: 1s - loss: 0.0292 - accuracy: 0.9722\n\n 52736/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e................] - ETA: 1s - loss: 0.0292 - accuracy: 0.9720\n\n 55296/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...............] - ETA: 1s - loss: 0.0292 - accuracy: 0.9719\n\n 57856/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..............] - ETA: 1s - loss: 0.0292 - accuracy: 0.9719\n\n 60288/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..............] - ETA: 0s - loss: 0.0291 - accuracy: 0.9719\n\n 62720/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.............] - ETA: 0s - loss: 0.0289 - accuracy: 0.9723\n\n 65280/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e............] - ETA: 0s - loss: 0.0287 - accuracy: 0.9725\n\n 67840/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e............] - ETA: 0s - loss: 0.0288 - accuracy: 0.9723\n\n 70400/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...........] - ETA: 0s - loss: 0.0287 - accuracy: 0.9724\n\n 72960/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..........] - ETA: 0s - loss: 0.0286 - accuracy: 0.9724\n\n 75392/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..........] - ETA: 0s - loss: 0.0286 - accuracy: 0.9724\n\n 77952/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.........] - ETA: 0s - loss: 0.0283 - accuracy: 0.9727\n\n 80512/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e........] - ETA: 0s - loss: 0.0284 - accuracy: 0.9726\n\n 83072/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e........] - ETA: 0s - loss: 0.0283 - accuracy: 0.9727\n\n 85632/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.......] - ETA: 0s - loss: 0.0282 - accuracy: 0.9728\n\n 88064/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e......] - ETA: 0s - loss: 0.0282 - accuracy: 0.9728\n\n 90496/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.....] - ETA: 0s - loss: 0.0282 - accuracy: 0.9728\n\n 93056/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.....] - ETA: 0s - loss: 0.0281 - accuracy: 0.9729\n\n 95488/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e....] - ETA: 0s - loss: 0.0281 - accuracy: 0.9729\n\n 97920/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...] - ETA: 0s - loss: 0.0280 - accuracy: 0.9729\n\n100480/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...] - ETA: 0s - loss: 0.0279 - accuracy: 0.9729\n\n102912/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..] - ETA: 0s - loss: 0.0279 - accuracy: 0.9729\n\n105472/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.] - ETA: 0s - loss: 0.0277 - accuracy: 0.9731\n\n108032/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.] - ETA: 0s - loss: 0.0277 - accuracy: 0.9731\n\n108400/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 2s 22us/step - loss: 0.0277 - accuracy: 0.9731 - val_loss: 0.0254 - val_accuracy: 0.9730\n\nEpoch 4/5\n\n   128/108400 [..............................] - ETA: 5s - loss: 0.0157 - accuracy: 0.9922\n\n  2688/108400 [..............................] - ETA: 2s - loss: 0.0236 - accuracy: 0.9773\n\n  5248/108400 [\u003e.............................] - ETA: 2s - loss: 0.0237 - accuracy: 0.9769\n\n  7680/108400 [\u003d\u003e............................] - ETA: 2s - loss: 0.0236 - accuracy: 0.9768\n\n 10240/108400 [\u003d\u003e............................] - ETA: 2s - loss: 0.0232 - accuracy: 0.9775\n\n 12800/108400 [\u003d\u003d\u003e...........................] - ETA: 1s - loss: 0.0231 - accuracy: 0.9773\n\n 15360/108400 [\u003d\u003d\u003d\u003e..........................] - ETA: 1s - loss: 0.0232 - accuracy: 0.9775\n\n 17920/108400 [\u003d\u003d\u003d\u003e..........................] - ETA: 1s - loss: 0.0235 - accuracy: 0.9771\n\n 20480/108400 [\u003d\u003d\u003d\u003d\u003e.........................] - ETA: 1s - loss: 0.0231 - accuracy: 0.9775\n\n 23040/108400 [\u003d\u003d\u003d\u003d\u003d\u003e........................] - ETA: 1s - loss: 0.0233 - accuracy: 0.9773\n\n 25600/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003e.......................] - ETA: 1s - loss: 0.0230 - accuracy: 0.9778\n\n 28160/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003e.......................] - ETA: 1s - loss: 0.0231 - accuracy: 0.9777\n\n 30720/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e......................] - ETA: 1s - loss: 0.0231 - accuracy: 0.9774\n\n 33280/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.....................] - ETA: 1s - loss: 0.0230 - accuracy: 0.9776\n\n 35712/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.....................] - ETA: 1s - loss: 0.0231 - accuracy: 0.9774\n\n 38272/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e....................] - ETA: 1s - loss: 0.0230 - accuracy: 0.9775\n\n 40832/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...................] - ETA: 1s - loss: 0.0227 - accuracy: 0.9777\n\n 43392/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..................] - ETA: 1s - loss: 0.0228 - accuracy: 0.9775\n\n 45952/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..................] - ETA: 1s - loss: 0.0229 - accuracy: 0.9774\n\n 48512/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.................] - ETA: 1s - loss: 0.0230 - accuracy: 0.9772\n\n 50944/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e................] - ETA: 1s - loss: 0.0228 - accuracy: 0.9774\n\n 53376/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e................] - ETA: 1s - loss: 0.0228 - accuracy: 0.9773\n\n 55936/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...............] - ETA: 1s - loss: 0.0229 - accuracy: 0.9771\n\n 58496/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..............] - ETA: 1s - loss: 0.0227 - accuracy: 0.9772\n\n 61056/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..............] - ETA: 0s - loss: 0.0225 - accuracy: 0.9774\n\n 63616/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.............] - ETA: 0s - loss: 0.0225 - accuracy: 0.9775\n\n 66176/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e............] - ETA: 0s - loss: 0.0225 - accuracy: 0.9775\n\n 68608/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e............] - ETA: 0s - loss: 0.0226 - accuracy: 0.9774\n\n 71040/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...........] - ETA: 0s - loss: 0.0226 - accuracy: 0.9774\n\n 73472/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..........] - ETA: 0s - loss: 0.0226 - accuracy: 0.9775\n\n 76032/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.........] - ETA: 0s - loss: 0.0226 - accuracy: 0.9774\n\n 78592/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.........] - ETA: 0s - loss: 0.0225 - accuracy: 0.9775\n\n 81152/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e........] - ETA: 0s - loss: 0.0225 - accuracy: 0.9774\n\n 83712/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.......] - ETA: 0s - loss: 0.0226 - accuracy: 0.9774\n\n 86272/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.......] - ETA: 0s - loss: 0.0226 - accuracy: 0.9772\n\n 88832/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e......] - ETA: 0s - loss: 0.0226 - accuracy: 0.9773\n\n 91392/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.....] - ETA: 0s - loss: 0.0226 - accuracy: 0.9772\n\n 93952/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e....] - ETA: 0s - loss: 0.0226 - accuracy: 0.9772\n\n 96512/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e....] - ETA: 0s - loss: 0.0225 - accuracy: 0.9773\n\n 99072/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...] - ETA: 0s - loss: 0.0225 - accuracy: 0.9773\n\n101632/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..] - ETA: 0s - loss: 0.0225 - accuracy: 0.9774\n\n104192/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..] - ETA: 0s - loss: 0.0226 - accuracy: 0.9772\n\n106752/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.] - ETA: 0s - loss: 0.0225 - accuracy: 0.9773\n\n108400/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 2s 22us/step - loss: 0.0225 - accuracy: 0.9773 - val_loss: 0.0251 - val_accuracy: 0.9733\n\nEpoch 5/5\n\n   128/108400 [..............................] - ETA: 5s - loss: 0.0176 - accuracy: 0.9766\n\n  2688/108400 [..............................] - ETA: 2s - loss: 0.0232 - accuracy: 0.9743\n\n  5248/108400 [\u003e.............................] - ETA: 2s - loss: 0.0208 - accuracy: 0.9783\n\n  7680/108400 [\u003d\u003e............................] - ETA: 2s - loss: 0.0204 - accuracy: 0.9792\n\n 10240/108400 [\u003d\u003e............................] - ETA: 2s - loss: 0.0199 - accuracy: 0.9804\n\n 12672/108400 [\u003d\u003d\u003e...........................] - ETA: 1s - loss: 0.0203 - accuracy: 0.9799\n\n 15104/108400 [\u003d\u003d\u003d\u003e..........................] - ETA: 1s - loss: 0.0203 - accuracy: 0.9804\n\n 17536/108400 [\u003d\u003d\u003d\u003e..........................] - ETA: 1s - loss: 0.0201 - accuracy: 0.9803\n\n 20096/108400 [\u003d\u003d\u003d\u003d\u003e.........................] - ETA: 1s - loss: 0.0199 - accuracy: 0.9806\n\n 22656/108400 [\u003d\u003d\u003d\u003d\u003d\u003e........................] - ETA: 1s - loss: 0.0199 - accuracy: 0.9804\n\n 25216/108400 [\u003d\u003d\u003d\u003d\u003d\u003e........................] - ETA: 1s - loss: 0.0199 - accuracy: 0.9806\n\n 27776/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003e.......................] - ETA: 1s - loss: 0.0200 - accuracy: 0.9805\n\n 30336/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e......................] - ETA: 1s - loss: 0.0196 - accuracy: 0.9808\n\n 32896/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.....................] - ETA: 1s - loss: 0.0197 - accuracy: 0.9805\n\n 35456/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.....................] - ETA: 1s - loss: 0.0198 - accuracy: 0.9805\n\n 38016/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e....................] - ETA: 1s - loss: 0.0197 - accuracy: 0.9807\n\n 40448/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...................] - ETA: 1s - loss: 0.0197 - accuracy: 0.9807\n\n 42880/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...................] - ETA: 1s - loss: 0.0197 - accuracy: 0.9807\n\n 45440/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..................] - ETA: 1s - loss: 0.0197 - accuracy: 0.9807\n\n 48000/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.................] - ETA: 1s - loss: 0.0196 - accuracy: 0.9809\n\n 50560/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.................] - ETA: 1s - loss: 0.0197 - accuracy: 0.9807\n\n 53120/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e................] - ETA: 1s - loss: 0.0196 - accuracy: 0.9807\n\n 55680/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...............] - ETA: 1s - loss: 0.0195 - accuracy: 0.9809\n\n 58240/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..............] - ETA: 1s - loss: 0.0193 - accuracy: 0.9811\n\n 60800/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..............] - ETA: 0s - loss: 0.0194 - accuracy: 0.9810\n\n 63360/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.............] - ETA: 0s - loss: 0.0194 - accuracy: 0.9810\n\n 65920/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e............] - ETA: 0s - loss: 0.0193 - accuracy: 0.9811\n\n 68480/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e............] - ETA: 0s - loss: 0.0193 - accuracy: 0.9811\n\n 71040/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...........] - ETA: 0s - loss: 0.0194 - accuracy: 0.9810\n\n 73472/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..........] - ETA: 0s - loss: 0.0195 - accuracy: 0.9808\n\n 75904/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.........] - ETA: 0s - loss: 0.0196 - accuracy: 0.9806\n\n 78336/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.........] - ETA: 0s - loss: 0.0195 - accuracy: 0.9807\n\n 80768/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e........] - ETA: 0s - loss: 0.0195 - accuracy: 0.9808\n\n 83200/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.......] - ETA: 0s - loss: 0.0195 - accuracy: 0.9807\n\n 85760/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.......] - ETA: 0s - loss: 0.0194 - accuracy: 0.9807\n\n 88320/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e......] - ETA: 0s - loss: 0.0194 - accuracy: 0.9808\n\n 90880/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.....] - ETA: 0s - loss: 0.0193 - accuracy: 0.9808\n\n 93440/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.....] - ETA: 0s - loss: 0.0194 - accuracy: 0.9808\n\n 95872/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e....] - ETA: 0s - loss: 0.0193 - accuracy: 0.9808\n\n 98432/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...] - ETA: 0s - loss: 0.0193 - accuracy: 0.9808\n\n100864/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e...] - ETA: 0s - loss: 0.0193 - accuracy: 0.9807\n\n103424/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e..] - ETA: 0s - loss: 0.0193 - accuracy: 0.9808\n\n105984/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003e.] - ETA: 0s - loss: 0.0193 - accuracy: 0.9808\n\n108400/108400 [\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d\u003d] - 2s 22us/step - loss: 0.0194 - accuracy: 0.9807 - val_loss: 0.0234 - val_accuracy: 0.9738\n\n* Accuracy on training set: 98.82%\n* Accuracy on test set: 97.38%\n"
      },
      "dateCreated": "Jan 16, 2018 10:15:27 AM",
      "dateStarted": "Jan 22, 2018 1:34:25 PM",
      "dateFinished": "Jan 22, 2018 1:35:32 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "Input : The MNIST dataset contains 60000 training images of 28x28 size and 10000 testing greyscale images of 28x28 size.\n    \n    The dataset is divided into one training batch with 60000 images and one test batch with 10000 images.\n    \n    Each batch contains a dictionary with the following elements:-\n    \n    data -- a 60000x28x28 numpy array of uint8s in case of training sample and 10000x28x28 numpy array of uint8s in case of testing sample. The first dimension of the array \n    marks the image number and the other dimensions is for the image matrix.\n    labels -- a list of 60000 numbers which are either 0 or 1 for training sample and 10000 numbers  for testing sample. The number at index i indicates the label of the ith            image in the array data.\n    \nOutput : The accuracy of the trained model on the training set and the testing set. Real number in the range 0-1.",
      "user": "prashantp@qubole.com",
      "dateUpdated": "Jan 22, 2018 1:34:25 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": false,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1516110102405_-153071877",
      "id": "20180116-134142_793682217_q_E2D5GV4MB61516358383",
      "dateCreated": "Jan 16, 2018 1:41:42 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "mnist_siamese",
  "id": "E2D5GV4MB61516358383",
  "angularObjects": {
    "2D7M1HZP5932661518613479637:shared_process": [],
    "2D6B43M8G932661515762929145:shared_process": [],
    "2D35KXZZK932661515762929137:shared_process": []
  },
  "config": {
    "isDashboard": false
  },
  "info": {},
  "source": "FCN"
}