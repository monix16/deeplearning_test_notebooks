{
  "paragraphs": [
    {
      "text": "from __future__ import print_function\n\nfrom collections import defaultdict\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle\nfrom PIL import Image\n\nfrom six.moves import range\n\nfrom keras.datasets import mnist\nfrom keras import layers\nfrom keras.layers import Input, Dense, Reshape, Flatten, Embedding, Dropout\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.convolutional import Conv2DTranspose, Conv2D\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam\nfrom keras.utils.generic_utils import Progbar\nimport numpy as np\n\nnp.random.seed(1337)\nnum_classes \u003d 10\n\n\ndef build_generator(latent_size):\n    # we will map a pair of (z, L), where z is a latent vector and L is a\n    # label drawn from P_c, to image space (..., 28, 28, 1)\n    cnn \u003d Sequential()\n\n    cnn.add(Dense(3 * 3 * 384, input_dim\u003dlatent_size, activation\u003d\u0027relu\u0027))\n    cnn.add(Reshape((3, 3, 384)))\n\n    # upsample to (7, 7, ...)\n    cnn.add(Conv2DTranspose(192, 5, strides\u003d1, padding\u003d\u0027valid\u0027,\n                            activation\u003d\u0027relu\u0027,\n                            kernel_initializer\u003d\u0027glorot_normal\u0027))\n\n    # upsample to (14, 14, ...)\n    cnn.add(Conv2DTranspose(96, 5, strides\u003d2, padding\u003d\u0027same\u0027,\n                            activation\u003d\u0027relu\u0027,\n                            kernel_initializer\u003d\u0027glorot_normal\u0027))\n\n    # upsample to (28, 28, ...)\n    cnn.add(Conv2DTranspose(1, 5, strides\u003d2, padding\u003d\u0027same\u0027,\n                            activation\u003d\u0027tanh\u0027,\n                            kernel_initializer\u003d\u0027glorot_normal\u0027))\n\n    # this is the z space commonly referred to in GAN papers\n    latent \u003d Input(shape\u003d(latent_size, ))\n\n    # this will be our label\n    image_class \u003d Input(shape\u003d(1,), dtype\u003d\u0027int32\u0027)\n\n    cls \u003d Flatten()(Embedding(num_classes, latent_size,\n                              embeddings_initializer\u003d\u0027glorot_normal\u0027)(image_class))\n\n    # hadamard product between z-space and a class conditional embedding\n    h \u003d layers.multiply([latent, cls])\n\n    fake_image \u003d cnn(h)\n\n    return Model([latent, image_class], fake_image)\n\n\ndef build_discriminator():\n    # build a relatively standard conv net, with LeakyReLUs as suggested in\n    # the reference paper\n    cnn \u003d Sequential()\n\n    cnn.add(Conv2D(32, 3, padding\u003d\u0027same\u0027, strides\u003d2,\n                   input_shape\u003d(28, 28, 1)))\n    cnn.add(LeakyReLU(0.2))\n    cnn.add(Dropout(0.3))\n\n    cnn.add(Conv2D(64, 3, padding\u003d\u0027same\u0027, strides\u003d1))\n    cnn.add(LeakyReLU(0.2))\n    cnn.add(Dropout(0.3))\n\n    cnn.add(Conv2D(128, 3, padding\u003d\u0027same\u0027, strides\u003d2))\n    cnn.add(LeakyReLU(0.2))\n    cnn.add(Dropout(0.3))\n\n    cnn.add(Conv2D(256, 3, padding\u003d\u0027same\u0027, strides\u003d1))\n    cnn.add(LeakyReLU(0.2))\n    cnn.add(Dropout(0.3))\n\n    cnn.add(Flatten())\n\n    image \u003d Input(shape\u003d(28, 28, 1))\n\n    features \u003d cnn(image)\n\n    # first output (name\u003dgeneration) is whether or not the discriminator\n    # thinks the image that is being shown is fake, and the second output\n    # (name\u003dauxiliary) is the class that the discriminator thinks the image\n    # belongs to.\n    fake \u003d Dense(1, activation\u003d\u0027sigmoid\u0027, name\u003d\u0027generation\u0027)(features)\n    aux \u003d Dense(num_classes, activation\u003d\u0027softmax\u0027, name\u003d\u0027auxiliary\u0027)(features)\n\n    return Model(image, [fake, aux])\n\nif __name__ \u003d\u003d \u0027__main__\u0027:\n\n    # batch and latent size taken from the paper\n    epochs \u003d 1\n    batch_size \u003d 100\n    latent_size \u003d 100\n\n    # Adam parameters suggested in https://arxiv.org/abs/1511.06434\n    adam_lr \u003d 0.0002\n    adam_beta_1 \u003d 0.5\n\n    # build the discriminator\n    print(\u0027Discriminator model:\u0027)\n    discriminator \u003d build_discriminator()\n    discriminator.compile(\n        optimizer\u003dAdam(lr\u003dadam_lr, beta_1\u003dadam_beta_1),\n        loss\u003d[\u0027binary_crossentropy\u0027, \u0027sparse_categorical_crossentropy\u0027]\n    )\n    discriminator.summary()\n\n    # build the generator\n    generator \u003d build_generator(latent_size)\n\n    latent \u003d Input(shape\u003d(latent_size, ))\n    image_class \u003d Input(shape\u003d(1,), dtype\u003d\u0027int32\u0027)\n\n    # get a fake image\n    fake \u003d generator([latent, image_class])\n\n    # we only want to be able to train generation for the combined model\n    discriminator.trainable \u003d False\n    fake, aux \u003d discriminator(fake)\n    combined \u003d Model([latent, image_class], [fake, aux])\n\n    print(\u0027Combined model:\u0027)\n    combined.compile(\n        optimizer\u003dAdam(lr\u003dadam_lr, beta_1\u003dadam_beta_1),\n        loss\u003d[\u0027binary_crossentropy\u0027, \u0027sparse_categorical_crossentropy\u0027]\n    )\n    combined.summary()\n\n    # get our mnist data, and force it to be of shape (..., 28, 28, 1) with\n    # range [-1, 1]\n    (x_train, y_train), (x_test, y_test) \u003d mnist.load_data()\n    x_train \u003d (x_train.astype(np.float32) - 127.5) / 127.5\n    x_train \u003d np.expand_dims(x_train, axis\u003d-1)\n\n    x_test \u003d (x_test.astype(np.float32) - 127.5) / 127.5\n    x_test \u003d np.expand_dims(x_test, axis\u003d-1)\n\n    num_train, num_test \u003d x_train.shape[0], x_test.shape[0]\n\n    train_history \u003d defaultdict(list)\n    test_history \u003d defaultdict(list)\n\n    for epoch in range(1, epochs + 1):\n        print(\u0027Epoch {}/{}\u0027.format(epoch, epochs))\n\n        num_batches \u003d int(x_train.shape[0] / batch_size)\n        progress_bar \u003d Progbar(target\u003dnum_batches)\n\n        # we don\u0027t want the discriminator to also maximize the classification\n        # accuracy of the auxiliary classifier on generated images, so we\n        # don\u0027t train discriminator to produce class labels for generated\n        # images (see https://openreview.net/forum?id\u003drJXTf9Bxg).\n        # To preserve sum of sample weights for the auxiliary classifier,\n        # we assign sample weight of 2 to the real images.\n        disc_sample_weight \u003d [np.ones(2 * batch_size),\n                              np.concatenate((np.ones(batch_size) * 2,\n                                              np.zeros(batch_size)))]\n\n        epoch_gen_loss \u003d []\n        epoch_disc_loss \u003d []\n\n        for index in range(num_batches):\n            # generate a new batch of noise\n            noise \u003d np.random.uniform(-1, 1, (batch_size, latent_size))\n\n            # get a batch of real images\n            image_batch \u003d x_train[index * batch_size:(index + 1) * batch_size]\n            label_batch \u003d y_train[index * batch_size:(index + 1) * batch_size]\n\n            # sample some labels from p_c\n            sampled_labels \u003d np.random.randint(0, num_classes, batch_size)\n\n            # generate a batch of fake images, using the generated labels as a\n            # conditioner. We reshape the sampled labels to be\n            # (batch_size, 1) so that we can feed them into the embedding\n            # layer as a length one sequence\n            generated_images \u003d generator.predict(\n                [noise, sampled_labels.reshape((-1, 1))], verbose\u003d0)\n\n            x \u003d np.concatenate((image_batch, generated_images))\n\n            # use soft real/fake labels\n            soft_zero, soft_one \u003d 0.1, 0.9\n            y \u003d np.array([soft_one] * batch_size + [soft_zero] * batch_size)\n            aux_y \u003d np.concatenate((label_batch, sampled_labels), axis\u003d0)\n\n            # see if the discriminator can figure itself out...\n            epoch_disc_loss.append(discriminator.train_on_batch(\n                x, [y, aux_y], sample_weight\u003ddisc_sample_weight))\n\n            # make new noise. we generate 2 * batch size here such that we have\n            # the generator optimize over an identical number of images as the\n            # discriminator\n            noise \u003d np.random.uniform(-1, 1, (2 * batch_size, latent_size))\n            sampled_labels \u003d np.random.randint(0, num_classes, 2 * batch_size)\n\n            # we want to train the generator to trick the discriminator\n            # For the generator, we want all the {fake, not-fake} labels to say\n            # not-fake\n            trick \u003d np.ones(2 * batch_size) * soft_one\n\n            epoch_gen_loss.append(combined.train_on_batch(\n                [noise, sampled_labels.reshape((-1, 1))],\n                [trick, sampled_labels]))\n\n            progress_bar.update(index + 1)\n\n        print(\u0027Testing for epoch {}:\u0027.format(epoch))\n\n        # evaluate the testing loss here\n\n        # generate a new batch of noise\n        noise \u003d np.random.uniform(-1, 1, (num_test, latent_size))\n\n        # sample some labels from p_c and generate images from them\n        sampled_labels \u003d np.random.randint(0, num_classes, num_test)\n        generated_images \u003d generator.predict(\n            [noise, sampled_labels.reshape((-1, 1))], verbose\u003dFalse)\n\n        x \u003d np.concatenate((x_test, generated_images))\n        y \u003d np.array([1] * num_test + [0] * num_test)\n        aux_y \u003d np.concatenate((y_test, sampled_labels), axis\u003d0)\n\n        # see if the discriminator can figure itself out...\n        discriminator_test_loss \u003d discriminator.evaluate(\n            x, [y, aux_y], verbose\u003dFalse)\n\n        discriminator_train_loss \u003d np.mean(np.array(epoch_disc_loss), axis\u003d0)\n\n        # make new noise\n        noise \u003d np.random.uniform(-1, 1, (2 * num_test, latent_size))\n        sampled_labels \u003d np.random.randint(0, num_classes, 2 * num_test)\n\n        trick \u003d np.ones(2 * num_test)\n\n        generator_test_loss \u003d combined.evaluate(\n            [noise, sampled_labels.reshape((-1, 1))],\n            [trick, sampled_labels], verbose\u003dFalse)\n\n        generator_train_loss \u003d np.mean(np.array(epoch_gen_loss), axis\u003d0)\n\n        # generate an epoch report on performance\n        train_history[\u0027generator\u0027].append(generator_train_loss)\n        train_history[\u0027discriminator\u0027].append(discriminator_train_loss)\n\n        test_history[\u0027generator\u0027].append(generator_test_loss)\n        test_history[\u0027discriminator\u0027].append(discriminator_test_loss)\n\n        print(\u0027{0:\u003c22s} | {1:4s} | {2:15s} | {3:5s}\u0027.format(\n            \u0027component\u0027, *discriminator.metrics_names))\n        print(\u0027-\u0027 * 65)\n\n        ROW_FMT \u003d \u0027{0:\u003c22s} | {1:\u003c4.2f} | {2:\u003c15.2f} | {3:\u003c5.2f}\u0027\n        print(ROW_FMT.format(\u0027generator (train)\u0027,\n                             *train_history[\u0027generator\u0027][-1]))\n        print(ROW_FMT.format(\u0027generator (test)\u0027,\n                             *test_history[\u0027generator\u0027][-1]))\n        print(ROW_FMT.format(\u0027discriminator (train)\u0027,\n                             *train_history[\u0027discriminator\u0027][-1]))\n        print(ROW_FMT.format(\u0027discriminator (test)\u0027,\n                             *test_history[\u0027discriminator\u0027][-1]))\n\n        # save weights every epoch\n        generator.save_weights(\n            \u0027params_generator_epoch_{0:03d}.hdf5\u0027.format(epoch), True)\n        discriminator.save_weights(\n            \u0027params_discriminator_epoch_{0:03d}.hdf5\u0027.format(epoch), True)\n\n        # generate some digits to display\n        num_rows \u003d 40\n        noise \u003d np.tile(np.random.uniform(-1, 1, (num_rows, latent_size)),\n                        (num_classes, 1))\n\n        sampled_labels \u003d np.array([\n            [i] * num_rows for i in range(num_classes)\n        ]).reshape(-1, 1)\n\n        # get a batch to display\n        generated_images \u003d generator.predict(\n            [noise, sampled_labels], verbose\u003d0)\n\n        # prepare real images sorted by class label\n        real_labels \u003d y_train[(epoch - 1) * num_rows * num_classes:\n                              epoch * num_rows * num_classes]\n        indices \u003d np.argsort(real_labels, axis\u003d0)\n        real_images \u003d x_train[(epoch - 1) * num_rows * num_classes:\n                              epoch * num_rows * num_classes][indices]\n\n        # display generated images, white separator, real images\n        img \u003d np.concatenate(\n            (generated_images,\n             np.repeat(np.ones_like(x_train[:1]), num_rows, axis\u003d0),\n             real_images))\n\n        # arrange them into a grid\n        img \u003d (np.concatenate([r.reshape(-1, 28)\n                               for r in np.split(img, 2 * num_classes + 1)\n                               ], axis\u003d-1) * 127.5 + 127.5).astype(np.uint8)\n\n        Image.fromarray(img).save(\n            \u0027plot_epoch_{0:03d}_generated.png\u0027.format(epoch))\n\n    pickle.dump({\u0027train\u0027: train_history, \u0027test\u0027: test_history},\n                open(\u0027acgan-history.pkl\u0027, \u0027wb\u0027))",
      "user": "prashantp@qubole.com",
      "dateUpdated": "Jan 22, 2018 12:41:26 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [],
        "numCompletedTasks": 0,
        "numTasks": 0,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1516096725710_1734328694",
      "id": "20180116-095845_1564258401_q_Y77BHVDMRM1516096724",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "java.net.ConnectException: Connection refused\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:579)\n\tat org.apache.thrift.transport.TSocket.open(TSocket.java:182)\n\tat org.apache.zeppelin.interpreter.remote.ClientFactory.create(ClientFactory.java:51)\n\tat org.apache.zeppelin.interpreter.remote.ClientFactory.create(ClientFactory.java:37)\n\tat org.apache.commons.pool2.BasePooledObjectFactory.makeObject(BasePooledObjectFactory.java:60)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.create(GenericObjectPool.java:861)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:435)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:363)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.getClient(RemoteInterpreterProcess.java:307)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.interpret(RemoteInterpreter.java:336)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:116)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:376)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:200)\n\tat org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:339)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n"
      },
      "dateCreated": "Jan 16, 2018 9:58:45 AM",
      "dateStarted": "Jan 22, 2018 12:41:26 PM",
      "dateFinished": "Jan 22, 2018 12:41:26 PM",
      "status": "ERROR",
      "errorMessage": "java.net.ConnectException: Connection refused\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)\n\tat java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)\n\tat java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\n\tat java.net.Socket.connect(Socket.java:579)\n\tat org.apache.thrift.transport.TSocket.open(TSocket.java:182)\n\tat org.apache.zeppelin.interpreter.remote.ClientFactory.create(ClientFactory.java:51)\n\tat org.apache.zeppelin.interpreter.remote.ClientFactory.create(ClientFactory.java:37)\n\tat org.apache.commons.pool2.BasePooledObjectFactory.makeObject(BasePooledObjectFactory.java:60)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.create(GenericObjectPool.java:861)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:435)\n\tat org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:363)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.getClient(RemoteInterpreterProcess.java:307)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.interpret(RemoteInterpreter.java:336)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:116)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:376)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:200)\n\tat org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:339)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "Input : The MNIST dataset contains 60000 training images of 28x28 size and 10000 testing greyscale images of 28x28 size.\n    \n    The dataset is divided into one training batch with 60000 images and one test batch with 10000 images.\n    \n    Each batch contains a dictionary with the following elements:-\n    \n    data -- a 60000x28x28 numpy array of uint8s in case of training sample and 10000x28x28 numpy array of uint8s in case of testing sample. The first dimension of the array \n    marks the image number and the other dimensions is for the image matrix.\n    labels -- a list of 60000 numbers which are either 0 or 1 for training sample and 10000 numbers  for testing sample. The number at index i indicates the label of the ith            image in the array data.\n    \nOutput : The final loss for each components of acgan genarator and discriminator. Trained - history model is saved along as pkl file. generated images are saved as .png.",
      "user": "prashantp@qubole.com",
      "dateUpdated": "Jan 22, 2018 12:41:26 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": false,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1516101193463_-1948529414",
      "id": "20180116-111313_958054365_q_Y77BHVDMRM1516096724",
      "dateCreated": "Jan 16, 2018 11:13:13 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1516625012580_649236705",
      "id": "20180122-124332_1567433457_q_Y77BHVDMRM1516096724",
      "dateCreated": "Jan 22, 2018 12:43:32 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "mnist_acgan_KERAS_TENSORFLOW",
  "id": "Y77BHVDMRM1516096724",
  "angularObjects": {
    "2D7M1HZP5932661518613479637:shared_process": [],
    "2D6B43M8G932661515762929145:shared_process": [],
    "2D35KXZZK932661515762929137:shared_process": []
  },
  "config": {
    "isDashboard": false
  },
  "info": {},
  "source": "FCN"
}