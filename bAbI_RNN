{
  "paragraphs": [
    {
      "text": "os.environ[\"KERAS_BACKEND\"] \u003d \"theano\"\nos.environ[\u0027THEANO_FLAGS\u0027] \u003d \u0027floatX\u003dfloat32,device\u003dcuda,dnn.library_path\u003d/usr/local/cuda/lib64,dnn.include_path\u003d/usr/local/cuda/include/\u0027\nos.environ[\u0027MKL_THREADING_LAYER\u0027] \u003d \"GNU\"",
      "user": "prashantp@qubole.com",
      "dateUpdated": "May 25, 2018 6:46:41 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [],
        "numCompletedTasks": 0,
        "numTasks": 0,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1516357425830_-1654927244",
      "id": "20180119-102345_1169506047_q_49VFPVTWB21527180980",
      "dateCreated": "Jan 19, 2018 10:23:45 AM",
      "dateSubmitted": "May 25, 2018 6:46:41 AM",
      "dateStarted": "May 25, 2018 6:46:41 AM",
      "dateFinished": "May 25, 2018 6:46:41 AM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "# Confirming backend is changed to Theano\nfrom __future__ import print_function\nimport keras\nfrom keras import backend as K\nprint(K.backend())",
      "user": "prashantp@qubole.com",
      "dateUpdated": "May 25, 2018 6:46:41 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [],
        "numCompletedTasks": 0,
        "numTasks": 0,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1516357432932_-1768412846",
      "id": "20180119-102352_1430423204_q_49VFPVTWB21527180980",
      "dateCreated": "Jan 19, 2018 10:23:52 AM",
      "dateSubmitted": "May 25, 2018 6:46:41 AM",
      "dateStarted": "May 25, 2018 6:46:41 AM",
      "dateFinished": "May 25, 2018 6:46:41 AM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "from __future__ import print_function\nfrom functools import reduce\nimport re\nimport tarfile\n\nimport numpy as np\n\nfrom keras.utils.data_utils import get_file\nfrom keras.layers.embeddings import Embedding\nfrom keras import layers\nfrom keras.layers import recurrent\nfrom keras.models import Model\nfrom keras.preprocessing.sequence import pad_sequences\n\n\ndef tokenize(sent):\n    \u0027\u0027\u0027Return the tokens of a sentence including punctuation.\n    \u003e\u003e\u003e tokenize(\u0027Bob dropped the apple. Where is the apple?\u0027)\n    [\u0027Bob\u0027, \u0027dropped\u0027, \u0027the\u0027, \u0027apple\u0027, \u0027.\u0027, \u0027Where\u0027, \u0027is\u0027, \u0027the\u0027, \u0027apple\u0027, \u0027?\u0027]\n    \u0027\u0027\u0027\n    return [x.strip() for x in re.split(\u0027(\\W+)?\u0027, sent) if x.strip()]\n\n\ndef parse_stories(lines, only_supporting\u003dFalse):\n    \u0027\u0027\u0027Parse stories provided in the bAbi tasks format\n    If only_supporting is true,\n    only the sentences that support the answer are kept.\n    \u0027\u0027\u0027\n    data \u003d []\n    story \u003d []\n    for line in lines:\n        line \u003d line.decode(\u0027utf-8\u0027).strip()\n        nid, line \u003d line.split(\u0027 \u0027, 1)\n        nid \u003d int(nid)\n        if nid \u003d\u003d 1:\n            story \u003d []\n        if \u0027\\t\u0027 in line:\n            q, a, supporting \u003d line.split(\u0027\\t\u0027)\n            q \u003d tokenize(q)\n            substory \u003d None\n            if only_supporting:\n                # Only select the related substory\n                supporting \u003d map(int, supporting.split())\n                substory \u003d [story[i - 1] for i in supporting]\n            else:\n                # Provide all the substories\n                substory \u003d [x for x in story if x]\n            data.append((substory, q, a))\n            story.append(\u0027\u0027)\n        else:\n            sent \u003d tokenize(line)\n            story.append(sent)\n    return data\n\n\ndef get_stories(f, only_supporting\u003dFalse, max_length\u003dNone):\n    \u0027\u0027\u0027Given a file name, read the file, retrieve the stories,\n    and then convert the sentences into a single story.\n    If max_length is supplied,\n    any stories longer than max_length tokens will be discarded.\n    \u0027\u0027\u0027\n    data \u003d parse_stories(f.readlines(), only_supporting\u003donly_supporting)\n    flatten \u003d lambda data: reduce(lambda x, y: x + y, data)\n    data \u003d [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) \u003c max_length]\n    return data\n\n\ndef vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n    xs \u003d []\n    xqs \u003d []\n    ys \u003d []\n    for story, query, answer in data:\n        x \u003d [word_idx[w] for w in story]\n        xq \u003d [word_idx[w] for w in query]\n        # let\u0027s not forget that index 0 is reserved\n        y \u003d np.zeros(len(word_idx) + 1)\n        y[word_idx[answer]] \u003d 1\n        xs.append(x)\n        xqs.append(xq)\n        ys.append(y)\n    return pad_sequences(xs, maxlen\u003dstory_maxlen), pad_sequences(xqs, maxlen\u003dquery_maxlen), np.array(ys)\n\nRNN \u003d recurrent.LSTM\nEMBED_HIDDEN_SIZE \u003d 50\nSENT_HIDDEN_SIZE \u003d 100\nQUERY_HIDDEN_SIZE \u003d 100\nBATCH_SIZE \u003d 32\nEPOCHS \u003d 5\nprint(\u0027RNN / Embed / Sent / Query \u003d {}, {}, {}, {}\u0027.format(RNN,\n                                                           EMBED_HIDDEN_SIZE,\n                                                           SENT_HIDDEN_SIZE,\n                                                           QUERY_HIDDEN_SIZE))\n\ntry:\n    path \u003d get_file(\u0027babi-tasks-v1-2.tar.gz\u0027, origin\u003d\u0027https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\u0027)\nexcept:\n    print(\u0027Error downloading dataset, please download it manually:\\n\u0027\n          \u0027$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\\n\u0027\n          \u0027$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz\u0027)\n    raise\ntar \u003d tarfile.open(path)\n# Default QA1 with 1000 samples\n# challenge \u003d \u0027tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt\u0027\n# QA1 with 10,000 samples\n# challenge \u003d \u0027tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt\u0027\n# QA2 with 1000 samples\nchallenge \u003d \u0027tasks_1-20_v1-2/en/qa2_two-supporting-facts_{}.txt\u0027\n# QA2 with 10,000 samples\n# challenge \u003d \u0027tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt\u0027\ntrain \u003d get_stories(tar.extractfile(challenge.format(\u0027train\u0027)))\ntest \u003d get_stories(tar.extractfile(challenge.format(\u0027test\u0027)))\n\nvocab \u003d set()\nfor story, q, answer in train + test:\n    vocab |\u003d set(story + q + [answer])\nvocab \u003d sorted(vocab)\n\n# Reserve 0 for masking via pad_sequences\nvocab_size \u003d len(vocab) + 1\nword_idx \u003d dict((c, i + 1) for i, c in enumerate(vocab))\nstory_maxlen \u003d max(map(len, (x for x, _, _ in train + test)))\nquery_maxlen \u003d max(map(len, (x for _, x, _ in train + test)))\n\nx, xq, y \u003d vectorize_stories(train, word_idx, story_maxlen, query_maxlen)\ntx, txq, ty \u003d vectorize_stories(test, word_idx, story_maxlen, query_maxlen)\n\nprint(\u0027vocab \u003d {}\u0027.format(vocab))\nprint(\u0027x.shape \u003d {}\u0027.format(x.shape))\nprint(\u0027xq.shape \u003d {}\u0027.format(xq.shape))\nprint(\u0027y.shape \u003d {}\u0027.format(y.shape))\nprint(\u0027story_maxlen, query_maxlen \u003d {}, {}\u0027.format(story_maxlen, query_maxlen))\n\nprint(\u0027Build model...\u0027)\n\nsentence \u003d layers.Input(shape\u003d(story_maxlen,), dtype\u003d\u0027int32\u0027)\nencoded_sentence \u003d layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(sentence)\nencoded_sentence \u003d layers.Dropout(0.3)(encoded_sentence)\n\nquestion \u003d layers.Input(shape\u003d(query_maxlen,), dtype\u003d\u0027int32\u0027)\nencoded_question \u003d layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(question)\nencoded_question \u003d layers.Dropout(0.3)(encoded_question)\nencoded_question \u003d RNN(EMBED_HIDDEN_SIZE)(encoded_question)\nencoded_question \u003d layers.RepeatVector(story_maxlen)(encoded_question)\n\nmerged \u003d layers.add([encoded_sentence, encoded_question])\nmerged \u003d RNN(EMBED_HIDDEN_SIZE)(merged)\nmerged \u003d layers.Dropout(0.3)(merged)\npreds \u003d layers.Dense(vocab_size, activation\u003d\u0027softmax\u0027)(merged)\n\nmodel \u003d Model([sentence, question], preds)\nmodel.compile(optimizer\u003d\u0027adam\u0027,\n              loss\u003d\u0027categorical_crossentropy\u0027,\n              metrics\u003d[\u0027accuracy\u0027])\n\nprint(\u0027Training\u0027)\nmodel.fit([x, xq], y,\n          batch_size\u003dBATCH_SIZE,\n          epochs\u003dEPOCHS,\n          validation_split\u003d0.05)\nloss, acc \u003d model.evaluate([tx, txq], ty,\n                           batch_size\u003dBATCH_SIZE)\nprint(\u0027Test loss / test accuracy \u003d {:.4f} / {:.4f}\u0027.format(loss, acc))",
      "user": "prashantp@qubole.com",
      "dateUpdated": "May 25, 2018 6:46:41 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [],
        "numCompletedTasks": 0,
        "numTasks": 0,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1516019341729_-919397811",
      "id": "20180115-122901_1736191046_q_49VFPVTWB21527180980",
      "dateCreated": "Jan 15, 2018 12:29:01 PM",
      "dateSubmitted": "May 25, 2018 6:46:41 AM",
      "dateStarted": "May 25, 2018 6:46:41 AM",
      "dateFinished": "May 25, 2018 6:49:25 AM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "Model :- RNN.\nInput :- Facebook bAbI dataset. Contains stories etc. Contains 10000 training stories.\nThe file format for each task is as follows:\nID text\nID text\nID text\nID question[tab]answer[tab]supporting fact IDS.\n...\n\nThe IDs for a given \"story\" start at 1 and increase.\nWhen the IDs in a file reset back to 1 you can consider the following sentences as a new \"story\".\n\nOutput :- Accuracy which is calculated as : questions are asked based on the stories and its accuracy is measured. Real number in the range 0-1.",
      "user": "prashantp@qubole.com",
      "dateUpdated": "May 25, 2018 6:46:41 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": false,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1516019440778_-1534706536",
      "id": "20180115-123040_1534675501_q_49VFPVTWB21527180980",
      "dateCreated": "Jan 15, 2018 12:30:40 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "user": "prashantp@qubole.com",
      "dateUpdated": "May 25, 2018 6:46:41 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1516599370329_816506645",
      "id": "20180122-053610_191919234_q_49VFPVTWB21527180980",
      "dateCreated": "Jan 22, 2018 5:36:10 AM",
      "dateSubmitted": "May 25, 2018 6:46:42 AM",
      "dateStarted": "May 25, 2018 6:49:25 AM",
      "dateFinished": "May 25, 2018 6:49:25 AM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "bAbI_RNN",
  "id": "49VFPVTWB21527180980",
  "angularObjects": {
    "2DEK3QV851090031527168688484:shared_process": [],
    "2DE1GQNCV1090031527180995055:shared_process": [],
    "2DENTX6JV1090031527168688466:shared_process": [],
    "2DDED8P751090031527168688477:shared_process": [],
    "2DE8JRQYX1090031527228139990:shared_process": [],
    "2DDBWFX441090031527168688480:shared_process": []
  },
  "config": {
    "isDashboard": false,
    "defaultLang": "pyspark"
  },
  "info": {},
  "source": "FCN"
}