{
  "paragraphs": [
    {
      "text": "os.environ[\"KERAS_BACKEND\"] \u003d \"theano\"\nos.environ[\u0027THEANO_FLAGS\u0027] \u003d \u0027floatX\u003dfloat32,device\u003dcuda,dnn.library_path\u003d/usr/local/cuda/lib64,dnn.include_path\u003d/usr/local/cuda/include/\u0027\nos.environ[\u0027MKL_THREADING_LAYER\u0027] \u003d \"GNU\"",
      "user": "prashantp@qubole.com",
      "dateUpdated": "May 25, 2018 7:28:57 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [],
        "numCompletedTasks": 0,
        "numTasks": 0,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1516359775380_781921444",
      "id": "20180119-110255_1589126496_q_42TKSQ7CTP1527184367",
      "dateCreated": "Jan 19, 2018 11:02:55 AM",
      "dateSubmitted": "May 24, 2018 5:53:22 PM",
      "dateStarted": "May 24, 2018 5:53:35 PM",
      "dateFinished": "May 24, 2018 5:54:25 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "# Confirming backend is changed to Theano\nfrom __future__ import print_function\nimport keras\nfrom keras import backend as K\nprint(K.backend())",
      "dateUpdated": "Jan 23, 2018 5:26:35 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [],
        "numCompletedTasks": 0,
        "numTasks": 0,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1516359778053_1415602883",
      "id": "20180119-110258_1544642407_q_42TKSQ7CTP1527184367",
      "dateCreated": "Jan 19, 2018 11:02:58 AM",
      "dateSubmitted": "May 24, 2018 5:53:35 PM",
      "dateStarted": "May 24, 2018 5:54:33 PM",
      "dateFinished": "May 24, 2018 5:54:57 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nfrom keras.layers import Input, Dense, Lambda, Flatten, Reshape, Layer\nfrom keras.layers import Conv2D, Conv2DTranspose\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras import metrics\nfrom keras.datasets import mnist\n\n# input image dimensions\nimg_rows, img_cols, img_chns \u003d 28, 28, 1\n# number of convolutional filters to use\nfilters \u003d 64\n# convolution kernel size\nnum_conv \u003d 3\n\nbatch_size \u003d 100\nif K.image_data_format() \u003d\u003d \u0027channels_first\u0027:\n    original_img_size \u003d (img_chns, img_rows, img_cols)\nelse:\n    original_img_size \u003d (img_rows, img_cols, img_chns)\nlatent_dim \u003d 2\nintermediate_dim \u003d 128\nepsilon_std \u003d 1.0\nepochs \u003d 5\n\nx \u003d Input(shape\u003doriginal_img_size)\nconv_1 \u003d Conv2D(img_chns,\n                kernel_size\u003d(2, 2),\n                padding\u003d\u0027same\u0027, activation\u003d\u0027relu\u0027)(x)\nconv_2 \u003d Conv2D(filters,\n                kernel_size\u003d(2, 2),\n                padding\u003d\u0027same\u0027, activation\u003d\u0027relu\u0027,\n                strides\u003d(2, 2))(conv_1)\nconv_3 \u003d Conv2D(filters,\n                kernel_size\u003dnum_conv,\n                padding\u003d\u0027same\u0027, activation\u003d\u0027relu\u0027,\n                strides\u003d1)(conv_2)\nconv_4 \u003d Conv2D(filters,\n                kernel_size\u003dnum_conv,\n                padding\u003d\u0027same\u0027, activation\u003d\u0027relu\u0027,\n                strides\u003d1)(conv_3)\nflat \u003d Flatten()(conv_4)\nhidden \u003d Dense(intermediate_dim, activation\u003d\u0027relu\u0027)(flat)\n\nz_mean \u003d Dense(latent_dim)(hidden)\nz_log_var \u003d Dense(latent_dim)(hidden)\n\n\ndef sampling(args):\n    z_mean, z_log_var \u003d args\n    epsilon \u003d K.random_normal(shape\u003d(K.shape(z_mean)[0], latent_dim),\n                              mean\u003d0., stddev\u003depsilon_std)\n    return z_mean + K.exp(z_log_var) * epsilon\n\n# note that \"output_shape\" isn\u0027t necessary with the TensorFlow backend\n# so you could write `Lambda(sampling)([z_mean, z_log_var])`\nz \u003d Lambda(sampling, output_shape\u003d(latent_dim,))([z_mean, z_log_var])\n\n# we instantiate these layers separately so as to reuse them later\ndecoder_hid \u003d Dense(intermediate_dim, activation\u003d\u0027relu\u0027)\ndecoder_upsample \u003d Dense(filters * 14 * 14, activation\u003d\u0027relu\u0027)\n\nif K.image_data_format() \u003d\u003d \u0027channels_first\u0027:\n    output_shape \u003d (batch_size, filters, 14, 14)\nelse:\n    output_shape \u003d (batch_size, 14, 14, filters)\n\ndecoder_reshape \u003d Reshape(output_shape[1:])\ndecoder_deconv_1 \u003d Conv2DTranspose(filters,\n                                   kernel_size\u003dnum_conv,\n                                   padding\u003d\u0027same\u0027,\n                                   strides\u003d1,\n                                   activation\u003d\u0027relu\u0027)\ndecoder_deconv_2 \u003d Conv2DTranspose(filters,\n                                   kernel_size\u003dnum_conv,\n                                   padding\u003d\u0027same\u0027,\n                                   strides\u003d1,\n                                   activation\u003d\u0027relu\u0027)\nif K.image_data_format() \u003d\u003d \u0027channels_first\u0027:\n    output_shape \u003d (batch_size, filters, 29, 29)\nelse:\n    output_shape \u003d (batch_size, 29, 29, filters)\ndecoder_deconv_3_upsamp \u003d Conv2DTranspose(filters,\n                                          kernel_size\u003d(3, 3),\n                                          strides\u003d(2, 2),\n                                          padding\u003d\u0027valid\u0027,\n                                          activation\u003d\u0027relu\u0027)\ndecoder_mean_squash \u003d Conv2D(img_chns,\n                             kernel_size\u003d2,\n                             padding\u003d\u0027valid\u0027,\n                             activation\u003d\u0027sigmoid\u0027)\n\nhid_decoded \u003d decoder_hid(z)\nup_decoded \u003d decoder_upsample(hid_decoded)\nreshape_decoded \u003d decoder_reshape(up_decoded)\ndeconv_1_decoded \u003d decoder_deconv_1(reshape_decoded)\ndeconv_2_decoded \u003d decoder_deconv_2(deconv_1_decoded)\nx_decoded_relu \u003d decoder_deconv_3_upsamp(deconv_2_decoded)\nx_decoded_mean_squash \u003d decoder_mean_squash(x_decoded_relu)\n\n\n# Custom loss layer\nclass CustomVariationalLayer(Layer):\n    def __init__(self, **kwargs):\n        self.is_placeholder \u003d True\n        super(CustomVariationalLayer, self).__init__(**kwargs)\n\n    def vae_loss(self, x, x_decoded_mean_squash):\n        x \u003d K.flatten(x)\n        x_decoded_mean_squash \u003d K.flatten(x_decoded_mean_squash)\n        xent_loss \u003d img_rows * img_cols * metrics.binary_crossentropy(x, x_decoded_mean_squash)\n        kl_loss \u003d - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis\u003d-1)\n        return K.mean(xent_loss + kl_loss)\n\n    def call(self, inputs):\n        x \u003d inputs[0]\n        x_decoded_mean_squash \u003d inputs[1]\n        loss \u003d self.vae_loss(x, x_decoded_mean_squash)\n        self.add_loss(loss, inputs\u003dinputs)\n        # We don\u0027t use this output.\n        return x\n\n\ny \u003d CustomVariationalLayer()([x, x_decoded_mean_squash])\nvae \u003d Model(x, y)\nvae.compile(optimizer\u003d\u0027rmsprop\u0027, loss\u003dNone)\nvae.summary()\n\n# train the VAE on MNIST digits\n(x_train, _), (x_test, y_test) \u003d mnist.load_data()\n\nx_train \u003d x_train.astype(\u0027float32\u0027) / 255.\nx_train \u003d x_train.reshape((x_train.shape[0],) + original_img_size)\nx_test \u003d x_test.astype(\u0027float32\u0027) / 255.\nx_test \u003d x_test.reshape((x_test.shape[0],) + original_img_size)\n\nprint(\u0027x_train.shape:\u0027, x_train.shape)\n\nvae.fit(x_train,\n        shuffle\u003dTrue,\n        epochs\u003depochs,\n        batch_size\u003dbatch_size,\n        validation_data\u003d(x_test, None))\n\n# build a model to project inputs on the latent space\nencoder \u003d Model(x, z_mean)\n\n# display a 2D plot of the digit classes in the latent space\nx_test_encoded \u003d encoder.predict(x_test, batch_size\u003dbatch_size)\nplt.figure(figsize\u003d(6, 6))\nplt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c\u003dy_test)\nplt.colorbar()\nplt.show()\n\n# build a digit generator that can sample from the learned distribution\ndecoder_input \u003d Input(shape\u003d(latent_dim,))\n_hid_decoded \u003d decoder_hid(decoder_input)\n_up_decoded \u003d decoder_upsample(_hid_decoded)\n_reshape_decoded \u003d decoder_reshape(_up_decoded)\n_deconv_1_decoded \u003d decoder_deconv_1(_reshape_decoded)\n_deconv_2_decoded \u003d decoder_deconv_2(_deconv_1_decoded)\n_x_decoded_relu \u003d decoder_deconv_3_upsamp(_deconv_2_decoded)\n_x_decoded_mean_squash \u003d decoder_mean_squash(_x_decoded_relu)\ngenerator \u003d Model(decoder_input, _x_decoded_mean_squash)\n\n# display a 2D manifold of the digits\nn \u003d 15  # figure with 15x15 digits\ndigit_size \u003d 28\nfigure \u003d np.zeros((digit_size * n, digit_size * n))\n# linearly spaced coordinates on the unit square were transformed through the inverse CDF (ppf) of the Gaussian\n# to produce values of the latent variables z, since the prior of the latent space is Gaussian\ngrid_x \u003d norm.ppf(np.linspace(0.05, 0.95, n))\ngrid_y \u003d norm.ppf(np.linspace(0.05, 0.95, n))\n\nfor i, yi in enumerate(grid_x):\n    for j, xi in enumerate(grid_y):\n        z_sample \u003d np.array([[xi, yi]])\n        z_sample \u003d np.tile(z_sample, batch_size).reshape(batch_size, 2)\n        x_decoded \u003d generator.predict(z_sample, batch_size\u003dbatch_size)\n        digit \u003d x_decoded[0].reshape(digit_size, digit_size)\n        figure[i * digit_size: (i + 1) * digit_size,\n               j * digit_size: (j + 1) * digit_size] \u003d digit\n\nplt.figure(figsize\u003d(10, 10))\nplt.imshow(figure, cmap\u003d\u0027Greys_r\u0027)\nplt.show()",
      "dateUpdated": "Jan 23, 2018 5:26:35 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [],
        "numCompletedTasks": 0,
        "numTasks": 0,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1516098621336_-1082320602",
      "id": "20180116-103021_403852998_q_42TKSQ7CTP1527184367",
      "dateCreated": "Jan 16, 2018 10:30:21 AM",
      "dateSubmitted": "May 24, 2018 5:54:33 PM",
      "dateStarted": "May 24, 2018 5:54:58 PM",
      "dateFinished": "May 24, 2018 5:55:06 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "Creating Variational Autoencoders using Deconvolution layers.\n\nInput : The MNIST dataset contains 60000 training images of 28x28 size and 10000 testing greyscale images of 28x28 size.\n    \n    The dataset is divided into one training batch with 60000 images and one test batch with 10000 images.\n    \n    Each batch contains a dictionary with the following elements:-\n    \n    data -- a 60000x28x28 numpy array of uint8s in case of training sample and 10000x28x28 numpy array of uint8s in case of testing sample. The first dimension of the array \n    marks the image number and the other dimensions is for the image matrix.\n    labels -- a list of 60000 numbers which are either 0 or 1 for training sample and 10000 numbers  for testing sample. The number at index i indicates the label of the ith            image in the array data.\n\nOutput : Accuracy of the classification done by the trained model. Real No. Between 0-1.",
      "user": "prashantp@qubole.com",
      "dateUpdated": "Jan 23, 2018 5:26:36 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": false,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1516171787457_-475158191",
      "id": "20180117-064947_1843275354_q_42TKSQ7CTP1527184367",
      "dateCreated": "Jan 17, 2018 6:49:47 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1516685196153_-2129343004",
      "id": "20180123-052636_816460250_q_42TKSQ7CTP1527184367",
      "dateCreated": "Jan 23, 2018 5:26:36 AM",
      "dateSubmitted": "May 24, 2018 5:54:58 PM",
      "dateStarted": "May 24, 2018 5:55:06 PM",
      "dateFinished": "May 24, 2018 5:55:06 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "variational_autoencoder_deconv",
  "id": "42TKSQ7CTP1527184367",
  "angularObjects": {
    "2DEK3QV851090031527168688484:shared_process": [],
    "2DENTX6JV1090031527168688466:shared_process": [],
    "2DDED8P751090031527168688477:shared_process": [],
    "2DG36A4RM1090031527184381695:shared_process": [],
    "2DDBWFX441090031527168688480:shared_process": []
  },
  "config": {
    "isDashboard": false,
    "defaultLang": "pyspark"
  },
  "info": {},
  "source": "FCN"
}