{
  "paragraphs": [
    {
      "text": "from __future__ import print_function\n\nfrom keras.preprocessing.image import load_img, img_to_array\nimport numpy as np\nimport scipy\nimport argparse\n\nfrom keras.applications import inception_v3\nfrom keras import backend as K\n\nbase_image_path \u003d \u0027/media/pexels.png\u0027\nresult_prefix \u003d \u0027deep_dream1\u0027\n\n# These are the names of the layers\n# for which we try to maximize activation,\n# as well as their weight in the final loss\n# we try to maximize.\n# You can tweak these setting to obtain new visual effects.\nsettings \u003d {\n    \u0027features\u0027: {\n        \u0027mixed2\u0027: 0.2,\n        \u0027mixed3\u0027: 0.5,\n        \u0027mixed4\u0027: 2.,\n        \u0027mixed5\u0027: 1.5,\n    },\n}\n\n\ndef preprocess_image(image_path):\n    # Util function to open, resize and format pictures\n    # into appropriate tensors.\n    img \u003d load_img(image_path)\n    img \u003d img_to_array(img)\n    img \u003d np.expand_dims(img, axis\u003d0)\n    img \u003d inception_v3.preprocess_input(img)\n    return img\n\n\ndef deprocess_image(x):\n    # Util function to convert a tensor into a valid image.\n    if K.image_data_format() \u003d\u003d \u0027channels_first\u0027:\n        x \u003d x.reshape((3, x.shape[2], x.shape[3]))\n        x \u003d x.transpose((1, 2, 0))\n    else:\n        x \u003d x.reshape((x.shape[1], x.shape[2], 3))\n    x /\u003d 2.\n    x +\u003d 0.5\n    x *\u003d 255.\n    x \u003d np.clip(x, 0, 255).astype(\u0027uint8\u0027)\n    return x\n\nK.set_learning_phase(0)\n\n# Build the InceptionV3 network with our placeholder.\n# The model will be loaded with pre-trained ImageNet weights.\nmodel \u003d inception_v3.InceptionV3(weights\u003d\u0027imagenet\u0027,\n                                 include_top\u003dFalse)\ndream \u003d model.input\nprint(\u0027Model loaded.\u0027)\n\n# Get the symbolic outputs of each \"key\" layer (we gave them unique names).\nlayer_dict \u003d dict([(layer.name, layer) for layer in model.layers])\n\n# Define the loss.\nloss \u003d K.variable(0.)\nfor layer_name in settings[\u0027features\u0027]:\n    # Add the L2 norm of the features of a layer to the loss.\n    assert layer_name in layer_dict.keys(), \u0027Layer \u0027 + layer_name + \u0027 not found in model.\u0027\n    coeff \u003d settings[\u0027features\u0027][layer_name]\n    x \u003d layer_dict[layer_name].output\n    # We avoid border artifacts by only involving non-border pixels in the loss.\n    scaling \u003d K.prod(K.cast(K.shape(x), \u0027float32\u0027))\n    if K.image_data_format() \u003d\u003d \u0027channels_first\u0027:\n        loss +\u003d coeff * K.sum(K.square(x[:, :, 2: -2, 2: -2])) / scaling\n    else:\n        loss +\u003d coeff * K.sum(K.square(x[:, 2: -2, 2: -2, :])) / scaling\n\n# Compute the gradients of the dream wrt the loss.\ngrads \u003d K.gradients(loss, dream)[0]\n# Normalize gradients.\ngrads /\u003d K.maximum(K.mean(K.abs(grads)), K.epsilon())\n\n# Set up function to retrieve the value\n# of the loss and gradients given an input image.\noutputs \u003d [loss, grads]\nfetch_loss_and_grads \u003d K.function([dream], outputs)\n\n\ndef eval_loss_and_grads(x):\n    outs \u003d fetch_loss_and_grads([x])\n    loss_value \u003d outs[0]\n    grad_values \u003d outs[1]\n    return loss_value, grad_values\n\n\ndef resize_img(img, size):\n    img \u003d np.copy(img)\n    if K.image_data_format() \u003d\u003d \u0027channels_first\u0027:\n        factors \u003d (1, 1,\n                   float(size[0]) / img.shape[2],\n                   float(size[1]) / img.shape[3])\n    else:\n        factors \u003d (1,\n                   float(size[0]) / img.shape[1],\n                   float(size[1]) / img.shape[2],\n                   1)\n    return scipy.ndimage.zoom(img, factors, order\u003d1)\n\n\ndef gradient_ascent(x, iterations, step, max_loss\u003dNone):\n    for i in range(iterations):\n        loss_value, grad_values \u003d eval_loss_and_grads(x)\n        if max_loss is not None and loss_value \u003e max_loss:\n            break\n        print(\u0027..Loss value at\u0027, i, \u0027:\u0027, loss_value)\n        x +\u003d step * grad_values\n    return x\n\n\ndef save_img(img, fname):\n    pil_img \u003d deprocess_image(np.copy(img))\n    scipy.misc.imsave(fname, pil_img)\n\n\n\"\"\"Process:\n- Load the original image.\n- Define a number of processing scales (i.e. image shapes),\n    from smallest to largest.\n- Resize the original image to the smallest scale.\n- For every scale, starting with the smallest (i.e. current one):\n    - Run gradient ascent\n    - Upscale image to the next scale\n    - Reinject the detail that was lost at upscaling time\n- Stop when we are back to the original size.\nTo obtain the detail lost during upscaling, we simply\ntake the original image, shrink it down, upscale it,\nand compare the result to the (resized) original image.\n\"\"\"\n\n\n# Playing with these hyperparameters will also allow you to achieve new effects\nstep \u003d 0.01  # Gradient ascent step size\nnum_octave \u003d 3  # Number of scales at which to run gradient ascent\noctave_scale \u003d 1.4  # Size ratio between scales\niterations \u003d 20  # Number of ascent steps per scale\nmax_loss \u003d 10.\n\nimg \u003d preprocess_image(base_image_path)\nif K.image_data_format() \u003d\u003d \u0027channels_first\u0027:\n    original_shape \u003d img.shape[2:]\nelse:\n    original_shape \u003d img.shape[1:3]\nsuccessive_shapes \u003d [original_shape]\nfor i in range(1, num_octave):\n    shape \u003d tuple([int(dim / (octave_scale ** i)) for dim in original_shape])\n    successive_shapes.append(shape)\nsuccessive_shapes \u003d successive_shapes[::-1]\noriginal_img \u003d np.copy(img)\nshrunk_original_img \u003d resize_img(img, successive_shapes[0])\n\nfor shape in successive_shapes:\n    print(\u0027Processing image shape\u0027, shape)\n    img \u003d resize_img(img, shape)\n    img \u003d gradient_ascent(img,\n                          iterations\u003diterations,\n                          step\u003dstep,\n                          max_loss\u003dmax_loss)\n    upscaled_shrunk_original_img \u003d resize_img(shrunk_original_img, shape)\n    same_size_original \u003d resize_img(original_img, shape)\n    lost_detail \u003d same_size_original - upscaled_shrunk_original_img\n\n    img +\u003d lost_detail\n    shrunk_original_img \u003d resize_img(original_img, shape)\n\nsave_img(img, fname\u003dresult_prefix + \u0027.png\u0027)",
      "user": "prashantp@qubole.com",
      "dateUpdated": "Jan 30, 2018 6:24:08 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [],
        "numCompletedTasks": 0,
        "numTasks": 0,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1516024280304_1052963722",
      "id": "20180115-135120_832158464_q_HW5E2JF2PC1516024279",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "Model loaded.\nProcessing image shape (454, 642)\n..Loss value at 0 : 0.923234\n..Loss value at 1 : 1.08647\n..Loss value at 2 : 1.36208\n..Loss value at 3 : 1.6844\n..Loss value at 4 : 2.01782\n..Loss value at 5 : 2.38182\n..Loss value at 6 : 2.73682\n..Loss value at 7 : 3.12554\n..Loss value at 8 : 3.5136\n..Loss value at 9 : 3.91813\n..Loss value at 10 : 4.30221\n..Loss value at 11 : 4.67709\n..Loss value at 12 : 5.05417\n..Loss value at 13 : 5.45245\n..Loss value at 14 : 5.85301\n..Loss value at 15 : 6.21283\n..Loss value at 16 : 6.60464\n..Loss value at 17 : 6.96794\n..Loss value at 18 : 7.33009\n..Loss value at 19 : 7.69144\nProcessing image shape (635, 900)\n..Loss value at 0 : 1.66045\n..Loss value at 1 : 2.60369\n..Loss value at 2 : 3.55662\n..Loss value at 3 : 4.60885\n..Loss value at 4 : 6.03533\n..Loss value at 5 : 8.20347\nProcessing image shape (890, 1260)\n..Loss value at 0 : 2.94361\n..Loss value at 1 : 6.15473\n"
      },
      "dateCreated": "Jan 15, 2018 1:51:20 PM",
      "dateStarted": "Jan 30, 2018 6:24:08 AM",
      "dateFinished": "Jan 30, 2018 6:25:25 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import os\nimport base64\nstartSvgTag \u003d \"\"\"\u003c?xml version\u003d\"1.0\" encoding\u003d\"UTF-8\" standalone\u003d\"no\"?\u003e\n\u003c!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n\"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\"\u003e\n\u003csvg version\u003d\"1.1\"\nxmlns\u003d\"http://www.w3.org/2000/svg\"\nxmlns:xlink\u003d\"http://www.w3.org/1999/xlink\"\nwidth\u003d\"240px\" height\u003d\"240px\" viewBox\u003d\"0 0 240 240\"\u003e\"\"\"\n\nendSvgTag \u003d \"\"\"\u003c/svg\u003e\"\"\"\nfor path, subdirs, files in os.walk(\u0027/media\u0027):\n    for name in files:\n      if name.endswith(\".png\"):\n         paths \u003d os.path.join(path,name)\n         pngFile \u003d open(paths, \u0027rb\u0027)\n         base64data \u003d base64.b64encode(pngFile.read())\n         base64String \u003d \u0027\u003cimage xlink:href\u003d\"data:image/png;base64,{0}\" width\u003d\"240\"   height\u003d\"240\" x\u003d\"0\" y\u003d\"0\" /\u003e\u0027.format(base64data)\n         print(os.path.splitext(name)[0])\n         f \u003d open(\"/tmp/\"+os.path.splitext(name)[0]+\".svg\",\u0027w\u0027)\n         f.write( startSvgTag + base64String + endSvgTag)\n",
      "user": "prashantp@qubole.com",
      "dateUpdated": "Jan 30, 2018 6:23:35 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python",
        "lineNumbers": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1517225005996_-190220424",
      "id": "20180129-112325_852425966_q_HW5E2JF2PC1516024279",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "pexels\n1555999\nsaved_image_deep_dream1\n2307315\n"
      },
      "dateCreated": "Jan 29, 2018 11:23:25 AM",
      "dateStarted": "Jan 30, 2018 6:23:35 AM",
      "dateFinished": "Jan 30, 2018 6:23:35 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "from IPython.display import Image\nimage \u003d Image(filename\u003d\u0027/media/pexels.png\u0027)\nz.show(image)",
      "user": "prashantp@qubole.com",
      "dateUpdated": "Jan 30, 2018 6:42:47 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "paragraphProgress": {
        "jobs": [],
        "numCompletedTasks": 0,
        "numTasks": 0,
        "truncated": false
      },
      "version": "v0",
      "jobName": "paragraph_1517232555774_428382108",
      "id": "20180129-132915_1426327769_q_HW5E2JF2PC1516024279",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\u003cIPython.core.display.Image object\u003e\n"
      },
      "dateCreated": "Jan 29, 2018 1:29:15 PM",
      "dateStarted": "Jan 30, 2018 6:42:47 AM",
      "dateFinished": "Jan 30, 2018 6:42:47 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "Needs an image file to be present already before execution. ",
      "user": "prashantp@qubole.com",
      "dateUpdated": "Jan 19, 2018 10:28:28 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": false,
        "editorMode": "ace/mode/python"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "version": "v0",
      "jobName": "paragraph_1516097158152_969662419",
      "id": "20180116-100558_1701227353_q_HW5E2JF2PC1516024279",
      "dateCreated": "Jan 16, 2018 10:05:58 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "deep_dream",
  "id": "HW5E2JF2PC1516024279",
  "angularObjects": {
    "2D7M1HZP5932661518613479637:shared_process": [],
    "2D6B43M8G932661515762929145:shared_process": [],
    "2D35KXZZK932661515762929137:shared_process": []
  },
  "config": {
    "isDashboard": false
  },
  "info": {},
  "source": "FCN"
}